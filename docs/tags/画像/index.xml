<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>画像 on MatLoverによるMatlab以外のブログ</title>
        <link>https://opqrstuvcut.github.io/blog/tags/%E7%94%BB%E5%83%8F/</link>
        <description>Recent content in 画像 on MatLoverによるMatlab以外のブログ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja-jp</language>
        <lastBuildDate>Sat, 31 May 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://opqrstuvcut.github.io/blog/tags/%E7%94%BB%E5%83%8F/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>外部知識を活用して効率的に性能向上を達成したYOLO-RD</title>
        <link>https://opqrstuvcut.github.io/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/</link>
        <pubDate>Sat, 31 May 2025 00:00:00 +0000</pubDate>
        
        <guid>https://opqrstuvcut.github.io/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/</guid>
        <description>&lt;img src="https://opqrstuvcut.github.io/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/thm.png" alt="Featured image of post 外部知識を活用して効率的に性能向上を達成したYOLO-RD" /&gt;&lt;p&gt;YOLO-RD (Retriever-Dictionary) は、物体検出の分野で定番となっているYOLO（You Only Look Once）シリーズの最新研究です.
今回は、ICLR2025で発表されたYOLO-RD(&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2410.15346&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://arxiv.org/abs/2410.15346&lt;/a&gt;)について解説します.&lt;/p&gt;
&lt;h2 id=&#34;従来手法の限界と問題意識&#34;&gt;従来手法の限界と問題意識
&lt;/h2&gt;&lt;p&gt;現在の画像系のモデルのアプローチは大きくわけて2種類です.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;CNN: 畳み込みによって得られる局所情報を組み合わせて推論&lt;/li&gt;
&lt;li&gt;Transformer: 画像内の広範な相互作用を活用して推論&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;これらも現在ではかなり高い精度を達成できるようになりましたが、「入力画像とモデル内で計算される情報」しか活用できません（当たり前なのですが）.&lt;br&gt;
もしRAGのように入力画像に応じて外部からの情報を参照することができれば、モデルの推論性能が向上するのでは？というのがYOLO-RDの発想になります.&lt;br&gt;
人間も、例えば犬のような動物を見たときに、犬ってこういう形だなぁと連想し、それから確かに目の前の動物は犬だと判断したり、あるいは犬じゃない他の動物かも？と判断していると思います．このように適宜情報を連想するという話に近いのかなと思います．&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig1.png&#34;
	width=&#34;946&#34;
	height=&#34;329&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig1_hu3745882659329699378.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig1_hu9918131085593678498.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;YOLO-RDの概要&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;287&#34;
		data-flex-basis=&#34;690px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;yolo-rdの概要&#34;&gt;YOLO-RDの概要
&lt;/h2&gt;&lt;p&gt;YOLO-RDは推論時に外部の知識（特徴量）を参照する仕組みを組み込んだYOLOです.&lt;br&gt;
これにより、モデルが画像からは直接取得できない情報を補い、パラメータ数を大きく増やすことなく高精度化を実現しています.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig2.png&#34;
	width=&#34;936&#34;
	height=&#34;346&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig2_hu2701416847664046125.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig2_hu8795489024073455909.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;YOLO-RDの全体像&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;270&#34;
		data-flex-basis=&#34;649px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上図はYOLO-RDの全体像をあらわした図です.
バックボーンから得られた特徴量をもとにして、辞書内の特徴量(atomというようです)ごとの重みを決めるというのをRetrieverでおこないます.&lt;br&gt;
得られた重みによる重み付き和を辞書内のatomに対して計算し、Neck層に渡すという流れになっています.&lt;/p&gt;
&lt;p&gt;式であらわすと次のようになっています.&lt;br&gt;
バックボーンから得られた$(w,h)$の位置のピクセルの特徴量$X_{w,h} \in \mathbb{R}^f$を用いて、&lt;/p&gt;
&lt;p&gt;$$
Y_{w,h} = \lambda \cdot X_{w,h} + (1 - \lambda) \cdot \sum_{i=1}^N c&amp;rsquo;_{i,w,h} \cdot \alpha_i.
$$&lt;/p&gt;
&lt;p&gt;を新たな特徴量とします.ここで&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$\alpha_i  \in \mathbb{R}^f$ は2-ノルムが1のatom&lt;/li&gt;
&lt;li&gt;$c&amp;rsquo;_{i,w,h} \in \mathbb{R}$はatomの取捨選択を表現している係数&lt;/li&gt;
&lt;li&gt;$\lambda \in [0, 1] $はハイパーパラメータ&lt;/li&gt;
&lt;li&gt;$N \in \mathbb{N}$は辞書サイズ&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;をあらわします.&lt;/p&gt;
&lt;h3 id=&#34;retriever&#34;&gt;Retriever
&lt;/h3&gt;&lt;p&gt;$(w,h)$上のピクセルの$i$番目のatomの係数$c&amp;rsquo;_{i,w,h}$は以下のようにして計算します.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;行列$W^G \in \mathbb{R}^{N \times f}$との内積によってベースとなるatomの重み$Y   \in \mathbb{R}^{N \times W \times H}$を計算
$$Y = G(X) = W^G \cdot X. $$&lt;/li&gt;
&lt;li&gt;depthwise convolutionをおこない、近傍ピクセルの情報の取り込み($i$はチャネル)
$$c_i = E(Y^{(i)}) = W^{E^{(i)}} * Y^{(i)}.$$&lt;/li&gt;
&lt;li&gt;学習を適切に進めるためにPositional Normalizationを適用（$\gamma,\beta$は学習するパラメーター）
$$
\begin{align*}
c^{\prime}_{i,w,h} &amp;amp;= \frac{c_{i,w,h} - \mu_{c_{w,h}}}{\sqrt{\sigma_{c_{w,h}} + \varepsilon}} \cdot \gamma + \beta,\\
\mu_{c_{w,h}} &amp;amp;= \frac{1}{N} \sum_{n=1}^N c_{n,w,h},\\
\sigma_{c_{w,h}} &amp;amp;= \frac{1}{N}\sum_{n=1}^N (c_{w,h} - \mu_{c_{w,h}})^2.
\end{align*}
$$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;1と2は一度のconvolutionにまとめることができますが、計算コストのためにこのようになっています.&lt;br&gt;
また、3についてはatomの重み付き和が$X$と同じになってしまうことを防ぐためにおこなわれています.バックボーンから得られる特徴量が良い特徴量であるという前提にたつと、そこに学習初期にはノイズに近いような値になっている辞書情報を足すことにメリットがありません.そして、辞書情報側のパラメーターをうまく活用してlossを下げるよりも、バックボーンと同じ特徴量がNeck層に渡るほうが早くlossを下げられるため、結果的に辞書が意味をなさなくなります.Normalizationを入れることで、$X$をそのままコピーするような出力が難しくなるため、適切に学習が進むようになります.&lt;/p&gt;
&lt;h3 id=&#34;dictionaryの初期化&#34;&gt;Dictionaryの初期化
&lt;/h3&gt;&lt;p&gt;atomの初期値には以下の3パターンが提案されています.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Vision model:
YOLOv9のバックボーンから得られた学習データの特徴量を辞書の初期値にする.&lt;/li&gt;
&lt;li&gt;Vision language model:
学習データの画像の各パッチに対するCLIPから得られる特徴量を辞書の初期値にする.&lt;/li&gt;
&lt;li&gt;Large language model:
MSCOCOデータセット、ImageNet21kのクラス名とMSCOCOのキャプション（説明文）をGPTに与えたときに得られる特徴量を辞書の初期値にする.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;学習データなどから得られる特徴量をそのまま利用すると辞書のサイズが大きすぎるため、k-meansを用いてクラスタリングし、クラスタの中心を辞書内のatomにするようにします.&lt;/p&gt;
&lt;h3 id=&#34;dictionaryの圧縮&#34;&gt;Dictionaryの圧縮
&lt;/h3&gt;&lt;p&gt;モデルが適切に学習されたとしても、実はあまり参照されることがないatomが存在するかもしれません.&lt;br&gt;
こういったatomを削減するために蒸留をおこなうことを提案しています.&lt;br&gt;
具体的にはもとの辞書を用いて得られる特徴量 $z^{rm RD}_{i,w,h}$ が小さい辞書を用いて得られる特徴量 $z^{\rm rd}_{i,w,h}$ と近くなるよう、以下のlossを最小化して小さな辞書を学習していきます.&lt;/p&gt;
&lt;p&gt;$$
\begin{align*}
\mathcal{L}_{i,w,h} = -\log \frac{\exp ({\rm sim}(z^{\rm rd}_{i,w,h}, z^{\rm RD}_{i,w,h})/\tau) }{\sum_{j=1}^B \sum_{w&amp;rsquo;,h&amp;rsquo;}^{W,H} \exp ({\rm sim} (z^{\rm rd}_{i,w,h}, z^{\rm RD}_{i,w,h})/\tau) }.
\end{align*}
$$&lt;/p&gt;
&lt;p&gt;この蒸留により、性能を下げることなく50%のatomを削減できているようです.&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験
&lt;/h2&gt;&lt;h3 id=&#34;提案手法の効果&#34;&gt;提案手法の効果
&lt;/h3&gt;&lt;p&gt;以下は提案手法の効果を示した表です.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab1.png&#34;
	width=&#34;967&#34;
	height=&#34;610&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab1_hu7767481251999724551.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab1_hu7764096949802347058.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;提案手法の効果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;158&#34;
		data-flex-basis=&#34;380px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;ParamsとLatencyをみると提案手法ではわずかな追加コストで済んでいますが、各スコアが向上し、YOLOv9で1%以上の向上を達成しています.&lt;br&gt;
&lt;a class=&#34;link&#34; href=&#34;https://github.com/WongKinYiu/yolov9&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;YOLOv9のレポジトリ&lt;/a&gt;をみてみると、サイズがCからEになると、計算量は102.1GFlopsから189.0GFlopsになり、パラメーター数は25.3Mから57.3Mになり、${\rm mAP}_{.5}^{val}$は2.6%あがっています.一方で、例えばVMのケースだと0.2Mのパラメーターの追加、0.16のLatencyの増加で1.46%向上しています.非常にコスパよく性能があがっていることがわかります.&lt;/p&gt;
&lt;h3 id=&#34;他タスクへの利用&#34;&gt;他タスクへの利用
&lt;/h3&gt;&lt;p&gt;他タスクへも提案手法を適用した結果が次の表になり、きちんと効果があることが示されています.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab2.png&#34;
	width=&#34;546&#34;
	height=&#34;315&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab2_hu3398812665705737277.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab2_hu599008810496615692.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;他タスクへの利用&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;173&#34;
		data-flex-basis=&#34;416px&#34;
	
&gt;&lt;/p&gt;
&lt;h3 id=&#34;既存手法との比較&#34;&gt;既存手法との比較
&lt;/h3&gt;&lt;p&gt;以下は既存の他手法との比較になります.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab3.png&#34;
	width=&#34;425&#34;
	height=&#34;258&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab3_hu2469063103545249303.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab3_hu13878300565109423738.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;他手法との比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;164&#34;
		data-flex-basis=&#34;395px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上記のうち、KDはYOLOv9-EからYOLOv9-Cへの蒸留をあらわしています.&lt;br&gt;
これらの手法と比較してみても、やはり提案手法は効率よく性能を上げられていることがわかります.&lt;/p&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;Ablation Study
&lt;/h3&gt;&lt;p&gt;以下はRetrieverの計算方法の違いをあらわしており、1行目は1度の畳み込みでatomの重みを計算してしまう方法、2行目は提案手法で使われている方法です.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab4.png&#34;
	width=&#34;557&#34;
	height=&#34;174&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab4_hu13486331114492203243.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/tab4_hu4579865393690535001.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Retrieverの計算の違い&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;320&#34;
		data-flex-basis=&#34;768px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;提案手法に比べ、1度のCNNに置き換えることで性能が0.3%程度あがるものの、パラメーター数は10倍近く増えてしまいます.このため、パラメーターあたりの性能向上の効率は非常に悪いことがわかります.&lt;/p&gt;
&lt;h3 id=&#34;可視化&#34;&gt;可視化
&lt;/h3&gt;&lt;h4 id=&#34;atomの係数の可視化&#34;&gt;atomの係数の可視化
&lt;/h4&gt;&lt;p&gt;以下では入力画像とatom、係数の関係を可視化しています.&lt;br&gt;
(a)の矩形部分がモデルへの入力、(b)がバックボーンからの特徴とatomとの相関（類似度を計算？）、(c)が各atomの係数となっています.
&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig4.png&#34;
	width=&#34;917&#34;
	height=&#34;361&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig4_hu7900514752653466333.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig4_hu6284167473182498972.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;atomの係数の可視化&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;609px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;どちらも高い領域に存在するatomに対応する画像は(c)と(d)の矩形部分になっており、きちんと関係のあるatomを参照することができています.
一方で、(b)において相関が低く、係数が0に近いatomは(e)と(f)のように入力画像とは関係のないものになっています.&lt;/p&gt;
&lt;h4 id=&#34;提案手法の有無によるバックボーンからの特徴量の比較&#34;&gt;提案手法の有無によるバックボーンからの特徴量の比較
&lt;/h4&gt;&lt;p&gt;以下は左図が入力画像、中央がバックボーンから得られた特徴量、右が提案手法の辞書を用いて得られた特徴量になります.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig5.png&#34;
	width=&#34;943&#34;
	height=&#34;393&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig5_hu8388633634531988210.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AD%98%E3%82%92%E6%B4%BB%E7%94%A8%E3%81%97%E3%81%A6%E5%8A%B9%E7%8E%87%E7%9A%84%E3%81%AB%E6%80%A7%E8%83%BD%E5%90%91%E4%B8%8A%E3%82%92%E9%81%94%E6%88%90%E3%81%97%E3%81%9Fyolo-rd/fig5_hu12720629858408410038.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;特徴量の比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;239&#34;
		data-flex-basis=&#34;575px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;バックボーンからの特徴量はあまり標識の特徴量を捉えられておらず、また背景の情報を大きく失われています.提案手法ではこれらがうまく特徴量として保持されていることがわかります.&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想
&lt;/h2&gt;&lt;p&gt;追加の計算コストが少なく精度をあげられるのはかなり魅力的です.実問題でも、あと一押し精度あげたいなというときにはかなり良いのではと思います. YOLO-RDのように外部知識を利用するという発想は今後色々なところで使われていくかもしれません.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>画像認識モデルの性能をあげるためのTips</title>
        <link>https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%80%A7%E8%83%BD%E3%82%92%E3%81%82%E3%81%92%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AEtips/</link>
        <pubDate>Sat, 13 Mar 2021 00:00:00 +0000</pubDate>
        
        <guid>https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%80%A7%E8%83%BD%E3%82%92%E3%81%82%E3%81%92%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AEtips/</guid>
        <description>&lt;p&gt;画像分類モデルを作っているときに予測精度をあげるのに役に立ったなぁという方法の一覧のメモです。
簡単にできるものから順に紹介しているつもりです。&lt;/p&gt;
&lt;h1 id=&#34;convolutionとfc層との橋渡しにはglobal-averaging-poolingを使う&#34;&gt;ConvolutionとFC層との橋渡しにはGlobal Averaging Poolingを使う
&lt;/h1&gt;&lt;p&gt;ネットにある転移学習の例をコピペすると、畳み込み層の出力を1次元にするところでFlattenを使ってたりします。
でも実はGlobal Averaging Poolingを使ったほうが精度が良くなるかもしれません。&lt;br&gt;
精度を改善するのもそうですが、モデルのパラメーターを大きく減らせることも非常に大きい恩恵だったりもします。&lt;/p&gt;
&lt;h1 id=&#34;efficientnetはnoisy-student版を使う&#34;&gt;EfficientNetはNoisy Student版を使う
&lt;/h1&gt;&lt;p&gt;転移学習に素のEfficientNetを利用している方は多いと思いますが、Noisy Stundent版の重みを用いて転移学習することでさらに性能があがるかもしれません。&lt;/p&gt;
&lt;p&gt;TensorFlowであれば、こちらのレポジトリが利用可能です。
&lt;a class=&#34;link&#34; href=&#34;https://github.com/qubvel/efficientnet&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;https://github.com/qubvel/efficientnet&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;label-smoothingを使う&#34;&gt;Label Smoothingを使う
&lt;/h1&gt;&lt;p&gt;1か0かのハードラベルではなく、ソフトラベルを使って過学習を抑える方法です。
TensorFlowだと、簡単に使えます。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;tf&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;keras&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;losses&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;categorical_crossentropy&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;y&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;y_hat&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;                                         &lt;span class=&#34;n&#34;&gt;label_smoothing&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mf&#34;&gt;0.1&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;learning-rate-schedulerを使う&#34;&gt;Learning Rate Schedulerを使う
&lt;/h1&gt;&lt;p&gt;学習率のスケジューラーを利用してみると、精度が良くなるかもしれません。&lt;br&gt;
例えば、lossが下がりきったタイミングで学習率を0.1倍にしてみると、lossが少し落ちたりします。&lt;br&gt;
性能が変わらないことも多いので、あまり期待しないほうが良いかも。&lt;/p&gt;
&lt;h1 id=&#34;randaugmentを使う&#34;&gt;RandAugmentを使う
&lt;/h1&gt;&lt;p&gt;RandAugmentは各画像に対して、ランダムにAugmentをいくつか利用する方法です。
RandAugmentのパラメーターはいくつのAugmentを利用するかと各Agumentでのパラメーターを制御するための値の2つのみになります。そのため、Augmentに関するパラメーターのグリッドサーチも一応可能です。&lt;br&gt;
&amp;ldquo;パラメーターがたったの2つでいいの！？&amp;ldquo;と普通の人は効果を疑うかと思いますが、実際に試してみるとかなりうまくいきました。&lt;/p&gt;
&lt;p&gt;使うときには、imgaugなどのライブラリを利用すると良いかと思います。次のようにしてRandAugmentを利用可能です。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-Python&#34; data-lang=&#34;Python&#34;&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;kn&#34;&gt;import&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;imgaug.augmenters&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;as&lt;/span&gt; &lt;span class=&#34;nn&#34;&gt;iaa&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;cl&#34;&gt;&lt;span class=&#34;n&#34;&gt;images&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;iaa&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;RandAugment&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;n&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt; &lt;span class=&#34;n&#34;&gt;m&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;mi&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)(&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;images&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;images&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1909.13719&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;論文&lt;/a&gt;によると、AutoAugmentよりも性能が良いという結果が出ています。&lt;/p&gt;
&lt;p&gt;PyTorch用の実装もネットで適当に探せばすぐに見つかります。&lt;/p&gt;
&lt;h1 id=&#34;gridmaskを使う&#34;&gt;GridMaskを使う
&lt;/h1&gt;&lt;p&gt;GridMaskはAugmentの一つで、Cutoutと同じような種類のAugmentになります。
Cutoutと違い、Grid状のMaskをかける方法になります。&lt;br&gt;
問題によっては結構性能があがりました。&lt;br&gt;
実装はググると色々見つかります。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>画像と自然言語でのマルチモーダルなImageBERT</title>
        <link>https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/</link>
        <pubDate>Mon, 24 Feb 2020 19:46:50 +0900</pubDate>
        
        <guid>https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/</guid>
        <description>&lt;img src="https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8.png" alt="Featured image of post 画像と自然言語でのマルチモーダルなImageBERT" /&gt;&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;最近Microsoftから発表されたImageBERTについて紹介します。&lt;br&gt;
ImageBERTはBERTの入力に自然言語だけではなく、画像も受け付けるようにしたマルチモーダルなモデルです。
また論文ではモデルのアーキテクチャだけではなく、学習方法にも新たな提案がされています。&lt;br&gt;
実験ではImage-to-Sentenceでの検索とSentence-to-Imageの検索タスクでSOTAが示されています。&lt;/p&gt;
&lt;p&gt;論文：&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2001.07966&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;アーキテクチャ&#34;&gt;アーキテクチャ
&lt;/h1&gt;&lt;p&gt;ImageBERTのアーキテクチャは以下のとおりです。
&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8.png&#34;
	width=&#34;2040&#34;
	height=&#34;966&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8_hu4782820121360725840.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8_hu4750603262513794147.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;211&#34;
		data-flex-basis=&#34;506px&#34;
	
&gt;
テキストの入力と画像の入力で分けて説明します。
なお、論文中では画像のcaptioningのデータセットを用いています。&lt;/p&gt;
&lt;h2 id=&#34;テキストの入力&#34;&gt;テキストの入力
&lt;/h2&gt;&lt;p&gt;テキストは通常のBERTのようにsubwordに分割して、それらのembeddingを入力します。
BERTでは2つの文を与えるときに、1つ目の文か2つ目の文かを識別する情報をsubwordのembeddingに加えますが、ImageBERTでも同じように画像か文かを識別する情報を加えます。図でいうところのSegment Embeddingになります。&lt;br&gt;
また、文の位置情報もBERTやTransformerでは与える必要があり、ImageBERTでも位置情報を加えます。しかし、ここではtokenの順番を昇順に与えるというシンプルなやり方のようです。これは図中のSequence Position Embeddingになります。&lt;/p&gt;
&lt;h2 id=&#34;画像の入力&#34;&gt;画像の入力
&lt;/h2&gt;&lt;p&gt;画像はそのままモデルに入力するのではなく、FasterRCNNで物体検出をして、検出された箇所の特徴量をそれぞれ入力する形になります（画像の特徴量はsubwordのembeddingと同じ次元に射影します）。&lt;br&gt;
テキストの場合と同じようにSegment EmbeddingとSequence Position Embeddingも与えるのですが、Sequence Position Embeddingはテキストの場合とは与え方が異なります。テキストの場合にはsubwordに順序がありましたが、画像中の物体には順序がありませんので、すべて同じSequence Position Embeddingを与えます。&lt;/p&gt;
&lt;p&gt;また、これら以外にPosition Embeddingというものも与えます。Position Emebeddingは以下で与えられるベクトルをsubwordのembeddingと同じ次元に射影したものです。
$$ c = \begin{pmatrix} \frac{x_{tl}}{W}, \frac{y_{tl}}{H}, \frac{x_{br}}{W}, \frac{y_{br}}{H}, \frac{(x_{br} - x_{tl}) (y_{br} - y_{tl}) }{WH} \end{pmatrix}.$$
ここで、$x_{tl}, y_{tl},  x_{br}, y_{br}$はそれぞれ物体の左上の$x$と$y$、右下の$x$と$y$座標になります。$W$と$H$は入力画像の横と縦の大きさです。
つまり、$c$は物体の位置と面積の割合の情報になります。&lt;/p&gt;
&lt;h1 id=&#34;事前学習のタスク&#34;&gt;事前学習のタスク
&lt;/h1&gt;&lt;p&gt;ImageBERTでは事前学習に次の4つタスクを解きます。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Masked Language Modeling (MLM)
これは通常のBERTと同じように、入力されるsubwordをランダムにマスクし、マスクされた単語を予測するようなタスクです。&lt;/li&gt;
&lt;li&gt;Masked Object Classification (MOC)
これはMLMの画像版のタスクです。検出された物体をランダムにマスクし、マスクされた物体のラベルを予測するようなタスクです。正解ラベルはFaster-RCNNで求まったラベルとしています。&lt;/li&gt;
&lt;li&gt;Masked Region Feature Regression (MRFR)
MOCはラベルを予測するようなタスクですが、MRFRはマスクされた物体の箇所の特徴量を予測するタスクです。&lt;/li&gt;
&lt;li&gt;Image-Text Matching (ITM)
入力テキストと画像が対応しているかを予測するタスクです。ランダムに画像を選ぶことで、対応していないテキストと画像のペアを作っています。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;マルチステージの事前学習&#34;&gt;マルチステージの事前学習
&lt;/h1&gt;&lt;p&gt;ImageBERTでは事前学習をデータセット単位で別々におこないます。実験結果で書かれていますが、別々にすることで性能が大きく変わります。
以下の図のように最初にLarge-Scale Weak-supervised Image-Text Data（これは次に説明します）
で事前学習をし、その次にConceptual CaptionsとSBU Captionsのデータセットで事前学習をします。最後にfinetuningをおこないます。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/8de8eb1666c40d3555f915cbdbcb5ff5.png&#34;
	width=&#34;1826&#34;
	height=&#34;610&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/8de8eb1666c40d3555f915cbdbcb5ff5_hu8942433182479781656.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/8de8eb1666c40d3555f915cbdbcb5ff5_hu2075952230978656483.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;299&#34;
		data-flex-basis=&#34;718px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;large-scale-weak-supervised-image-text&#34;&gt;Large-Scale Weak-supervised Image-Text
&lt;/h1&gt;&lt;p&gt;大量の画像とテキストのペアをweb上からクローリングして、事前学習に使っています。
論文中では画像とテキストのペアが10M個あるこのデータセットをLAIT (Large-scale weAk-supervised Image-Text) と読んでいます。&lt;/p&gt;
&lt;p&gt;LAITでは、webページ上の画像とHTMLのALTあるいはTITLEタグのテキストをcaptionとして対応付けています。単純にこれらを取得してくると、当然ノイジーなデータが多く含まれることになります。例えば、言語が英語ではない、画像のサイズが小さすぎる、現実の画像ではないなどが該当します。このようなペアをルールベースあるいは機械学習のモデルを用いてフィルタリングしています。&lt;br&gt;
一連の流れは以下のようになります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/7b99180c8ef8ce273378f8b2f51d9b85.png&#34;
	width=&#34;1098&#34;
	height=&#34;1540&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/7b99180c8ef8ce273378f8b2f51d9b85_hu11487969349023570822.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/7b99180c8ef8ce273378f8b2f51d9b85_hu10371836243830688777.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;71&#34;
		data-flex-basis=&#34;171px&#34;
	
&gt;&lt;/p&gt;
&lt;h1 id=&#34;実験&#34;&gt;実験
&lt;/h1&gt;&lt;h2 id=&#34;準備&#34;&gt;準備
&lt;/h2&gt;&lt;p&gt;事前学習したImageBERTはMSCOCOとFlickr30kを用いてfinetuningしています。&lt;br&gt;
finetuningするさいはITMのように、入力されたテキストと画像がペアであるかを正しく予測できるように学習していきます。事前学習でのITMに用いた出力のtokenを射影してfinetuningします。&lt;/p&gt;
&lt;h2 id=&#34;結果&#34;&gt;結果
&lt;/h2&gt;&lt;h3 id=&#34;性能比較&#34;&gt;性能比較
&lt;/h3&gt;&lt;p&gt;以下の表は他手法との性能の比較です。
&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/44d5e2d6fe2b015f68caecd6f1be4b3a.png&#34;
	width=&#34;1424&#34;
	height=&#34;656&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/44d5e2d6fe2b015f68caecd6f1be4b3a_hu17778336402145431527.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/44d5e2d6fe2b015f68caecd6f1be4b3a_hu5817616310874181610.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;217&#34;
		data-flex-basis=&#34;520px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Image Retrievalは与えられたテキストと対応づく画像を検索するタスク、Sentence Retrievalは与えられた画像から対応づくテキストを検索するタスクです。それぞれRecallで評価されています。&lt;/p&gt;
&lt;p&gt;他手法と比べて、ImageBERTは性能が良いことがわかります。全体傾向として、Sentence Retrievalは性能が高いですね。&lt;/p&gt;
&lt;h3 id=&#34;マルチステージの効果&#34;&gt;マルチステージの効果
&lt;/h3&gt;&lt;p&gt;以下の表がマルチステージの学習の効果をあらわしています。
&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/871f6199938140733fd9eea8bd43ac69.png&#34;
	width=&#34;1014&#34;
	height=&#34;356&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/871f6199938140733fd9eea8bd43ac69_hu15549283014864703573.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/871f6199938140733fd9eea8bd43ac69_hu2460027981471294082.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;284&#34;
		data-flex-basis=&#34;683px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;上4つがそれぞれのデータセットのみで学習した場合、一番下がLAITで事前学習したあとにConceptual CaptionsとSBU Captionsで学習した場合の結果になります。&lt;br&gt;
明らかにマルチステージで学習することに優位性がありますね。この結果はImageBERTに限らず参考になりそうです。&lt;/p&gt;
&lt;h3 id=&#34;ablation-study&#34;&gt;ablation study
&lt;/h3&gt;&lt;p&gt;ablation studyです。
&lt;img src=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/39a03c0d5cb86482b2603a07545d2d40.png&#34;
	width=&#34;1404&#34;
	height=&#34;650&#34;
	srcset=&#34;https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/39a03c0d5cb86482b2603a07545d2d40_hu6442853037209584085.png 480w, https://opqrstuvcut.github.io/blog/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/39a03c0d5cb86482b2603a07545d2d40_hu17623035205993790707.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;216&#34;
		data-flex-basis=&#34;518px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;それぞれ次を意味しています。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;画像全体の特徴量もImageBERTに与えるケースで性能が変わるかを示しています。どちらが良い性能化は一概にいえない結果になっています。&lt;/li&gt;
&lt;li&gt;MRFRの有無で性能が変わるかを示しています。すべてのケースでMRFRを解いたほうが良い性能になっています。&lt;/li&gt;
&lt;li&gt;Faster-RCNNで検出された物体を最大いくつ入力するかをあらわしています。最大36個与えるときより最大100個としたときのほうが高い性能になっています。&lt;/li&gt;
&lt;li&gt;finetuningのロスとしてどれがいいかを示しています。Binaryは正しいテキストと画像のペアか否かの2分類をBinary Cross Entropyを使って解いたケースをあらわします。2分類問題として解くのが一番良い性能になっています。理由はちょっとわかりません。&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;まとめ&#34;&gt;まとめ
&lt;/h1&gt;&lt;p&gt;マルチモーダルのモデルであるImageBERTを紹介しました。&lt;br&gt;
事前学習したモデルが公開されていれば色々試したいですが、自分で1から学習する気にはなかなかなりませんね。&lt;br&gt;
ImageBERTの事前学習モデルが公開されれば、以前公開した&lt;a class=&#34;link&#34; href=&#34;https://qrunch.net/@opqrstuvcut/entries/O37ZGE1YhN5or3Oi&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;記事&lt;/a&gt;と同じ要領で画像からcaptionを生成できるんじゃないかなと思ってます。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
