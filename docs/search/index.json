[{"content":"たまにTensorBoardを使うときに、ホスト環境などにTensorBoardを入れるより、それ用のコンテナをたてたくなったので、そのメモです。\nDocker Imageは以下のとおり。\nFROM python:3.8 RUN pip install tensorflow WORKDIR /logs ENTRYPOINT [\u0026quot;tensorboard\u0026quot;, \u0026quot;--logdir\u0026quot;, \u0026quot;/logs\u0026quot;, \u0026quot;--host\u0026quot;, \u0026quot;0.0.0.0\u0026quot;] 次のような感じでdocker buildとdocker runします。\n$ docker build -t tensorboard . $ docker run -it --rm -p 10000:6006 -v $PWD/logs:/logs tensorboard -vに指定するhost側のlogのディレクトリのパスと-pに指定するportは適当に変更する。\n","date":"2020-11-18T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/p/tensorboard%E3%81%AEdocker-image/","title":"TensorBoardのDocker Image"},{"content":"最近では実験用の環境なんかもDockerコンテナ上に用意することも多いです。\ndocker-composeも使うわけですが、たまにdocker-composeのbuildがいつになってもはじまらないことがあります。\nBuilding xxxがずっと表示されて、そこから進まないわけです。\n以前も同じケースに出くわしたのにすぐ思い出せなかったので、備忘録的に残りしておきます（似た記事はネットにたくさんありますが）。\ndocker-composeのbuildがはじまらない原因 学習に使うデータのように重いファイルを置いてあるディレクトリでdocker-composeをおこなうのが原因です。なぜこれが原因でbuildがはじまらないかといえば、Docker Imageのbuildをするときに、Dockerfileがあるディレクトリ上のデータはすべてDockerのデーモンに渡されるためです。\n全部のファイルをデーモンに渡そうとするので、学習データなんかがDockerfileと同じディレクトリ上にあると、それらの重たいファイルも渡そうとしてしまい、いつになってもbuildが進まないわけですね。\n対処方法  .dockerignoreにデーモンに渡してほしくないディレクトリ、ファイルを指定する ファイルパスを工夫する（でもdockerignoreを使うのが一番いいんじゃないでしょうか）  ","date":"2020-11-08T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/p/docker-compose%E3%81%AEbuild%E3%81%8C%E3%81%AF%E3%81%98%E3%81%BE%E3%82%89%E3%81%AA%E3%81%84%E3%81%A8%E3%81%8D/","title":"docker-composeのbuildがはじまらないとき"},{"content":"GPUサーバー上でTensorFlowを動かすアプリを作成し、nginxとの間にはuWSGIを挟む構成にしていたところ、次のエラーが出てしまいました。\nFailed to get device properties, error code: 3 他の記事の引用になってしまいますが、エラーメッセージ自体をググっても解決できなかったので、メモ程度に載せておきます。\n原因 確かとは断言できないのですが、次の記事にかかれていることが怪しいと推測しました。 https://keng000.hatenablog.com/entry/2020/05/05/092425\nつまり、マルチスレッドでのモデルの読み込み方が良くないのかと。\n対処 記事に書かれている通り、uWSGIのiniファイルで次のように追記しました。\n[uwsgi] lazy-apps = true とりあえずこれで解決しました。\n","date":"2020-11-08T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/p/gpu%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E3%81%AEtensorflow-uwsgi%E3%81%A7failed-to-get-device-properties-error-code-3/","title":"GPUサーバーでのTensorFlow + uWSGIでFailed to get device properties, error code: 3"},{"content":"本記事はQrunchからの転載です。\n 最近自然言語処理をよくやっていて、BERTを使うことも多いです。 BERTの性能は高く素晴らしいのですが、実際使う上では、私のような計算リソース弱者には辛いところがあります。\n例えば、BERTは非常にパラメータ数が多いことで有名ですが、パラメータが多いと、fine-tuningでの学習や推論の時間がかかることや大きめのメモリが積んであるGPUがないと学習ができない、といった部分がネックになりえます。\nBERTのパラメータ数を減らす試みとしてはTinyBERTやDistilBERTによる蒸留を使った手法がありますが、今回紹介するPoor Man’s BERT: Smaller and Faster Transformer ModelsではBERTのTransformerの数を単純に減らすことでパラメータ数を減らしています。\n実際にTinyBERTやDistilBERTと同じことをするのは難しいですが、今回のように層を減らして学習するのは容易にできますので、とても実用性があるのではないかと思います。\n比較実験 論文では12層のTransformerをもつBERTモデルから色々な方法でTransformerを減らし、性能比較をおこなっています。24層をもつ、いわゆるBERT-Largeは、貧乏人にはメモリが足らずにfine-tuningも難しいのです。\n次の図がTransformer層の減らし方の一覧です。  \n各方法の詳細は以下のとおりです。\nTop-Layer Dropping 先行研究によると、BERTの後ろの層は目的関数に特化したような重みになっているようです。つまり、BERTで汎用的に使えるように学習されている部分は前の層ということになります。 このため、後ろの層に関しては減らしても性能がそんなに悪化しないんじゃないかという仮定のもと、BERTの最後から4つあるいは6つのTransformerを削除します。\nEven Alternate Dropping、Odd Alternate Dropping 先行研究によると、BERTの各層では冗長性があります。つまり、隣り合った層の出力は似ているということです。 このため、1個おきにTransformerを削除します。\nContribution based Dropping Alternate Droppingと少し似ていますが、入力と出力があまり変わらないような層を削除するような方法です。 各Transformer層のなかで[CLS]の入力と出力のcosine類似度が大きい傾向にある層をあらかじめ見つけておき、それを削除します。\nSymmetric Dropping もしかすると、12層のTransformerのうち、真ん中のあたりはあまり重要じゃないかもしれません。 ということで、前と後ろは残して真ん中付近のTransformerを削除します。\nBottom-Layer Dropping BERTの最初のほうの層が文脈の理解に重要といわれており、最初のほうを消す理論的な理由はないですが、年のために最初のほうのTransformerを削除したモデルも試します。\n実験 手法間の性能比較 先程示した方法とDistilBERTをGLUEタスクのスコアで比較した結果が以下になります。BERTだけではなくXLNetでも実験してくれています。  \nこれから以下のことが分かります。\n 各方法のスコアは12層あるBertには劣る。 4層減らす分にはBottom-Layer Dropping以外の方法ではそれほど性能に差がでないが、6層減らす場合にはTop-Layer Dropping（最後の6層を消す）が性能劣化が小さい。 Top-Layer Droppingの6層を消した場合はDistilBERTと似たような性能になっている。学習の手間はDistilBERTのほうが圧倒的に大きいので、性能が同程度、計算時間も同程度ならば本手法を使うメリットが大きいです。 XLNetの場合には最後の4層を消したモデルでも12層あるXLNetとほぼ同じ性能が出せる（＝性能劣化が少ない）。  タスクごとの性能変化の検証 次にタスクごとの性能の変化を見ていきます。前の実験から後ろの層を消していくTop-Layer Droppingが良いとわかっているため、Top-Layer Droppingに限って実験がされています。  \n問題によっては6層消してもほとんど変化がなかったりします。\n余談ですが、私が自分で試したある問題では6層消して8ポイント分、4層消して4ポイント分の性能劣化、2層消して2ポイント分の性能劣化になりました。\nタスクごとの性能劣化がおこる層数の検証 タスクごとに後ろを何層削ると1%、2%、3%の性能劣化がおこるのかを示した表です。   ビックリしますが、XLNetは結構層を消しても性能劣化が起こりづらいですね。\nパラメータ数や計算時間比較 学習時間・推論時間は削った層の割合だけおおよそ減ることが予想されますが、実際に計算時間がどれくらい変わったかを示したのが以下の表です。   6層削ったモデルでは学習時間・推論時間の両方でだいたい半分くらいになってますね。\nBERTとXLNetの層数での比較 BERTとXLNetのTransformerの数を変えると、どう性能が変化するかを示したのが以下の図です。   なんとXLNetは7層にするあたりまではほどんど性能の変化がありません。BERTは層を減らすと順調に性能が悪化します。\n上記の話には実験的な根拠があり、それを示したのが以下の図です。   これはBERTとXLNetの事前学習モデルとfine-tunedモデル間で同じ層同士の出力のcosine類似度を計算した結果になります。つまり、小さい値になっているほど、fine-tuningで出力が大きく変わるような学習がおこなわれたことになります。 BERTの場合には後ろの層ほど大きな変化があることがわかります。またfine-tuningしても前の方の層はほとんど変わっていませんね。 一方でXLNetの場合には前の層の変化がないのはBERTと一緒ですが、後ろの層に関してもあまり変化がありません（もちろん12層目だけは大きく変わります）。つまり、問題を解くときにあまり8層以降は重要じゃないのではと考えられます。\n感想 私のような貧乏人には大変ありがたい論文でした。 計算リソースがあまりない方は使ってみましょう！\n","date":"2020-06-21T15:22:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/ade970e39b6211acf56131ea9aadba79_hua9848c14f6fc76924d36dd77c390b808_485748_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/","title":"貧乏人なのでPoor Man’s BERTを読んで解説"},{"content":"本記事はQrunchからの転載です。\n AWSのLambda（Python）からPostgresを利用するためのライブラリの使い方のメモです。何もトラブルなく使えましたが、一応。 ライブラリのレポジトリはこちらです。\n ライブラリのclone  git clone https://github.com/jkehler/awslambda-psycopg2.git  適切な名前にリネーム LambdaでPython3.6を利用する場合にはcloneしてきたレポジトリにあるpsycopg2-3.6をpsycopg2にリネームします。あるいはPython3.7を利用する方はpsycopg2-3.7をpsycopg2にリネームします。\n  適切な位置への配置\npsycopg2をLambdaにデプロイするコードと同じディレクトリに配置します。 例： lambda/hoge.pyというPythonスクリプトをデプロイする場合にはlambdaディレクトリ以下にpsycopg2を配置する。\n  Lambdaにデプロイする！\n  ","date":"2020-05-04T13:35:16+09:00","permalink":"https://opqrstuvcut.github.io/blog/p/aws%E3%81%AElambda%E3%81%8B%E3%82%89postgres%E3%82%92%E5%88%A9%E7%94%A8/","title":"AWSのLambdaからPostgresを利用"},{"content":"本記事はQrunchからの転載です。\n 関数が上に凸であることの必要十分条件はヘッセ行列が半負定値であることです。ネット上だと日本語でまとまっている文献があんまりないかもと思ったので、今回はこの証明をまとめます。 なお、関数が下に凸のときにはヘッセ行列は半正定値となります。上に凸の定義を使っているところを下に凸の定義に置き換え、正定値を負定値に置き換えれば、同じ議論が可能です。 また出てくる関数$f$は暗黙的に定義域で2階微分可能としています。\n定義 関数が上に凸の定義 関数$f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$が上に凸とは任意の元$\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)} \\in \\mathbb{R}^{n}$と任意の$t \\in [0,1]$に対して以下が成り立つことを指します。 $$ f(t\\mathbf{x}^{(2)} + (1 -t)\\mathbf{x}^{(1)}) \\geq tf(\\mathbf{x}^{(2)}) + (1 -t) f(\\mathbf{x}^{(1)}).$$\nヘッセ行列の定義 関数$f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$のヘッセ行列$H$を以下のように定義します。 $$H_f = \\nabla^2 f = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026amp;\\frac{\\partial^2 f}{\\partial x_1\\partial x_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\cr \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2} \u0026amp; \\dots \u0026amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\cr \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\cr \\frac{\\partial^2 f}{\\partial x_n\\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial^2 f}{ \\partial x_n^2} \\end{pmatrix}.$$\n行列の半負定値性の定義 ある対称行列$A \\in \\mathbb{R}^{n \\times n}$に対して任意のベクトル$\\mathbf{x} \\in \\mathbb{R}^n$が次を満たすとき、$A$を半負定値といいます。 $$ \\mathbf{x}^T A \\mathbf{x} \\leq 0.$$\n証明 本題の証明に使うため、以下を先に証明しておきます。\n凸関数の一次条件 証明$(\\Rightarrow)$ 上に凸であることの定義から、 $$ \\begin{aligned} f(t\\mathbf{x}^{(2)} + (1 -t)\\mathbf{x}^{(1)}) \\geq \u0026amp; tf(\\mathbf{x}^{(2)}) + (1 -t) f(\\mathbf{x}^{(1)}) \\cr f(\\mathbf{x}^{(1)} + t(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)})) \\geq \u0026amp; f(\\mathbf{x}^{(1)}) + t(f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)})) \\cr \\frac{f(\\mathbf{x}^{(1)} + t(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)})) - f(\\mathbf{x}^{(1)}) }{t} \\geq \u0026amp; f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)}) \\end{aligned} $$ となります。 ここで、$g(t) = f(\\mathbf{x}^{(1)} + t(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)}))$とおくと、以下が成り立ちます。 $$ \\begin{aligned} \\frac{g(t) - g(0)}{t} \\geq \u0026amp; f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)}) \\end{aligned}. $$ さらに$t \\rightarrow 0$とすれば、以下のようになります。 $$ \\begin{aligned} g'(0) \\geq \u0026amp; f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)}) \\end{aligned}. $$ $g'(0)$がなんであるかというと、これは単純に計算すればよく、 $$g'(0) = \\left.\\frac{{\\rm d}g}{{\\rm d}t}\\right|_{t=0} = \\nabla f(\\mathbf{x}^{(1)})^ T(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)})$$ となります。 以上から、 $$ f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)})\\leq \\nabla f(\\mathbf{x}^{(1)})^T(\\mathbf{x}^{(2)}- \\mathbf{x}^{(1)})$$ が示されました。\n$(\\Leftarrow)$ 適当な$0\\leq t \\leq 1$を用いて$\\mathbf{z}=t\\mathbf{x}^{(2)} + (1-t)\\mathbf{x}^{(1)} $とおきます。仮定から以下が成り立ちます。 $$ \\begin{aligned} f(\\mathbf{x}^{(2)}) \\leq \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(2)}- \\mathbf{z}), \\cr f(\\mathbf{x}^{(1)}) \\leq \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(1)}- \\mathbf{z}). \\end{aligned} $$ 1つめの式に$t$を掛け、2つめの式に$1-t$を掛けて足し合わせることで以下のようになります。 $$ \\begin{aligned} tf(\\mathbf{x}^{(2)}) + (1 -t )f(\\mathbf{x}^{(1)}) \\leq \u0026amp; tf(\\mathbf{z}) + (1-t)f(\\mathbf{z}) + t\\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(2)}- \\mathbf{z}) + (1-t) \\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(1)}- \\mathbf{z}) \\cr = \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(t(\\mathbf{x}^{(2)}-\\mathbf{z}) + (1-t) (\\mathbf{x}^{(1)}- \\mathbf{z})) \\cr = \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(t\\mathbf{x}^{(2)} + (1-t) \\mathbf{x}^{(1)}-\\mathbf{z}) \\cr = \u0026amp; f(\\mathbf{z}) \\cr = \u0026amp; f(t\\mathbf{x}^{(2)} + (1-t)\\mathbf{x}^{(1)} ) . \\cr \\end{aligned} $$ 以上から逆も証明できました。\nこの証明の参考： http://mathgotchas.blogspot.com/2011/10/proof-for-first-order-condition-of.html\nテイラーの定理 本題の証明に用いるため、テイラーの定理の特別な場合を紹介しておきます。 テイラーの定理の証明はここではしません。また実際は成り立つための条件がありますが、ここでは以下のように利用できるとします。\n関数が上に凸であることの必要十分条件がヘッセ行列が半負定値であることの証明 本題の証明です。\n証明$(\\Rightarrow)$ 適当な元$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{n}$を考えます。テイラーの定理より以下が成り立ちます。 $$ \\begin{aligned} f(\\mathbf{x} + \\mathbf{y}) = \u0026amp;f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y} + \\frac{1}{2} \\mathbf{y}^T H_f(\\mathbf{x}) \\mathbf{y} + o(||\\mathbf{y}||^2_2).\\tag{1} \\end{aligned}$$ また、$f$が上に凸であるという仮定から、一次条件より $$ f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y}\\tag{2}$$ が成り立ちます。式(1)と式(2)から、 $$ \\frac{1}{2} \\mathbf{y}^T H_f(\\mathbf{x})\\mathbf{y} + o(||\\mathbf{y}||^2_2) \\leq 0. $$ $\\mathbf{y} = \\alpha \\mathbf{z} , ||\\mathbf{z}||_2 = 1 $とすると、\n$$ \\begin{aligned} \\frac{1}{2} \\alpha^2 \\mathbf{z}^T H_f(\\mathbf{x})\\mathbf{z} + o(\\alpha^2) \\leq \u0026amp; 0 \\cr \\frac{1}{2} \\mathbf{z}^T H_f(\\mathbf{x})\\mathbf{z} + \\frac{o(\\alpha^2)}{\\alpha^2} \\leq \u0026amp; 0 . \\end{aligned} $$ このとき、$\\alpha \\rightarrow 0$とすると、$o(x^n)$の定義から以下が成り立ちます。 $$ \\frac{1}{2} \\mathbf{z}^T H_f(\\mathbf{x}) \\mathbf{z} \\leq 0. $$ $\\mathbf{y}$は任意のベクトルでしたので、$H_f(\\mathbf{x})$は半負定値となります。\u001c よって$f$が上に凸であるとき、ヘッセ行列$H_f(\\mathbf{x})$は任意の$\\mathbf{x}$で半負定値であることが示されました。\n$(\\Leftarrow)$ テイラーの定理より、ある$0 \u0026lt; t \u0026lt; 1$を用いて以下が成り立ちます。 $$ \\begin{aligned} f(\\mathbf{x} + \\mathbf{y}) = \u0026amp;f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y} + \\frac{1}{2} \\mathbf{y}^T H_f(\\mathbf{x} + t\\mathbf{y} ) \\mathbf{y}. \\end{aligned}$$ ヘッセ行列$H_f(\\mathbf{x} + t\\mathbf{y} ) $が半負定値であるため、明らかに $$ f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y} $$ が成り立ちます。これは先程示した一次条件であらわれる式と同じです。 以上から、ヘッセ行列$H_f(\\mathbf{x})$が任意の$\\mathbf{x}$で半負定値であるとき、$f$が上に凸であることが示されました。\n","date":"2020-03-11T00:08:01+09:00","permalink":"https://opqrstuvcut.github.io/blog/p/%E9%96%A2%E6%95%B0%E3%81%8C%E4%B8%8A%E3%81%AB%E5%87%B8%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E5%BF%85%E8%A6%81%E5%8D%81%E5%88%86%E6%9D%A1%E4%BB%B6%E3%81%AF%E3%83%98%E3%83%83%E3%82%BB%E8%A1%8C%E5%88%97%E3%81%8C%E5%8D%8A%E8%B2%A0%E5%AE%9A%E5%80%A4%E3%81%AE%E8%A8%BC%E6%98%8E/","title":"関数が上に凸であることの必要十分条件はヘッセ行列が半負定値の証明"},{"content":"本記事はQrunchからの転載です。\n みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。\nKL divergence KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。 具体的には次のように定義されます。 $$ KL(p||q) = \\int p(\\mathbf{x}) \\log \\left(\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\right) {\\rm d\\mathbf{x}}. $$ $p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。 なお、$KL(p||q) \\geq 0$が成り立つことに注意してください。\nKL divergenceの最小化問題 KL(p||q)のケース 仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。\n前述したKL divergenceの定義をみてみると、$p(\\mathbf{x})$が0でない値をもつ領域では$q(\\mathbf{x})$も$p(\\mathbf{x})$に近い値かあるいは$p(\\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような$q$は$p$全体をカバーするように広がる分布になると考えられます。\nKL(q||p)のケース 次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は $$ KL(q||p) = \\int q(\\mathbf{x}) \\log \\left(\\frac{q(\\mathbf{x})}{p(\\mathbf{x})}\\right) {\\rm d\\mathbf{x}}$$ ですね。 $KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\\mathbf{x})$が0に近いような領域で$q(\\mathbf{x})$が小さくなるようにすればよいです。$p(\\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような$q$は$p$の値が大きいところに集中するような分布になると考えられます。\n実験 上記の話が成り立つのかを実験してみます。\n実験準備 $p(\\mathbf{x})$は次のようにします。\n$$p(\\mathbf{x}|\\mathbf{u},\\Sigma)=\\frac{1}{{2\\pi}|\\Sigma|^{1/2}}\\exp\\biggl[-\\frac{(\\mathbf{x}-\\mathbf{u})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\mathbf{u})}{2}\\biggr].$$ また$\\mathbf{u}$と$\\Sigma$はそれぞれ $$\\mathbf{u} = \\begin{pmatrix} 0.3 \\\\ -0.2 \\end{pmatrix}, \\Sigma =\\begin{pmatrix} 0.9\u0026amp;-0.7 \\\\ -0.7 \u0026amp; 0.9 \\end{pmatrix}$$ とました。 $p$を確率密度毎に色わけして表示してみると、以下のとおりです。  \nまた$q(\\mathbf{x})$は次のようにします。 $$q(\\mathbf{x}|\\mathbf{s},\\alpha)=\\frac{1}{{2\\pi}\\alpha}\\exp\\biggl[-\\frac{(\\mathbf{x}-\\mathbf{s})^{\\top}(\\mathbf{x}-\\mathbf{s})}{2\\alpha}\\biggr].$$\n$q$のうち、$\\mathbf{s}$と$\\alpha$が最適化するべきパラメータです。 $q$は同心円状に確率密度をもつ分布になりますので、パラメータをどうやっても$p$と一致することはできません。\n実験結果 $KL(p||q)$を最小化したケースをまず示します。   白い線が$p$の等高線です。色分けされて表示されているのが、$q$の確率密度になります。 先程の話のとおり、$q$は$p$に対して広がった分布になっていることがわかります。\n次に$KL(q||p)$を最小化したケースです。   こちらも先程の話のとおり、$q$は$p$の値が大きい箇所に集中した分布になっています。\nまとめ 今回は$q$を$p$に近づける話に限定しましたが、KL divergenceに与える分布を入れ替えると結果が変わるケースが多そうだなと想像できたんじゃないかと思います。 頭の片隅に留めておくと役立つかもしれません。\n実験に使ったスクリプト #! /usr/bin/env python import argparse import os import logging import matplotlib.pyplot as plt import numpy as np from scipy.stats import multivariate_normal import torch from torch.distributions import MultivariateNormal import torch.optim as optim def parse_argument(): parser = argparse.ArgumentParser(\u0026#34;\u0026#34;, add_help=True) parser.add_argument(\u0026#34;-o\u0026#34;, \u0026#34;--output_dir\u0026#34;, type=str) args = parser.parse_args() return args def make_data(border=5): xy = np.mgrid[-border:border:0.005, -border:border:0.005] grids = xy.shape[1] x = xy[0] y = xy[1] xy = xy.reshape(2, -1).T p_pdf = multivariate_normal.pdf(xy, np.array([0, 0]), np.array([[.9, -.7], [-.7, .9]])) return xy, x, y, p_pdf, grids def kl_div(p, q): finite_index = ~((q == 0.) | (torch.isinf(p))) q = q[finite_index] logq = torch.log(q) return torch.sum(q * (logq - p[finite_index])) def optimize_q(xy, p_pdf, invert=False): p_pdf = torch.tensor(p_pdf, requires_grad=False, dtype=torch.float32) mean = torch.tensor([0.3, -0.2], requires_grad=True) cov_coeff = torch.tensor(1., requires_grad=True) xy = torch.tensor(xy, requires_grad=False, dtype=torch.float32) optimizer = optim.SGD([mean, cov_coeff], lr=.000005) if not invert: p_pdf = torch.log(p_pdf) for i in range(25): optimizer.zero_grad() cov = torch.eye(2, 2) * cov_coeff norm_torch = MultivariateNormal(mean, cov) q_pdf = norm_torch.log_prob(xy) if invert: loss = kl_div(q_pdf, p_pdf) else: q_pdf = torch.exp(q_pdf) loss = kl_div(p_pdf, q_pdf) loss.backward() optimizer.step() logging.info( f\u0026#34;[{i + 1}iter] loss:{loss}, mean:{mean}, cov_alpha:{cov_coeff}\u0026#34;) return mean.detach().numpy(), cov_coeff.detach().numpy() def plot_dist(x, y, p_pdf, border, output_path, contour=None): plt.pcolormesh(x, y, p_pdf, cmap=\u0026#34;nipy_spectral\u0026#34;) plt.colorbar() if contour is not None: plt.contour(x, y, contour, colors=\u0026#34;white\u0026#34;, levels=5) plt.savefig(output_path) plt.close() if __name__ == \u0026#34;__main__\u0026#34;: logger = logging.basicConfig(level=logging.INFO) args = parse_argument() output_dir = args.output_dir border = 5 xy, x, y, p_pdf, grids = make_data(border) data_dist_path = os.path.join(output_dir, \u0026#34;data_dist.png\u0026#34;) plot_dist(x, y, p_pdf.reshape(grids, grids), border, data_dist_path) for invert in [True, False]: mean, cov_coeff = optimize_q(xy, p_pdf, invert=invert) output_path = os.path.join( output_dir, f\u0026#34;gauss_m{mean}_c{cov_coeff}.png\u0026#34;) q_pdf = multivariate_normal.pdf(xy, mean, np.eye(2, 2) * cov_coeff) plot_dist(x, y, q_pdf.reshape(grids, grids), border, output_path, contour=p_pdf.reshape(grids, grids)) logging.info(f\u0026#34;{output_path} is saved.\u0026#34;) ","date":"2020-03-02T18:01:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/983be7a190c4aaf3488cd6c3d4471158_hucd343461d408ae8b77fea0b56735bfcd_50111_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/","title":"KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？"},{"content":"本記事はQrunchからの転載です。\n 最近Microsoftから発表されたImageBERTについて紹介します。\nImageBERTはBERTの入力に自然言語だけではなく、画像も受け付けるようにしたマルチモーダルなモデルです。 また論文ではモデルのアーキテクチャだけではなく、学習方法にも新たな提案がされています。\n実験ではImage-to-Sentenceでの検索とSentence-to-Imageの検索タスクでSOTAが示されています。\n論文：ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data\nアーキテクチャ ImageBERTのアーキテクチャは以下のとおりです。   テキストの入力と画像の入力で分けて説明します。 なお、論文中では画像のcaptioningのデータセットを用いています。\nテキストの入力 テキストは通常のBERTのようにsubwordに分割して、それらのembeddingを入力します。 BERTでは2つの文を与えるときに、1つ目の文か2つ目の文かを識別する情報をsubwordのembeddingに加えますが、ImageBERTでも同じように画像か文かを識別する情報を加えます。図でいうところのSegment Embeddingになります。\nまた、文の位置情報もBERTやTransformerでは与える必要があり、ImageBERTでも位置情報を加えます。しかし、ここではtokenの順番を昇順に与えるというシンプルなやり方のようです。これは図中のSequence Position Embeddingになります。\n画像の入力 画像はそのままモデルに入力するのではなく、FasterRCNNで物体検出をして、検出された箇所の特徴量をそれぞれ入力する形になります（画像の特徴量はsubwordのembeddingと同じ次元に射影します）。\nテキストの場合と同じようにSegment EmbeddingとSequence Position Embeddingも与えるのですが、Sequence Position Embeddingはテキストの場合とは与え方が異なります。テキストの場合にはsubwordに順序がありましたが、画像中の物体には順序がありませんので、すべて同じSequence Position Embeddingを与えます。\nまた、これら以外にPosition Embeddingというものも与えます。Position Emebeddingは以下で与えられるベクトルをsubwordのembeddingと同じ次元に射影したものです。 $$ c = \\begin{pmatrix} \\frac{x_{tl}}{W}, \\frac{y_{tl}}{H}, \\frac{x_{br}}{W}, \\frac{y_{br}}{H}, \\frac{(x_{br} - x_{tl}) (y_{br} - y_{tl}) }{WH} \\end{pmatrix}.$$ ここで、$x_{tl}, y_{tl}, x_{br}, y_{br}$はそれぞれ物体の左上の$x$と$y$、右下の$x$と$y$座標になります。$W$と$H$は入力画像の横と縦の大きさです。 つまり、$c$は物体の位置と面積の割合の情報になります。\n事前学習のタスク ImageBERTでは事前学習に次の4つタスクを解きます。\n Masked Language Modeling (MLM) これは通常のBERTと同じように、入力されるsubwordをランダムにマスクし、マスクされた単語を予測するようなタスクです。 Masked Object Classification (MOC) これはMLMの画像版のタスクです。検出された物体をランダムにマスクし、マスクされた物体のラベルを予測するようなタスクです。正解ラベルはFaster-RCNNで求まったラベルとしています。 Masked Region Feature Regression (MRFR) MOCはラベルを予測するようなタスクですが、MRFRはマスクされた物体の箇所の特徴量を予測するタスクです。 Image-Text Matching (ITM) 入力テキストと画像が対応しているかを予測するタスクです。ランダムに画像を選ぶことで、対応していないテキストと画像のペアを作っています。  マルチステージの事前学習 ImageBERTでは事前学習をデータセット単位で別々におこないます。実験結果で書かれていますが、別々にすることで性能が大きく変わります。 以下の図のように最初にLarge-Scale Weak-supervised Image-Text Data（これは次に説明します） で事前学習をし、その次にConceptual CaptionsとSBU Captionsのデータセットで事前学習をします。最後にfinetuningをおこないます。\n \nLarge-Scale Weak-supervised Image-Text 大量の画像とテキストのペアをweb上からクローリングして、事前学習に使っています。 論文中では画像とテキストのペアが10M個あるこのデータセットをLAIT (Large-scale weAk-supervised Image-Text) と読んでいます。\nLAITでは、webページ上の画像とHTMLのALTあるいはTITLEタグのテキストをcaptionとして対応付けています。単純にこれらを取得してくると、当然ノイジーなデータが多く含まれることになります。例えば、言語が英語ではない、画像のサイズが小さすぎる、現実の画像ではないなどが該当します。このようなペアをルールベースあるいは機械学習のモデルを用いてフィルタリングしています。\n一連の流れは以下のようになります。\n \n実験 準備 事前学習したImageBERTはMSCOCOとFlickr30kを用いてfinetuningしています。\nfinetuningするさいはITMのように、入力されたテキストと画像がペアであるかを正しく予測できるように学習していきます。事前学習でのITMに用いた出力のtokenを射影してfinetuningします。\n結果 性能比較 以下の表は他手法との性能の比較です。  \nImage Retrievalは与えられたテキストと対応づく画像を検索するタスク、Sentence Retrievalは与えられた画像から対応づくテキストを検索するタスクです。それぞれRecallで評価されています。\n他手法と比べて、ImageBERTは性能が良いことがわかります。全体傾向として、Sentence Retrievalは性能が高いですね。\nマルチステージの効果 以下の表がマルチステージの学習の効果をあらわしています。  \n上4つがそれぞれのデータセットのみで学習した場合、一番下がLAITで事前学習したあとにConceptual CaptionsとSBU Captionsで学習した場合の結果になります。\n明らかにマルチステージで学習することに優位性がありますね。この結果はImageBERTに限らず参考になりそうです。\nablation study ablation studyです。  \nそれぞれ次を意味しています。\n 画像全体の特徴量もImageBERTに与えるケースで性能が変わるかを示しています。どちらが良い性能化は一概にいえない結果になっています。 MRFRの有無で性能が変わるかを示しています。すべてのケースでMRFRを解いたほうが良い性能になっています。 Faster-RCNNで検出された物体を最大いくつ入力するかをあらわしています。最大36個与えるときより最大100個としたときのほうが高い性能になっています。 finetuningのロスとしてどれがいいかを示しています。Binaryは正しいテキストと画像のペアか否かの2分類をBinary Cross Entropyを使って解いたケースをあらわします。2分類問題として解くのが一番良い性能になっています。理由はちょっとわかりません。  まとめ マルチモーダルのモデルであるImageBERTを紹介しました。\n事前学習したモデルが公開されていれば色々試したいですが、自分で1から学習する気にはなかなかなりませんね。\nImageBERTの事前学習モデルが公開されれば、以前公開した記事と同じ要領で画像からcaptionを生成できるんじゃないかなと思ってます。\n","date":"2020-02-24T19:46:50+09:00","image":"https://opqrstuvcut.github.io/blog/p/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8_hu83eaf0560f54c0fc88795e970098e27b_819557_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/","title":"画像と自然言語でのマルチモーダルなImageBERT"},{"content":"本記事はQrunchからの転載です。\n Pandasのgroupbyについては雰囲気でやっていたところがありますので、ちょっと真面目に使い方を調べてみました。使っているPandasのバージョンは1.0.1です。\n以下では次のようなDataFrameを使用します。\ndf = pd.DataFrame({\u0026#34;名字\u0026#34;: [\u0026#34;田中\u0026#34;, \u0026#34;山田\u0026#34;, \u0026#34;上田\u0026#34;, \u0026#34;田中\u0026#34;, \u0026#34;田中\u0026#34;], \u0026#34;年齢\u0026#34;: [10, 20, 30, 40, 50], \u0026#34;出身\u0026#34;: [\u0026#34;北海道\u0026#34;, \u0026#34;東京\u0026#34;, None, \u0026#34;沖縄\u0026#34;, \u0026#34;北海道\u0026#34;]})     名字 年齢 出身     0 田中 10 北海道   1 山田 20 東京   2 上田 30    3 田中 40 沖縄   4 田中 50 北海道    Pandasのgroupby PandasのgroupbyはSQLにおけるgroupbyと似たような働きになります。つまるところ、主に集計に使われます。\n例えば名字という列をキーとしてgroupbyするときには次のようにします。\ndf.groupby(\u0026#34;名字\u0026#34;) ただしこれだけでは全く意味がありません。 以下ではgroupbyをしたあとにどう利用することができるかを示します。\nグループ毎にDataFrameを取り出す forを使う forを使ってグループ毎にDataFrameとしてデータを取り出せます。\nfor name, grouped_df in df.groupby(\u0026#34;名字\u0026#34;): print(f\u0026#34;名字：{name}\u0026#34;) print(grouped_df) 名字：上田\n    名字 年齢 出身     2 上田 30     名字：山田\n    名字 年齢 出身     1 山田 20 東京    名字：田中\n    名字 年齢 出身     0 田中 10 北海道   3 田中 40 沖縄   4 田中 50 北海道    get_groupを使う get_groupを使えば1つのグループを指定することもできます。\ndf.groupby(\u0026#34;名字\u0026#34;).get_group(\u0026#34;田中\u0026#34;)     名字 年齢 出身     0 田中 10 北海道   3 田中 40 沖縄   4 田中 50 北海道    groupbyを用いた集計 describe グループごとに統計量を色々計算できます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].describe()    名字 count mean std min 25% 50% 75% max     上田 1 30 nan 30 30 30 30 30   山田 1 20 nan 20 20 20 20 20   田中 3 33.3333 20.8167 10 25 40 45 50    グループの個数のカウント グループごとに行数が数えられます。\ndf.groupby(\u0026#34;名字\u0026#34;).size()    名字 0     上田 1   山田 1   田中 3    次のcountも似たような感じで行数を数えます。ただし、欠損値はカウントされませんので、次のcountの結果とsizeの結果は異なります。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;出身\u0026#34;].count()    名字 出身     上田 0   山田 1   田中 3    演算 和 グループごとに和を計算します。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].sum()    名字 年齢     上田 30   山田 20   田中 100    平均 グループごとに平均を計算します。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].mean()    名字 年齢     上田 30   山田 20   田中 33.3333    特定のデータ取得 グループの先頭を取得します。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].first()    名字 年齢     上田 30   山田 20   田中 10    次のnthでもグループの先頭を取得できます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].nth(0)    名字 年齢     上田 30   山田 20   田中 10    nthの引数を1にするとグループの2番目のデータが取得できます。 dfでは名字が\u0026quot;田中\u0026quot;のケースのみが複数行存在しますので、\u0026ldquo;田中\u0026quot;の2番目だけが取得されます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].nth(1)    名字 年齢     田中 40    関数を渡して計算 次のように任意の関数をaggregateに渡すことで、好きなように集計できます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate(np.sum)    名字 年齢     上田 30   山田 20   田中 100    aggregateに渡す関数はもちろんlambda式でもOKです。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate(lambda vals: sum(vals))    名字 年齢     上田 30   山田 20   田中 100    ちなみにこんな感じで複数個の関数を渡すこともできます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate([np.mean, np.sum, lambda vals: sum(vals)])    名字 mean sum \u0026lt;lambda_0\u0026gt;     上田 30 30 30   山田 20 20 20   田中 33.3333 100 100    集計結果に名前をつけることもできます。以下ではmax_ageという名前の集計結果が得られます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate(max_age=pd.NamedAgg(column=\u0026#34;年齢\u0026#34;, aggfunc=np.max))    名字 max_age     上田 30   山田 20   田中 50    transformでグループの集計結果と各行の値から計算 transformを使えばグループの集計結果と行の値を組み合わせた値を計算できます。\n例1 以下のようにして、行の値とグループの平均との差を計算できます。lambda式のxがグループの各行であり、x.mean()でグループの平均を計算しています。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].transform(lambda x: (x - x.mean())     年齢     0 -23.3333   1 0   2 0   3 6.66667   4 16.6667    例2 グループの平均よりも大きいかどうかをあらわすbooleanが得られます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].transform(lambda x: (x \u0026gt; x.mean())     年齢     0 0   1 0   2 0   3 1   4 1    ブロードキャストの例 transformに渡す関数がスカラ値を返せば、同じグループのなかで同じ値をもつことになります。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].transform(lambda x: x.mean())     年齢     0 33.3333   1 20   2 30   3 33.3333   4 33.3333    前後の値を使う計算 rolling 移動平均の計算などで使うrollingはグループ単位でおこなえます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].rolling(window=2).mean()     年齢     (\u0026lsquo;上田\u0026rsquo;, 2) nan   (\u0026lsquo;山田\u0026rsquo;, 1) nan   (\u0026lsquo;田中\u0026rsquo;, 0) nan   (\u0026lsquo;田中\u0026rsquo;, 3) 25   (\u0026lsquo;田中\u0026rsquo;, 4) 45    cumsum 累積和もグループ単位でおこなえます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].cumsum()     年齢     0 10   1 20   2 30   3 50   4 100    累積和は以下のようにしても計算できます。\ndf.groupby(\u0026#34;名字\u0026#34;).expanding().sum()     年齢     (\u0026lsquo;上田\u0026rsquo;, 2) 30   (\u0026lsquo;山田\u0026rsquo;, 1) 20   (\u0026lsquo;田中\u0026rsquo;, 0) 10   (\u0026lsquo;田中\u0026rsquo;, 3) 50   (\u0026lsquo;田中\u0026rsquo;, 4) 100    applyでグループ毎に好き勝手に処理する applyを使えば、グループ単位のDataFrameを好きに処理したあとにconcatできます。 以下の例を参照ください。\ndef f(df): return pd.DataFrame({\u0026#34;name\u0026#34;: df[\u0026#34;名字\u0026#34;], \u0026#34;age\u0026#34;: df[\u0026#34;年齢\u0026#34;] - df[\u0026#34;年齢\u0026#34;].mean()}) df.groupby(\u0026#34;名字\u0026#34;).apply(lambda x: f(x))     name age     0 田中 -23.3333   1 山田 0   2 上田 0   3 田中 6.66667   4 田中 16.6667    グループをfilteringする filterを使えば、条件を満たすグループのみを残すような処理が可能です。\n例1 グループ内の個数が1より大きいグループだけを残す。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].filter(lambda x: len(x) \u0026gt; 1)     年齢     0 10   3 40   4 50    例2 グループの値の最大値が40未満であるグループだけを残す。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].filter(lambda x: x.max() \u0026lt; 40)     年齢     1 20   2 30    例3 グループに北海道出身の人が含まれるグループだけを残す。\ndf.groupby(\u0026#34;名字\u0026#34;).filter(lambda x: \u0026#34;北海道\u0026#34; in x[\u0026#34;出身\u0026#34;].tolist()     名字 年齢 出身     0 田中 10 北海道   3 田中 40 沖縄   4 田中 50 北海道    ","date":"2020-02-14T12:04:01+09:00","permalink":"https://opqrstuvcut.github.io/blog/p/pandas%E3%81%AEgroupby%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B/","title":"Pandasのgroupbyの使い方をまとめる"},{"content":"本記事はQrunchからの転載です。\n Pandas1.0からは次のようにしてDataFrameをMarkdownの表として出力できます。\nprint(df.to_markdown()) 以下のように表示されます。\n| | 名字 | 年齢 | 出身 | |---:|:-------|-------:|:-------| | 0 | 田中 | 10 | 北海道 | | 1 | 山田 | 20 | 東京 | | 2 | 上田 | 30 | | | 3 | 田中 | 40 | 沖縄 | | 4 | 田中 | 50 | 北海道 | QrunchやQiitaに大体そのままコピーできます。 ちゃんと以下のように表示されます。\n    名字 年齢 出身     0 田中 10 北海道   1 山田 20 東京   2 上田 30    3 田中 40 沖縄   4 田中 50 北海道    上手く表として表示されないときは、左上の空白のセルに全角スペース入れたり頑張りましょう。\n","date":"2020-02-13T01:55:35+09:00","permalink":"https://opqrstuvcut.github.io/blog/p/pandas%E3%81%AEdataframe%E3%82%92%E6%9C%80%E9%AB%98%E3%81%AB%E7%B0%A1%E5%8D%98%E3%81%ABmarkdown%E3%81%AE%E8%A1%A8%E3%81%A8%E3%81%97%E3%81%A6%E5%87%BA%E5%8A%9B/","title":"PandasのDataFrameを最高に簡単にMarkdownの表として出力"},{"content":"本記事はQrunchからの転載です。\n モデルの予測結果を説明する方法としてLIMEがあります。 LIMEはディープラーニングに限らず、任意のモデルに対して予測結果を適用することができます。 また手法としては結構有名かと思います。\n今回はそんなLIMEの理論について説明します。\n論文：“Why Should I Trust You?” Explaining the Predictions of Any Classifie\nLIMEの戦略 任意のモデル$f$に入力$x \\in \\mathbb{R}^d$が与えられたときの予測結果$f(x)$への特徴量の寄与を求めることを考えます。\nLIMEでは$x$近傍（近傍については後述）に対しては$f$と同じような予測をすることができる、かつ解釈が容易なモデル$g$を求めます。 例えば$g$が線形モデルの場合には、$g$の各係数を見ることで特徴量の寄与を得ることが可能です。あるいは$g$が決定木であれば、人間でもある程度容易にモデルの解釈が可能です。ですから、このようなモデル$g$を$f$の代わりに使って、予測結果の解釈をしようというモチベーションです。 ただし、LIMEでは$g$には特徴量の値が$0$か$1$となるベクトル$x'$が入力として与えられるものとします。これは何らかのルールで$x$の要素と$x'$の要素が対応づいているとします。ここも詳細をあとで述べます。 以上のように、解釈が難しいモデル$f$を解釈が容易なモデル$g$に落とし込むことがLIMEのやりたいことになります。\n実際にどうやって$g$を求めるのかといえば、次式のようになります。 $${\\rm argmin_{g \\in G}} \\ L(f, g, \\pi_x) + \\Omega(g).$$\nここで、\n $L$は損失関数です。$x$近傍で$g$の予測値が$f$の予測値に近いと、小さくなるように$L$を定義します。 $\\pi_x$は損失関数で使われる重みで、$x$の近傍点が$x$から遠いほど小さい値を取るようにします。詳細は後述する線形モデルの項を参照。 $\\Omega$はモデルの複雑さとなります。決定木を使う場合には木の深さであったり、線形モデルの場合には非ゼロの重みの数になります。モデルを解釈するためには、モデルはシンプルな方が良いため、$\\Omega$を加えることで$g$をなるべく人間にやさしいモデルにしてあげます。  まだ色々と詳細を述べていないため、わからないところは多々あると思いますが、上式はなるべくシンプルなモデルで$x$の近傍で$f$と近似する$g$を見つけるといったことを意味します。 この局所的に近似された$g$が得られれば、$x$近傍での特徴量が$g$へ与える寄与がわかる、つまり$f$へ与える寄与が近似的にはわかります。\n次に画像の場合のケースについて、詳細に踏み込みます。\n画像に対する線形モデルでのLIME superpixel 画像にLIMEを適用する場合、まず次のように入力画像をsuperpixelに分割し、領域ごとに寄与を求めていきます。\n   引用元：https://towardsdatascience.com/understanding-how-lime-explains-predictions-d404e5d1829c\n 実際には上記のようにある程度細かく領域を分けますが、以下では例として扱いやすいように次のような画像を考えて、粗く領域を分けていきます（左がオリジナルのくまモンで、右がsuperpixelに分割されたくまモンです）。    各領域を$g$に与える入力$x'$の各要素に対応させます。例えば1番の領域が$x'$の1番目の要素、2番が2番目の要素のようにします。その上で、$x'$の各要素が1のときには対応する領域のピクセルが$x$と同じピクセル値、0のときにはその領域がグレーで埋められた画像と対応していると考えます。 具体的には $$x' = [0, 0, 1, 1, 0,0,0,0]$$ としたとき、3番目と4番目だけが1ですので、この$x'$に対応した画像は次のようになります。  \n近傍のサンプリング LIMEでは $x$の近傍のサンプリングをおこないます。 画像の場合に近傍とはどうなるんでしょうか？直感的には謎じゃないでしょうか。\nLIMEの場合には分割された領域のうち、適当な個数（個数もランダムに決めますが、個数の下限は決めておきます）をそのままにし、それ以外をグレーに置き換える処理をします。 $x'$の話でいえば、適当な個数の要素については1とし、それ以外は0とする処理に等しいです。\nこのようにして得られた画像を$x$の近傍として扱います。またこのようにして近傍を得ることを、近傍のサンプリングとします。 先程示した$x'$に対応した画像も$x$の近傍になります。\n線形モデルのケース $g$が線形モデルの場合には$g(z')$は次のようになります。  \n線形モデルの係数（寄与）を求めるため、次のように損失関数$L$を定義します。 $$ L(f, g, \\pi_x) = \\sum_{z,z'∈Z}\\pi_x(z) (f(z) − g(z'))^2.$$ ここで$\\pi_x$は以下のとおりです。 $$ \\pi_x = \\exp(−D(x, z)^2/\\sigma^2).$$ $z$は$x$近傍の画像をあらわし、$z'$は先程まで説明していた（$z$に対応する）$x'$と同じものです。$Z$はサンプリングされた$z$と$z'$のペアになります。\n上式の意味合いとしては、近傍画像$z$の学習済みモデルでの予測値$f(z)$と解釈が容易なモデル$g(z')$が近い値になるように$g$を学習していきます。\nまた、$\\pi_x$の存在のため、$z$が$x$に近ければ（=近傍が入力画像に近い）二乗誤差$(f(z) − g(z'))^2$が$L$に与える影響は大きいですが、一方で$z$が$x$と大きく異なれば（=近傍が入力画像と大きく異なる）、$(f(z) − g(z'))^2$が$L$に与える影響が小さくなります。 より$x$に近い$z$に関しては$g(z')$が良く$f(x)$に近似されるべきですので、このように重み付けされているのは分かる話かと思います。\nなお論文中ではLasso回帰として${\\rm argmin_{g \\in G}} \\ L(f, g, \\pi_x) + \\Omega(g)$を解いています。\nLIMEの実験結果 実験結果は他の方のブログなどで散々書かれていますので、そちらを参考ください（力尽きました）。\n","date":"2020-02-12T00:23:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/1341b73a17da593ffc43cebc86969604_hu43488a00ab78527cedf1030543e53948_126524_120x120_fill_q75_box_smart1.jpg","permalink":"https://opqrstuvcut.github.io/blog/p/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/","title":"モデルの予測結果を説明するLIMEの理論"},{"content":"本記事はQrunchからの転載です。\n Uberが公開している機械学習モデルの予測と特徴量の関係性を可視化するツールであるManifoldを紹介します。\nManifoldを試す Manifoldでできることを見ていきます。\nインストール レポジトリをgit cloneしてから、githubのページにあるように以下のようにしてインストールできました。\n# under the root directory, install all dependencies yarn # demo app is in examples/manifold directory cd examples/manifold # instal demo app dependencies yarn # start the app npm run start 準備 まずユーザーは次の3つのデータを用意します。\n 入力データの特徴量を記述したcsv 入力データに対するラベル 入力データに対するモデルの予測値（分類問題の場合には各クラスに属する確率になります）  モデルはなんでも良く、必要なのは予測値であることに注意してください。\n今回はkaggleのタイタニックのデータから適当にテストデータを作ってみました。\u0008 テストデータとlightgbmのモデルを用いて、次のような感じでManifoldに必要なデータを作ってます。\nwith open(\u0026#34;./titanic_res/features.csv\u0026#34;, \u0026#34;w\u0026#34;) as f: columns = \u0026#34;,\u0026#34;.join(list(X_test.columns)) # X_testがテストデータの特徴量 f.write(f\u0026#34;{columns}\\n\u0026#34;) for i, features in X_test.iterrows():　f_string = \u0026#34;,\u0026#34;.join([str(x) for x in features]) f.write(f\u0026#34;{f_string}\\n\u0026#34;) with open(\u0026#34;./titanic_res/pred.csv\u0026#34;, \u0026#34;w\u0026#34;) as f: pred = bst.predict(X_test) # bstがlightgbmのモデル f.write(\u0026#34;survived,death\\n\u0026#34;) for prob in pred: f.write(f\u0026#34;{prob},{1-prob}\\n\u0026#34;) with open(\u0026#34;./titanic_res/truth.csv\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#34;truth\\n\u0026#34;) for truth in y_test: # y_testがテストデータのラベル label = \u0026#34;survived\u0026#34; if truth == 1 else \u0026#34;death\u0026#34; f.write(f\u0026#34;{label}\\n\u0026#34;) Manifoldでの可視化 アップロード npm run startを実行すると、ブラウザ上でアプリが立ち上がります。 立ち上げ直後はファイルのアップロードを促されるので、準備したファイルをドラッグアンドドロップしてアップロードします。  \n性能の分布 アップロードすると、次のような画面になります。  \nManifoldでは予測値の当たり具合によって、自動で各データがsegmentに分けられています。 各segmentはlog lossの値をK-meansに適用することでできたクラスタになっています。\nグラフの横軸がlog lossとなっており、segmentにわけてデータの個数が描画されています。 segment0に含まれるデータは性能が良く、segment3に含まれるデータは性能が悪いという見方になります。\nまた各segmentはGroup0かGroup1に振り分けられます。Group0が性能が悪く、Group1が性能が良いです。\n各segmentがどのGroupに入るのかはユーザー側で変えることが可能です。\n特徴量の分布 ManifoldではGroup毎の特徴量の分布の違いを見ることができます。  \n一番上の行がGroupによる性別の分布の違いをあらわしたもので、そこにマウスをもってくると次のようになります。  \n性別が男性である場合にはGroup1に入っている（性能が良い）データが46個、Group0に入っている（性能が悪い）データが7個となっています。 また、マウスを右側にもっていくと次のようになります。  \nこれによると、性別が女性である場合にはGroup1に入っている（性能が良い）データは29個、Group0に入っている（性能が悪い）データは8個となっています。 よって、女性のほうが予測が上手くいっていないことになります。 タイタニック号に乗っていた女性は男性よりも優先されて救命ボートに乗っていました。男性である場合にはほぼボートに乗れず、そのような人たちは生き残らないため、予測が容易という解釈になるのかなと思います。\n一方で女性のなかでもボートに乗れるかどうかは他の要素によって左右されるため、予測が男性に比べると難しいという解釈かと思います。\n今は一番上が性別、2行目が年齢となっています。この並びの順番ですが、KL-divergenceによって求められた「2つのGroupでの特徴量の分布」が違う順になっています。 下にいくほどGroup間での違いがない特徴量であることを示します。\nどう使うべきか？ Manifoldで出来ることは、予測が上手くいっていないデータの傾向を可視化することです。 どうしてモデルの予測が上手くいっていないのかを説明すること、またモデルを改善することに役立てることができるのではないかと思います。\n","date":"2020-01-28T22:52:36+09:00","image":"https://opqrstuvcut.github.io/blog/p/uber%E8%A3%BD%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%84%E3%83%BC%E3%83%ABmanifold/2f11d99ea2c3cf569c040d9555f5ab2c_hu0d0289d789c0c959994f816a625d3cae_309272_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/uber%E8%A3%BD%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%84%E3%83%BC%E3%83%ABmanifold/","title":"Uber製の機械学習モデルのデバッグツールManifold"},{"content":"本記事はQrunchからの転載です。\n 吹き出しのライブラリ Flutterで吹き出しを出すためのライブラリとしてBubbleがあります。こちらを使うと吹き出しを簡単に表示できます。 もう一つSpeechBubbleというライブラリもありますが、Bubbleのほうが色々オプションが設定できます。\nBubble Bubbleを使うと以下のような吹き出しが簡単に表示できます。\n   \n最もシンプルな吹き出しの作り方は以下のようになります。\nBubble( nip: BubbleNip.leftTop, child: Text(\u0026#39;Hi, developer!\u0026#39;), ) Bubbleのオプション Bubbleでは次がオプションとして選べます。\n 吹き出しの色 吹き出しの形状 吹き出しからちょこんと出ているところの位置 影 マージン、パディング  欲しい機能は一通り揃っていてとても便利です。詳細はBubbleのgithubのページをご覧ください。\nBubbleの不満 素晴らしいライブラリなのですが、ちょっとだけ不満があります。 吹き出しからちょこんと出ているやつ（なんというか知らないんですが）の位置が現状は左上、左下、右上、右下しか選べません。\nなので、forkして左中央に位置を指定できるようにしてみました。 https://github.com/opqrstuvcut/bubble\nこちらを使うと次のように吹き出しの左中央からちょこんとあれが出せます。  \nコードは以下の通り。\nBubble( nip: BubbleNip.leftCenter, child: Text(\u0026#39;ちょこんとでるのが左中央だよ\u0026#39;), ) ","date":"2020-01-28T00:29:30+09:00","image":"https://opqrstuvcut.github.io/blog/p/flutter%E3%81%A7%E5%90%B9%E3%81%8D%E5%87%BA%E3%81%97%E3%82%92%E4%BD%9C%E3%82%8B/56ff1ce17d741bc7f6aeb54a9c567e76_hub4d832f83afaad986e804b9ab594a001_70782_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/flutter%E3%81%A7%E5%90%B9%E3%81%8D%E5%87%BA%E3%81%97%E3%82%92%E4%BD%9C%E3%82%8B/","title":"Flutterで吹き出しを作る"},{"content":"本記事はQrunchからの転載です。\n Matplotlibの凡例を外側に出したい人用に色々な例を書いておきます。\n次のような凡例の位置をいじらずに表示した状態からいじっていきます。\ndata = np.random.rand(10, 3) labels = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;] plt.plot(range(10), data, marker=\u0026#34;o\u0026#34;, linewidth=3) plt.legend(labels) plt.title(\u0026#34;title\u0026#34;) plt.ylabel(\u0026#34;y label\u0026#34;) plt.xlabel(\u0026#34;x label\u0026#34;) plt.show()  \n右上に表示 凡例の枠の上部をグラフの枠の上部にあわせて、右上に表示するときは以下のようにします。\nplt.legend(labels, loc=\u0026#39;upper left\u0026#39;, bbox_to_anchor=(1, 1))  \n右中央に表示 凡例の上下の位置をグラフと揃えて、右に表示するときは以下のようにします。\nplt.legend(labels, loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1., .5))  \n上に表示 凡例の左右の位置をグラフと揃えて、上に表示するときは以下のようにします。 ncol=3とすることで横一列に3つ分のグラフの凡例を表示できます。\nplt.legend(labels, loc=\u0026#39;lower center\u0026#39;, bbox_to_anchor=(.5, 1.1), ncol=3)  \n下に表示 凡例の左右の位置をグラフと揃えて、下に表示するときは以下のようにします。\nplt.legend(labels, loc=\u0026#39;upper center\u0026#39;, bbox_to_anchor=(.5, -.15), ncol=3)  \n理屈 plt.legendの引数のlocに指定した凡例の箇所がbbox_to_anchorで指定した座標になるように位置が調整されます。ここで、座標はグラフの枠の左下が(0,0)で右上が(1,1)となります。  \n例1loc=\u0026lsquo;upper left\u0026rsquo;、bbox_to_anchor=(1, 1)であるときには、凡例の枠の左上（locがupper leftなので）が(1,1)になるように凡例が配置されます。\n例2loc=\u0026lsquo;lower center\u0026rsquo;、bbox_to_anchor=(0.5, 1.1)であるときには、凡例の枠の中央下（locがlower centerなので）が(0.5,1.1)になるように凡例が配置されます。\n","date":"2020-01-20T21:09:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/matplotlib%E3%81%AE%E5%87%A1%E4%BE%8B%E3%82%92%E5%A4%96%E5%81%B4%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%97%E3%81%9F%E3%81%84%E4%BA%BA%E3%81%B8/bc011ef843d7d97092a83521e65e6a15_hu444fc9dc9cc4f90ff4e3d49d636ee18f_32079_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/matplotlib%E3%81%AE%E5%87%A1%E4%BE%8B%E3%82%92%E5%A4%96%E5%81%B4%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%97%E3%81%9F%E3%81%84%E4%BA%BA%E3%81%B8/","title":"Matplotlibの凡例を外側に表示したい人へ"},{"content":"本記事はQrunchからの転載です。\n Pythonのnamedtuple使ってますか？ 案外使っていない方が多いので、ご紹介しておきます。\nnamedtupleとは？ 通常のタプルはインデックス指定でのみ要素を参照します。一方で、NamedTupleはタプルの各要素を名前によって参照できます。\n例えばpというnamedtupleの要素にnameというものがあれば、次のようにして参照できます。\nname = p.name 他の部分はほとんど通常のタプルと同じと思って問題ありません。\nnamedtupleを使うメリット 要素に名前がつけられるようになっただけですが、私が思うメリットは以下の通りです。\n タプルのようなインデックスの指定では参照する要素を誤る可能性が出てきますが、名前で指定することで誤りを防ぐことができます。 タプルの各要素の意味がはっきりするのでコードの可読性がよくなります。 タプルを生成する箇所が複数あった場合に、要素の順番を誤ったり要素数を誤ったりすることがなくなります。  他にもいいところがあるかもしれませんね。\nnamedtupleの使い方 その1 使い方はそれほど難しくありません。以下のようにしてnamedtupleを定義できます。\nfrom collections import namedtuple Person = namedtuple(\u0026#34;Person\u0026#34;, [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;sex\u0026#34;]) 上記により、Personのタプルが宣言できました。Personはnamedtupleの第二引数に指定されたnameとageとsexを要素にもつタプルです。ちなみに以下のようにリストではなく、スペース区切りの文字列で与えても同じ意味となります。\nPerson = namedtuple(\u0026#34;Person\u0026#34;, \u0026#34;name age sex\u0026#34;) 宣言したPersonというタプルを生成するには以下のようにします。\np = Person(\u0026#34;太郎\u0026#34;, 10, \u0026#34;男\u0026#34;) このpの要素の参照は以下のようにしてできます。\nprint(p.name, p.age, p.sex) # output: 太郎 10 男 簡単です！\nその2 （おそらく）Python3.6からは次のようにもnamedtupleが利用できます。\nfrom typing import NamedTuple class Person(NamedTuple): name: str age: int sex: str p = Person(\u0026#34;太郎\u0026#34;, 10, \u0026#34;男\u0026#34;) print(p.name, p.age, p.sex) # output: 太郎 10 男 個人的にはこの書き方のほうがぱっと見たときにわかりやすいような気がして好きです。 あと多分補完もこちらのほうが効きやすいのではと思っています。\nまとめ コードの可読性があがり、間違いも減るので、namedtupleの利用をオススメします！\n","date":"2020-01-06T21:57:05+09:00","permalink":"https://opqrstuvcut.github.io/blog/p/python%E3%81%AEnamedtuple%E3%82%92%E4%BD%BF%E3%81%8A%E3%81%86/","title":"Pythonのnamedtupleを使おう"},{"content":"本記事はQrunchからの転載です。\n BERTのパラメータの数を減らしたモデルであるALBERTについての概要を書いていきます。\n参考論文：ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n問題意識 2018年に提案されたBERTは自然言語界隈では非常に上手くいった手法です。先程論文の引用数を見たら、もう3000を超えていまして、この数字を見てもよくわかります。\nBERTは高い性能で色々な問題に適用することができる汎用性の高いモデルですが、パラメータ数が多いという特徴があります。なんでパラメータ数が多いかといえば、全結合層が沢山使われるからです。これは内部的にはそれなりに大きな行列を沢山持っているような状態です。\nパラメータ数が多いことで以下のような問題が起こります。\n メモリにモデルが乗らない 計算量が多い（論文中で特に言われているのが、分散処理での通信のコストです。通信は遅いのであまりやりたくありません。）  また、パラメータ数を増やしていっても順調に性能が高まるわけではなく、逆に大きく性能を落とすことがあります。以下の表がそれを示しています。\n \nBERT-xlargeというのがBERT-largeよりも隠れ層のパラメータ数を多くしたものですが、RACEを解いたときのAccuracyが大きく下がっているのがわかります（過学習のように思われますが、過学習だと明確にわかるようなサインが出ていないと論文には書かれています）。\n提案手法 語彙の埋め込みの行列分解 英版のBERTでは30000の語彙が存在します。BERTではこの語彙の埋め込みベクトルの次元が隠れ層の次元と同じですので、BERT-largeの場合には30000×1024のサイズの行列をもつことになります。\nこれに対してALBERTでは行列を分解して、語彙の埋め込みベクトルのサイズと隠れ層のサイズを別にしてしまいます。具体的には、語彙の数を$V$、語彙の埋め込みベクトルの次元を$E$、隠れ層の次元を$H$としたとき、語彙の埋め込みベクトルの行列のサイズは$V \\times E$となり、それに$E \\times H$のサイズの行列を掛けて$H$次元の空間に射影するようにします。そうすることで、もともとパラメータ数が$O(V \\times H)$だったのが、$O(V \\times E + E \\times H)$となり、$E \\ll H$のときには大きくパラメータ数が削減されることになります。\nこのようにしてしまって問題ないかと疑問が出てきますね。\n語彙のベクトル自体は文脈に依存しないベクトルで、その後の隠れ層を経て文脈を考慮したベクトルへと変わっていきます。この文脈に依存しないベクトルが持つ情報は大きくなく、次元を隠れ層ほど大きくする必要がないため、上記のようにしても問題がないということのようです。\n層間のパラメータの共有 BERTではEncoderを何度も重ねる構造になっています。ALBERTでは各層の重みを共通にすることで、パラメータ数を大きく削減しています。\nNSPからSOPへの変更 BERTではMASKされたトークンを予測することと、与えられた2つの文が連続しているかどうかを予測するタスクであるnext-sentence prediction(NSP)を同時に解けるように学習していきます。\nNSPの学習のため、実際に連続した文を与えるケースとランダムに選ばれた2つの文を与えるケースを用意します。NSPの意図はBERTに文の一貫性の理解を促すためです。しかしながら、ランダムに選ばれた2つの文だと、そもそも文のトピックが異なるために、あまり文脈を理解できなくともNSPが解けてしまいます。NSPは問題が簡単すぎるということです。\nこれを修正するため、ALBERTではsentence-order prediction(SOP)を提案しています。\nSOPは2つの連続した文の順番がそのままの順番か、逆になっているかを予測する問題です。これを解けるようにすることで、文の一貫性をモデルが理解できるようになるだろうという狙いです。トピックによって判断することができず、NSPよりも難しい問題設定になっていますね。\n実験結果 実験で使われているALBERTのモデルは以下のとおりです。  \nALBERTは隠れ層の次元が大きくてもBERTに比べて大きくパラメータ数が抑えられていますね。\nBERTとの比較 BERTとの比較実験です。  \nALBERTではパラメータ数が減るだけではなく、性能も大きく向上しています。少しじゃなく結構良くなっている感じですね。 訓練時間の速度比が最後の列です。すべてBERTのxlargeに比べての速度比です。同じ隠れ層の大きさのBERTに比べれば速いですが、ALBERTのxlargeがBERTのlargeより速くなるというほどのスピードアップではないことに気をつけてください。\n他の手法と比較 XLNetやRoBERTaとの比較です。    \n大体のタスクにおいて、ALBERTの性能が高いことがわかります。\n感想 ALBERTはどれくらいのメモリや訓練時間が必要なのかが気になって読んでみました。 BERTに比べるとパラメータ数と訓練時間が減っていますが、まだまだ自分で学習をさせられるものではないなぁという印象です。\n","date":"2019-12-28T23:36:43+09:00","image":"https://opqrstuvcut.github.io/blog/p/bert%E3%82%92%E8%BB%BD%E9%87%8F%E5%8C%96%E3%81%97%E3%81%9Falbert%E3%81%AE%E6%A6%82%E8%A6%81/2b1477117e8654c1b558183f0277acdf_hu9dc25053793792fcc48628a740947cda_243061_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/bert%E3%82%92%E8%BB%BD%E9%87%8F%E5%8C%96%E3%81%97%E3%81%9Falbert%E3%81%AE%E6%A6%82%E8%A6%81/","title":"BERTを軽量化したALBERTの概要"},{"content":"本記事はQrunchからの転載です。\n ディープラーニングのモデルに対する特徴量の寄与を求める方法の1つである、DeepLiftについて今回は説明します。\n参考文献：Learning Important Features Through Propagating Activation Differences\n従来法の問題点 DeepLiftを提案している論文では、以下の2つが従来手法の問題点として挙げられています。\nsaturation problem saturation problemは勾配が0であるような区間では寄与が0になってしまう問題です。 従来手法には勾配を利用する手法が多いですが、そのような手法ではsaturation problemが発生してしまいます。 以下の図をご覧ください。   図中の関数は$y = 1 - {\\rm ReLU(1 - x)}$で、この関数を1つのネットワークとして考えてみます。 この関数では$x \u0026lt; 1$では勾配が$1$となり、$x\u0026gt;1$では勾配が$0$になります。 入力が$x=0$の場合に比べれば、$x=2$の場合は出力値が1だけ大きくなるため、寄与は$x=0$の場合よりも大きくなって欲しいです。しかしながら、寄与=勾配$\\times$入力とする寄与の計算方法の場合には、 $$ x = 0 \\Rightarrow \\ \\mbox{寄与} = 1 \\times 0 = 0 $$$$ x = 1 \\Rightarrow \\mbox{寄与} = 0 \\times 2 = 0 $$ となり、残念ながら寄与が等しく0になってしまいます。 このようにReLUによって勾配が0になってしまうことは、Integrated Gradientsの提案論文のなかでも同様に問題として挙げられています。\ndiscontinuous gradients 2つ目に挙げられている問題がdiscontinuous gradientsです。これも下図をご覧ください。   左から、ネットワークをあらわしている関数$y={\\rm ReLU(x - 10)}$、その勾配、寄与=勾配$\\times $入力です。 このような関数に対しては計算される寄与値が$x=10$で不連続となり、$x=10$までは寄与が全く無いのに、$x=10$を超えると突然寄与の値が$10$を超えるようになります。 入力値のちょっとした差で寄与が大きく変わるのは良くないですね。\nDeepLift 前述した2つの問題を解決するDeepLiftのアイディアと適用結果について述べていきます。DeepLift以外にも、Integrated Gradientsがこれら2つの問題を解決していますが、求まった寄与が直感的ではない場合があります。このことは適用結果で示します。\nなお、DeepLiftで利用されているアイディアの1つとして、RevealCancel Ruleというものがありますが、書くのが大変になりそうなので省略します。\nDeepLiftのアイディア DeepLiftはIntegrated GradientsやSHAPと同様に、基準となる点を決めておき、そこから入力$x$がどれだけ異なるか、また基準点と$x$のネットワークの出力がどれだけ異なるかをもとにして寄与値を計算していきます。 この基準となる点を$x_1^0, \\cdots, x_n^0$としておきます。\nディープラーニングで使われる計算は線形変換と非線形変換の2つに分けられ、DeepLiftではこれによって次のように寄与の計算方法が変わってきます。\nLinear Rule まず線形変換の方からです。線形変換には全結合層、畳み込み層が該当します。\n入力（あるいはある隠れ層の出力）$x_1,\\cdots, x_n$から次の層のあるニューロン$y$が、重み$w_i$とバイバス$b$を用いて次のようにあらわされるとします。 $$y = \\sum_{i=1}^N w_i x_i + b$$ 基準点$x_1^0, \\cdots, x_n^0$でも同様に $$y^0 = \\sum_{i=1}^N w_i x_i^0 + b$$ となります。\nこのとき、基準点$x_1^0, \\cdots, x_n^0$に対して、$x_1,\\cdots, x_n$における$y$の変化量は $$ \\Delta y =\\sum_{i=1}^N w_i \\Delta x_i $$ となります。ここで$\\Delta y = y - y^0, \\Delta x_i = x_i - x_i^0$です。\nDeepLiftではこの変化量に着目し、各入力$x_i$に対する$y$への寄与度$C_{\\Delta x_i \\Delta y} $を計算していきます。具体的には次のようになります。 $$ C_{\\Delta x_i \\Delta y} = w_i \\Delta x_i .$$\nつまり、入力$x_i$が基準点に比べてどれだけ$y$の変化に影響を及ぼしたかによって寄与が決まります。\nRescale Rule 次に活性化関数で用いられる非線形変換を扱っていきます。 非線形変換のときも線形変換の場合と同様にして考え、基準点に対するニューロンの出力からどれだけ変化を及ぼしたかによって、寄与を決定します。 ただしReLUやtanhなどは1変数$x$を入力としますから、線形変換の場合とは異なり、 $$C_{\\Delta x \\Delta y} = \\Delta y $$です。\nsaturation problemとdiscontinuous gradientsの解決 Linear RuleとRescale Ruleの2つを定義しましたが、このルールに則って寄与を計算することで、前述した2つの問題を解決することができます（どちらもRescale Rule絡みになりますが）。\nsaturation problem以下の図のように、DeepLiftでは勾配が0になる状況でも寄与は0になりません。  \ndiscontinuous gradients以下の3列目がDeepLiftでの寄与をあらわしたグラフです。DeepLiftでは寄与が不連続になりません。  \n非常に単純なアイディアですが、問題にあがっていた2つを解決することができました。\n連鎖律 ここまでで扱ってきた内容は、入力を線形変換したときの寄与、あるいは入力を非線形変換したときの寄与の計算になります。 それでは、入力に線形変換と非線形変換を順番に適用するときには、入力の最終的な出力に対する寄与はどのようにして求めると良いでしょうか。またディープラーニングのように層が複数あるようなケースではどうやって計算すれば良いでしょうか。 DeepLiftでは次のmultiplierとそれに対する連鎖律を導入することで、この計算を可能にしています。\nまず、multiplier $m_{\\Delta x \\Delta y}$の定義は以下のようになります。 $$ m_{\\Delta x \\Delta y} = \\frac {C_{\\Delta x \\Delta y}}{\\Delta x}.$$ これは$\\partial y/ \\partial x$と似たような形式になっています。特にRescale ruleのときには$C_{\\Delta x \\Delta y}=\\Delta y$ですから、意味合いは近いものがあります。\n次に連鎖律の定義です。 ネットワークへの入力を$x_1,\\cdots,x_n$、隠れ層のニューロンを$y_1,\\cdots, y_\\ell$、出力層のある1つのニューロンを$z$とします。このとき、multiplierに対して次のように連鎖律を定義します。 $$ m_{\\Delta x_i \\Delta z} = \\sum_{j=1}^\\ell m_{\\Delta x_i \\Delta y_j} m_{\\Delta y_j \\Delta z}.$$\nこれは丁度ディープラーニングでの計算で使われる連鎖律と同じものです。つまり、 $$ \\frac{\\partial z}{\\partial x_i} = \\sum_{j=1}^\\ell \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} $$ と同じ形式です。 ただし、multiplierの連鎖律は導かれるものではなく、定義であることに注意が必要です。\nmultiplierの連鎖律を使うことで、backpropagationのようにして任意の層に対する任意の層へのmultiplierが求まります。こうして求まったmultiplierに対して基準点からの差をかけ合わせれば寄与が求まります。さきほどの連鎖律の話に出てきた変数の定義をそのまま使うと、 $$ C_{\\Delta x_i \\Delta z} = m_{\\Delta x_i \\Delta z} \\Delta x_i $$ が$x_i$が$z$への寄与になります。\nDeepLiftの適用結果 MNISTに適用した結果を示します。   1つの行が1つの手法をあらわしています（DeepLiftはRevealCancelとありますが、これは今回説明を省いたアイディアです）。1列目がオリジナルの画像で、2列目がCNNによって計算された「8」である確率への寄与でをあらわします。明るい部分が正の寄与で、暗いところが負の寄与になります。ちなみに基準点となる入力は全ピクセル値を0とした真っ黒な画像です。3列目は「3」である確率への寄与です。また4列目はオリジナルの画像から「3」である確率への寄与が高いピクセルを抜き出しているものです。 上2つの手法はピクセル間での寄与の差があまり明確ではありません。また4列目をみてみると、勾配と入力の積を寄与とした方法やIntegrated Gradientsよりも、「3」と判定するために必要なピクセルへはっきりと高い寄与を割り当てることができています。\nDeepLiftとIntegrated Gradients DeepLiftとIntegrated Gradientsは論文の中でお互いの問題点を指摘しあっています。\n DeepLiftの提案論文の主張： Integrated Gradientsは直感的でない寄与の割当がおこる。\n  Integrated Gradientsの提案論文の主張： DeepLiftはmultiplierの連鎖律の部分が数学的に問題がある。\n SHAPでも上記2つの手法を利用した計算が可能です。どちらが良いのかは悩ましいですが、結果が直感的になりやすいのはDeepLift、数学的に理論がしっかりしているのがIntegrated Gradientsという感じでしょうか（あとは実装しやすいのはIntegrated Gradientsとか計算量が少ないのはDeepLiftなどの観点もありますね）。\n","date":"2019-12-19T02:03:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/64a618e3bf36cb2953ac208966e42b90_hu1d3b8c411e821887d60eecf621421a88_489801_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/","title":"ディープラーニングのモデルの特徴量の寄与を求めるDeepLift"},{"content":"本記事はQrunchからの転載です。\n FlutterでS3へファイルをアップロードするための公式のライブラリはありませんが、有志によるライブラリamazon_s3_cognitoがあります。 今回はこちらの紹介+forkしてちょっと修正したのでよければ使ってねという話になります。\n事前準備 AWS cognitoでIDプールを作っておく必要があります。 cognitoのページを開くと以下のような表示がされるので、「IDプールの管理」を押します。  \n新しいIDプールの作成を押し、以下のような感じで設定をします。  \n次のページでRoleのポリシーの設定ができますので、「詳細を表示」 -\u0026gt; 「ポリシードキュメントを表示」 からポリシーを編集します。Uauthと書いてある方だけ編集すればOKです。  \nポリシーは以下のようにすれば大丈夫ですが、バケット名は自分で適当なものに変更してください。\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;mobileanalytics:PutEvents\u0026quot;, \u0026quot;cognito-sync:*\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*Object\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(バケット名)*\u0026quot; } ] } おそらくこれでAWS側の設定は大丈夫かと思います。\nFlutter側からファイルを送信する amazon_s3_cognitoをpubspec.yamlに追加して、flutter pub getしたら使う準備はできました。 次のようなコードでファイルをS3に送ることができます。\nimport 'package:amazon_s3_cognito/amazon_s3_cognito.dart'; import 'package:amazon_s3_cognito/aws_region.dart'; String uploadedImageUrl = await AmazonS3Cognito.upload( imagePath, BUCKET_NAME, IDENTITY_POOL_ID, IMAGE_NAME, AwsRegion.AP_NORTHEAST_1, AwsRegion.AP_NORTHEAST_1)  imagePathはスマートフォン内の送りたいファイルのパスを指定します。 BUCKET_NAMEはS3のバケット名を指定します。 IDENTITY_POOL_IDはさきほど設定したAWS cognitoから次のような詳細ページにいくことで、取得できます。以下のIDプールのIDと書かれている行のダブルクォーテーションの部分をコピペすればOKです。   IMAGE_NAMEはS3のバケット以下のファイルの保存先のパスを指定します。 AwsRegion.AP_NORTHEAST_1はregionを指定しています。2つ目はsub region？の設定らしいですが、なければ同じもので特に問題ありません。  返り値はS3上の保存先のファイルパスになります。失敗したときは\u0026quot;Failed\u0026quot;だったり空のパスが渡ってきます。\nあれ、iOSでは失敗する… Androidではここまでの設定等でうまくいったのですが、iOSでは常にうまく送信できませんし、空のパスが返り値として受け取られます。 実はこれはアクセス権の問題でうまく動きませんでした。iOS版の実装をみると、publicなバケットにしかファイルを送れないようになっていました。 というわけで、amazon_s3_cognitoのレポジトリをforkして、privateなバケットにもファイルを送れるように修正しましたので、よければ使ってみてください。 https://github.com/opqrstuvcut/amazon_s3_cognito\npubspec.yamlには以下のようにかけばOKです。\n amazon_s3_cognito: git: url: https://github.com/opqrstuvcut/amazon_s3_cognito ","date":"2019-12-08T19:04:32+09:00","image":"https://opqrstuvcut.github.io/blog/p/flutter%E3%81%A7s3%E3%81%AB%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B/5156acb6c29b977915456e21c1d96fb8_hua0e9ef26a9d3ab210e1922bc431eb065_258199_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/flutter%E3%81%A7s3%E3%81%AB%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B/","title":"FlutterでS3にファイルをアップロードする"},{"content":"本記事はQrunchからの転載です。\n 機械学習のモデルの出力に対する入力された特徴量の寄与を求める手法の1つに、Integrated Gradientsというものがあります。 Integrated Gradientsはディープラーニング向けの手法ですが、他のディープラーニング向けの手法では満たしていない公理（性質）をいくつも満たしているという点で優れています。 今回はそんなIntegrated Gradientsを解説します。\n参考論文：Axiomatic Attribution for Deep Networks\n先にbaselineのお話 本題に入る前に、大事な考え方であるbaselineを説明しておきます。\n人間が何か起こったことに対して原因を考えるとき、何かの基準となる事がその人の中にはあり、それに比べ、「ここが良くない」とか「ここが良かったから結果としてこういう結果になったんだな」、と考えるんじゃないでしょうか。 Integrated Gradientsの場合もその考え方を用います。 先程の例の基準がbaselineと呼ばれ、画像のタスクでは例えば真っ黒の画像が使われたり、自然言語のタスクではすべてを0にしたembeddingが使われたりします（これは手法によって異なります）。つまり、真っ黒の何も写っていない画像に比べて猫の写った画像はこういう風に異なるから、これは猫の画像と判断したんだな、というように考えていくことになります。\n2つの公理 特徴量の寄与を求める既存手法の中でも勾配を用いた手法というのは多いです。しかしながら、論文中では勾配を用いた既存手法には問題があると指摘しています。 例えばGuided back-propagationは次のSensitivity(a)を満たしていませんし、DeepLiftはImplementation Invarianceを満たしていません。\nSensitivity(a) Sensitivity(a)の定義は以下のとおりです（ちなみにaと書いてあるのはbもあるということです。詳しく知りたい方は論文を参照ください）。\n Sensitivity(a): 入力値に対する出力がbaselineの出力と異なったとき、baselineと異なる値をもつ入力の特徴量の寄与は非ゼロである。\n 次のような例を考えると、勾配を用いる手法におけるSensitivity(a)の必要性がわかります。 $f(x) = 1 - {\\rm Relu}(1-x)$というネットワークを考えます。baselineが$x=0$、入力値が$x=2$とします。$f(0)=0$、$f(2)=1$となりますのでbaselineとは出力値が変わっています。しかしながら、$x=2$では勾配が$0$になりますので、例えば「勾配×入力値」で寄与を求める場合、寄与も$0$になります。 baselineに比べて出力値が変わったのに、寄与が$0$というのはおかしい結果だというのは納得いく話かなと思います。 このため、Sensitivity(a)は寄与を求める手法として満たすべきものだと著者は主張しています。\n \nImplementation Invariance Implementation Invarianceの定義は以下のとおりです。\n Implementation Invariance: 実装方法が異なっていても、同じ入力に対しては求まる寄与値は等しい。\n 具体例を次に示します。\nImplementation Invarianceの例例えば勾配${\\partial f}/{\\partial x}$を計算する手法の場合、この計算は隠れ層の出力$h$を使って、 $$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial h}\\frac{\\partial h}{\\partial x}$$ とあらわせます。 勾配を求める際に${\\partial f}/{\\partial x}$を直接計算しても、連鎖律を使って右辺の計算を用いても結果は一緒になります。 このケースはImplementation Invarianceを満たします。\nImplementation Invarianceではない例DeepLiftの場合は離散化した勾配を用いて寄与を計算します。 連続値を扱っている限りは連鎖律が成り立ちますが、離散化すると連鎖律が成り立たなくなります。 つまり、 $$ \\frac{f(x_1) - f(x_0)}{x_1 - x_0} \\neq \\frac{f(x_1) - f(x_0)}{h(x_1) - h(x_0)} \\frac{h(x_1) - h(x_0)}{x_1 -x_0}$$ となります。 このように計算方法（実装方法）によって結果が変わる場合はImplementation Invarianceを満たしません。\nImplementation Invarianceを満たさないことの問題点 Implementation Invarianceを満たさないことの問題点って何なんでしょうか。論文中で指摘されているのは次のようなケースです。 あるモデルのパラメータ数が多いなどが理由で、自由度が非常に高いモデルがあるとします。このモデルを学習した結果として、同じ入力に対して出力が同じになるが、パラメータの値が異なる組み合わせであるような学習済みモデル1とモデル2の2つが得られたとします。 このような状況で2つのモデルに対する離散化された勾配は、モデル1の隠れ層を$h_1$、モデル2の隠れ層を$h_2$としたとき、 $$ \\frac{f(x_1) - f(x_0)}{h_1(x_1) - h_1(x_0)} \\frac{h_1(x_1) - h_1(x_0)}{x_1 -x_0} \\neq \\frac{f(x_1) - f(x_0)}{h_2(x_1) - h_2(x_0)} \\frac{h_2(x_1) - h_2(x_0)}{x_1 -x_0}$$ となりえます。なぜかといえば、Implementation Invarianceではないので、どちらも$ {f(x_1) - f(x_0)}/{x_1 - x_0}$とは異なる何らかの値になるためです。 入力と出力が同じであるのに、モデルによって寄与が異なるというのは確かに違和感がありますね。たしかにImplementation Invarianceも満たすべきであるといえそうです。\nIntegrated Gradients Integrated Gradientsは前述した2つの公理を満たす手法になります。\nアルゴリズム 手法は単純で、また実装も簡単です。 Integrated Gradientsではbaseline $x'$から入力$x$までの勾配を積分し、入力とbaselineとの差と積を取るだけです。式であらわすと以下のようになります。 $$ {\\rm Integrated\\ Gradients} = (x - x') \\int_{0}^{1} \\nabla F(x' + \\alpha(x - x')){\\rm d} \\alpha .$$\n上記のように勾配の積分を寄与とすることで、baselineから入力$x$までの勾配をすべて考慮することができ、その結果としてSensitivity(a)を満たすことになります。またIntegrated Gradientsは勾配と積分から成りますので、Implementation Invarianceも満たされます。\nコンピュータ上では上記の積分をそのまま実行することはできませんので、実際には数値積分をして、近似値を求めることになります。数値積分を厳密にやろうとするほど、計算量が多く掛かることに注意してください。\nIntegrated Gradientsの適用結果 画像の分類問題の例 GoogleNetを使って画像の分類問題を学習させ、それにIntegrated Gradientsを適用した結果が以下のとおりです。\n  一番左が入力画像で、その隣にラベルとスコアが書いてあり、3列目がIntegrated Gradients×入力画像、4列目が勾配×入力画像です。 これを見ると、単に勾配を用いる場合に比べて、物体を認識するのに必要そうな箇所が寄与していると判定されていることがわかります。例えば、2行目のfireboatは勾配の場合よりも、Integrated Gradientsのほうがより水しぶきの細かい部分に着目していると判定できています。\nテキストの分類問題の例 次にテキストの分類問題の例です。 質問の答えがどういった種類の回答かを予測する問題です。例えば答えが数値なのか、日付なのかなどを予測します。\n下が予測モデルにIntegrated Gradientsを適用した結果です。  \n赤いほど予測への寄与が大きい単語となっています。 結果を見てみると、疑問詞やyearなどに着目していることがわかります。それらしい結果になっていますね。\nおわりに Integrated GradientsではDeepLiftのココがダメと言及している一方で、DeepLiftはIntegrated Gradientsの性能が低いと指摘しています。使う側は難しいですね。\n","date":"2019-12-08T16:17:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/33ce0e44d5ce1595ba0980aaa9a27c83_hua0d4d4b24f4da3a102e4452ac738b95f_358238_120x120_fill_q75_box_smart1.jpg","permalink":"https://opqrstuvcut.github.io/blog/p/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/","title":"ディープラーニング向けの特徴量の寄与を求めるIntegrated Gradientsの解説"},{"content":"本記事はQrunchからの転載です。\n CNNの表現能力の高さはすばらしいものがありますが、何でもうまくいくわけではありません。例えば、画像中の位置情報を考慮しないと解けないような問題は、通常のCNNではうまく対応できません（具体的な例はこの後説明します）。\nこのような問題に対応した手法としてCoordConvというものがあります。CoordConvは座標情報をCNNのなかに組み込む手法で、これを使うことで解けるようになるケースや性能が大きく改善されるようなケースがあります。また「効くか分からないけど、とりあえず組み込む」ということをしても、デメリットはそれほどありません。\n今回はこのCoordConvの紹介です。\n論文：https://arxiv.org/pdf/1807.03247.pdf Keras実装：https://github.com/titu1994/keras-coordconv\nPyTorch実装：https://github.com/mkocabas/CoordConv-pytorch\nちなみに、Keras実装は使ったことがありますが、いい感じに仕事してくれました。\n通常のCNNだと解けない問題 解けない問題の紹介 以下の図は論文で示されている、通常のCNNではうまく解けない、あるいは性能が悪い問題設定です。\n  \n  Supervised Coordinate Classification は2次元座標xとyを入力として2次元のグレイスケールの画像を出力する問題です。入力の(x,y)の座標に対応するピクセルだけが1、それ以外のところは0になるように出力します。出力されるピクセルの数の分類問題となります。 Supervised Renderingも画像を出力しますが、入力(x,y)を中心とした9×9の四角に含まれるピクセルは1、それ以外は0になるように出力します。 Unsupervised Density LearningはGANによって赤か青の四角と丸が書かれた画像を出力する問題となります。 上記の画像にはないのですが、Supervised Coordinate Classification の入力と出力を逆にした問題も論文では試されています。つまり、1ピクセルだけ1でそれ以外は0であるようなone hot encodingを入力として、1の値をもつピクセルの座標(x,y)を出力するような問題です。  Supervised Coordinate Classificationを通常のCNNで学習させた結果 Supervised Coordinate Classificationを通常のCNNで学習させたときの結果を示します。\n訓練データとテストデータの分け方で2種類の実験をおこなっています。\n1つは取りうる座標全体からランダムに訓練データとテストデータに分けたケースです。もう一つは座標全体のうち、右下の部分をテストデータにし、それ以外を訓練データとするケースです。これをあらわしたのが、それぞれ以下の図のUniform splitとQuadrant splitになります。\n  上記の2つのパターンでそれぞれ訓練データでCNNを訓練し、accuracyを計測した結果が以下の図になります。\n  \n 1つの点が1つの学習されたモデルでの訓練データとテストデータのaccuracyに対応しています（多分それぞれのモデルはハイパーパラメータが異なるのですが、はっきりと読み取れませんでした）。\nこのグラフから、Uniform splitのときには訓練データのaccuracyは1.0になることがあっても、テストデータは高々0.86程度にしかならないことがわかります。また、Quadrant splitのときにはさらにひどい状況で、テストデータはまったく正解しません（ほとんど0ですね）。\n問題設定を見ると、一見簡単な問題のように思えますが、実際には驚くほど解きにくい問題であることがわかります。\nUnsupervised Density Learningを通常のCNNで学習させた結果 次にGANのケースも見てみます。\n学習データでは青の図形と赤の図形はそれぞれ平面上に一様に分布します。下図の上段右がそれを示しており、赤の点と青の点がそれぞれの色の図形の中心位置をプロットしたものです。GANで生成する画像もこのように、図形が一様に色々なところに描かれて欲しいところです。\nしかしながら、CNNを使ったGANのモデルが生成した画像では赤の図形と青の図形の位置の分布には偏りがあります（モード崩壊）。下図の下段右がこれを示しています。  \nCoordConv 前述の問題はなぜ解きにくいのでしょうか。\n理由としては、CNNでは畳み込みの計算をおこなうだけであり、この畳み込みの計算では画像中のどこを畳み込んでいるのかは考慮できておらず、座標を考慮する必要がある問題がうまく解けないということが挙げられます。\n座標を考慮できていないから解けないならば、畳み込むときに座標情報を付与すればよいのでは、というのがCoordConvの発想です。\n具体的には以下の右の層がCoordConvになります。  \n通常のCNNとの違いは、画像の各ピクセルのx軸の座標をあらわしたチャネル（i coordinate）とy軸の座標をあらわしたチャネル（j coordinate）を追加するということだけです。ただし、それぞれのチャネルの値は[-1,1]に正規化されています。\n例えば、5×5の画像の場合では、x軸の座標をあらわしたチャネル（i coordinate）は以下のような行列になります。\n$$ {\\rm (i \\ coordinate)} = \\begin{bmatrix} -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ \\end{bmatrix} $$\nまた、y軸の座標をあらわしたチャネル（j coordinate）は以下のような行列になります。\n$${\\rm (j \\ coordinate)} = \\begin{bmatrix} -1 \u0026amp; -1 \u0026amp; -1 \u0026amp; -1 \u0026amp; -1 \\\\ -0.5 \u0026amp; -0.5 \u0026amp; -0.5 \u0026amp; -0.5 \u0026amp; -0.5 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0.5 \u0026amp; 0.5 \u0026amp; 0.5 \u0026amp; 0.5 \u0026amp; 0.5 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ \\end{bmatrix} $$\nまた、画像の中心からの距離をあらわしたチャネルを追加することでも性能が向上するようで、論文ではこちらも利用されています。\nCoordConvによってすべてのCNNを代替すべきなのか、一部にしてもどこを置き換えるべきなのかは議論の余地があるかもしれませんが、例えばSupervised Coordinate Classificationの問題では、次の緑の部分にCoordConvが使われています。  \nちなみにCoordConvの性能面に関して、論文中では以下の2点に関して言及されています。\n 追加されたチャネル分の畳み込みが増えるだけですので、それほど計算量は大きくなりません。 CoordConvによる性能の悪影響があり得るんじゃないかと思えますが、そのような場合には重みが0に近づくように学習されるはずなので、予測結果に悪影響を与えないはず。  CoordConvの効果 CoordConvの効果について実験結果を述べていきます。\nSupervised Coordinate Classificationの結果 Supervised Coordinate Classificationの結果が以下のようになります。   Convolutionの行が通常のCNNの場合、CoordConvの行がCoordConvを使った場合の結果です。\nCoordConvではデータの分割方法によらず、accuracyが1になり、非常にうまく問題が解けるようになります。また、収束性も非常によくなり、通常のCNNでは4000秒かかってテストデータのaccuracyが0.8を超えていますが、CoordConvでは20秒でaccuracyが1になっています。\nUnsupervised Density Learningの結果 GANの結果が以下のようになります。   3行目のCoordConvを導入したGANでは赤と青の図形の位置の偏りが緩和されていることがわかります。 ただしc列が2つの図形の中心座標の差を示しているのですが、これを見ると、残念ながらまだ分布に偏りがあるといえそうです。\n強化学習の結果 論文では実際の問題にも適用して有効性を確認しています。 強化学習で使われているCNNをCoordConvに置き換えてatariのゲームを学習させています。結果は以下の通りです。  \n縦軸がゲームのスコアだと思いますが、9つのゲームのなかで、6つは性能が向上し、2つは変わらず、1つは悪くなった（理屈の上では悪影響がでないはずですが…？）という結果になりました。パックマンなど一部のゲームは非常に性能が良くなっていますね。\n終わりに CoordConvは問題によっては非常に有用です。座標を考慮したほうがが良いと思ったら、とりあえず利用するといいと思います。 既存のCNNをCoordConvに置き換えるのも簡単です！\n","date":"2019-11-30T21:57:17+09:00","image":"https://opqrstuvcut.github.io/blog/p/cnn%E3%81%A7%E7%94%BB%E5%83%8F%E4%B8%AD%E3%81%AE%E3%83%94%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AE%E5%BA%A7%E6%A8%99%E6%83%85%E5%A0%B1%E3%82%92%E8%80%83%E6%85%AE%E3%81%A7%E3%81%8D%E3%82%8Bcoordconv/87ac6257d733ab494c7d120ec4e79a99_huad0610d545df126cbc0dbaec61a8f428_203414_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/cnn%E3%81%A7%E7%94%BB%E5%83%8F%E4%B8%AD%E3%81%AE%E3%83%94%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AE%E5%BA%A7%E6%A8%99%E6%83%85%E5%A0%B1%E3%82%92%E8%80%83%E6%85%AE%E3%81%A7%E3%81%8D%E3%82%8Bcoordconv/","title":"CNNで画像中のピクセルの座標情報を考慮できるCoordConv"},{"content":"本記事はQrunchからの転載です。\n 逆行列を使った計算というのは機械学習ではそれなりに出てきます。 例えば、最小二乗法では $$ x = (X^T X) ^{-1} Xb$$ の形の式を計算する必要がありますし、正規分布の分散を扱うときにも逆行列が出てきます。 こういうときにnp.linalg.invを使って逆行列を求めて、その後にベクトルとの積を求めるは簡単にできますから、特に何も考えずにそういうふうにしたくなります。\nでもそれって本当に逆行列の計算が必要ですか？多くの問題では逆行列の値そのものよりも、$x=A^{-1}b$のような逆行列とベクトルとの積が必要になります。そのような場合、実は計算はもっと速くできますよ、というのが今日のお話です。\nただし今回は式を深く追うことはしませんので、細かい計算量などが気になる方は別途どこかの講義資料などの参照をお願いします。\n逆行列を求めるための計算量 逆行列を求めるための方法として多くの人が思いつくのが、おそらく線形代数の教科書に載っている掃き出し法でしょう。掃き出し法は逆行列を求めたい行列$A$に対して操作をおこない、単位行列にしていくやり方ですね。 行列$A$のサイズを$n \\times n$としたとき、掃き出し法に必要な乗除算は$n^3$回、引き算は$n(n-1)^2$回です。 また別途、行列$b$との積を計算する場合には乗算が$n^2$回、足し算が$n(n-1)$回かかることに注意してください。\n実際にはnp.linalg.invはこの方法ではなく、後述する方法を利用して（半ば無理やり？）逆行列を求めますが、そうしても計算量は上記と同じ程度になります。\n連立一次方程式を解く方法 $x=A^{-1}b$の計算は、$Ax=b$の形をした連立一次方程式とみなすことができます（$x=A^{-1}b$の両辺に左から$A$を掛けるとわかりますね）。よって、連立一次方程式が解ければ、逆行列を求める必要はないということです。\n以下ではnp.linalg.solveでもおこなわれている、LU分解と前進後退代入を使った連立一次方程式の解き方について述べます。\nLU分解 行列$A$に対してLU分解をおこなうことを考えます。LU分解というのは下三角行列$L$と上三角行列$U$の積に行列$A$を分解することを指します。つまり、$$A = LU$$が成り立つような$L$と$U$を求めます。\nLU分解の計算量は乗除算が$(n-1)(n^2+n+3)/3$回で引き算が$n(n-1)(2n-1)/6$回です。ここまでは先程出てきた逆行列を求めるための計算量よりも大分少ない計算量です。\nもちろんLU分解だけでは連立一次方程式は解けず、次の前進後退代入をおこなう必要があります。\n前進後退代入 LU分解が済んでいるとすると、$Ax=b$は$LUx=b$とあらわせます。$y=Ux$とおいてあげると、 $$Ax=LUx= Ly=b$$ となりますので、$Ly=b$の連立一次方程式が出てきます。これを$y$について解くと次に $$Ux = y$$ の連立一次方程式があらわれます。最後にこれを$x$について解くことで、ようやく欲しかった$x$が求まります。\n$Ly=b$と$Ux=y$という連立一次方程式を解くなんて計算が重そうだ！と思うかもしれません。 しかしながら、$L$は下三角行列、$U$は上三角行列であるということを考慮するとそれほど計算量は多くなりません。実際、\n $Ly=b$を求める計算（前進代入）：乗算$n(n-1)/2$回、加減算$n(n-1)/2$回 $Ux=y$を求める計算（後退代入）：乗除算$n(n+1)/2$回、加減算$n(n-1)/2$回 上2つの計算量の和：乗除算$n^2$回、加減算$n(n-1)$回  となります。なんとこれは前述した$A^{-1}$を$b$に掛けるときの計算量と等しいです！ 一見大変そうな計算をしているのに、実は行列とベクトルの積と同じ計算量だなんて驚きです。\nLU分解と前進後退代入から逆行列を求める方法 np.linalg.invでは連立一次方程式の計算を利用して逆行列を求めるといいました。これは単位行列$E$を右辺とした連立一次方程式を解くことを指しています。つまり以下の方程式です（右辺と解$X$が行列になりますが、単純に列の分だけ解くべき方程式が増えたと思えばOKです）。 $$A X = E.$$ この方程式を解くと、$X = A^{-1}$となるのがわかりますね。\nこの方法の前進後退代入の計算量は乗除算$n(2n^2+1)/3$回、加減算$n(n-1)(4n-5)/6$回となります（この計算量の計算は結構大変…）。 LU分解の計算量との合計は乗除算が$n^3 + n- 1$回、加減算が$n(n-1)^2$回となります。掃き出し法と比べて乗除算が$n-1$回増えますが、$n$が大きくなれば無視できる程度の差です。\n計算量のまとめ 計算量についてまとめると、以下のようになります。\n   方法 乗除算 加減算     掃き出し法による逆行列の計算 $n^3$ $n(n-1)^2$   行列とベクトルの積 $n^2$ $n(n-1)$   LU分解 $(n-1)(n^2+n+3)/3$ $n(n-1)(2n-1)/6$   前進後退代入 $n^2$ $n(n-1)$   LU分解+前進後退代入による逆行列の計算 $n^3+n-1$ $n(n-1)^2$    LU分解と前進後退代入によって$Ax=b$を解いた場合の計算量では$n^3$に$1/3$がかかっていますから、「逆行列を求める+ベクトルとの積を計算する」の場合に比べて$1/3$程度計算量が減ることがわかります。\nNumPyで実験 実際にNumPyで計算時間を比較してみましょう。 以下のようにして行列とベクトルを作ります。\nimport numpy as np A = np.random.rand(1000, 1000) b = np.random.rand(1000) 次に、計算にかかった時間をそれぞれ測ります。\n 逆行列を求める+ベクトルとの積を計算  %%timeit inv_x = np.dot(np.linalg.inv(A), b) 結果：80.8 ms ± 4.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n 連立一次方程式を解く  %%timeit solve_x = np.linalg.solve(A, b) 結果：27.7 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nおおよそ3倍くらいの差がつきましたね！\nまとめ 連立一次方程式の形に落とし込める場合には、逆行列を求めずに、連立一次方程式として解いてあげましょう！実は、計算量が減ることで数値誤差が増えづらくなり、精度面も連立一次方程式のほうが有利と言われています。色々な面で逆行列を計算するメリットはないのです。\n参考書籍：伊理 正夫、藤野 和建．数値計算の常識.\n","date":"2019-11-15T01:35:01+09:00","permalink":"https://opqrstuvcut.github.io/blog/p/%E5%AE%89%E6%98%93%E3%81%AB%E9%80%86%E8%A1%8C%E5%88%97%E3%82%92%E6%95%B0%E5%80%A4%E8%A8%88%E7%AE%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AF%E3%82%84%E3%82%81%E3%82%88%E3%81%86/","title":"安易に逆行列を数値計算するのはやめよう"},{"content":"本記事はQrunchからの転載です。\n AWSのS3を使うようなシステムを開発するときに、S3と連携する部分だけAWSにつなぐより、ローカルにS3が欲しいなぁってふと思いました。でもそんな都合が良い話があるわけないよなぁ、なんて思ったら実はありました！その名もMinIO。 今回はMinIOの使い方を簡単にご紹介します。とても簡単です。\nMinIOのページはこちら。https://min.io\n導入 自分はDockerを利用しましたので、Docker経由での使い方になります。 Dockerは嫌だという場合には公式のページをご確認下さい。https://docs.min.io/\n Dockerをインストール。 Dockerを入れていない人はこの機会にぜひ入れましょう！今使っていなくとも、きっといつの日か別の機会にも使うんじゃないかと思います。インストールにはこの辺が参考になりそうです。http://docs.docker.jp/engine/installation/docker-ce.html# ターミナル等で次を実行して、MinIOのサーバを立ち上げる。  docker run -p 9000:9000 \\ --name minio_test \\ -e \u0026quot;MINIO_ACCESS_KEY=access_key_dayo\u0026quot; \\ -e \u0026quot;MINIO_SECRET_KEY=secret_key_dayo\u0026quot; \\ minio/minio server /data MINIO_ACCESS_KEYがAWSのアクセスキーで、MINIO_SECRET_KEYはシークレットキーに対応します。都合がよいように決めましょう。\n上のコマンドの初回実行時にはdocker imageのdownloadなどが走るのでちょっと時間がかかります。\n（Dockerを知らない人向け）アクセスするときにポートが9000は嫌だという人は、9000:9000の左側の数字を変えましょう。例えば8888:9000とかです。\n実行がうまくいくと次のようなメッセージが表示されるかと思います。これでS3のようなものができました！すごく簡単\nhttp://127.0.0.1:9000 からMinIOのサーバにアクセスできるはずです。\nEndpoint: http://172.17.0.2:9000 http://127.0.0.1:9000 Browser Access: http://172.17.0.2:9000 http://127.0.0.1:9000 Object API (Amazon S3 compatible): Go: https://docs.min.io/docs/golang-client-quickstart-guide Java: https://docs.min.io/docs/java-client-quickstart-guide Python: https://docs.min.io/docs/python-client-quickstart-guide JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide .NET: https://docs.min.io/docs/dotnet-client-quickstart-guide 使ってみる ブラウザで利用 アクセス ブラウザで http://127.0.0.1:9000 にアクセスすると次のような画面が表示されます。\nAccess KeyとSecret Keyはdocker runコマンドのときに指定したMINIO_ACCESS_KEYとMINIO_SECRET_KEYの値を入れましょう。これでログインできます。\n \nログインすると以下のような画面になります。  \nバケット生成 ここでAWSのS3のバケット相当のものが作れます。\n右下の+マークを押して、Create bucketを選択後、バケット名を入力すればOKです。この手順で、例えばtestという名前のバケットを作ると以下のようになります。   左側に生成したバケットが表示されていますね。\nファイルを配置 画面の上のほうにファイルをドラッグするとこのバケット内にファイルが置けます。  \nboto3で利用 boto3を使ってS3の操作をおこないます。Python3を例にあげます。\nS3と接続するためのオブジェクト生成 以下のclientの引数のaws_access_key_idとaws_secret_access_keyには、docker runしたときに設定したMINIO_ACCESS_KEYとMINIO_SECRET_KEYをそれぞれ指定しましょう。\nimport boto3 s3_client = boto3.client(\u0026quot;s3\u0026quot;, endpoint_url=\u0026quot;http://127.0.0.1:9000\u0026quot;, aws_access_key_id=\u0026quot;access_key_dayo\u0026quot;, aws_secret_access_key=\u0026quot;secret_key_dayo\u0026quot;) ファイル検索 ファイルの検索を試します。以下のように、作ったBucketを指定してやります。\ncontents = s3_client.list_objects(Bucket=\u0026quot;test\u0026quot;, Prefix=\u0026quot;\u0026quot;).get(\u0026quot;Contents\u0026quot;) print(contents) 上記を実行すると、次のように表示されました。配置したファイルがちゃんと見つけられますね！\n[{'Key': 'テキストたよ.txt', 'LastModified': datetime.datetime(2019, 11, 9, 3, 7, 27, 360000, tzinfo=tzutc()), 'ETag': '\u0026quot;59681d790d132065f97faf0f52e9aa41-1\u0026quot;', 'Size': 27, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': '', 'ID': '02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4'}}] ファイル取得 ファイルの取得も試しましょう。以下を実行します。\ntext = s3_client.get_object(Bucket=\u0026quot;test\u0026quot;, Key=\u0026quot;テキストたよ.txt\u0026quot;)[\u0026quot;Body\u0026quot;].read().decode('utf-8') print(text) 結果：\nテキストの中身だよ 正しくテキストを取得できています！\nまとめ MinIOはとても簡単に使えて、便利だと思いました。よくできていて感動しますね。 手元の環境にS3が欲しいなぁって思った方は使いましょう！\n","date":"2019-11-09T12:48:01+09:00","image":"https://opqrstuvcut.github.io/blog/p/minio%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%81%ABs3%E3%81%BF%E3%81%9F%E3%81%84%E3%81%AA%E3%82%82%E3%81%AE%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%A6%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B/2d89cc4c8b3b3d34194b32b843ff40bf_hud6cb81a775978a88f5a41e08842d2fb5_16521_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/minio%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%81%ABs3%E3%81%BF%E3%81%9F%E3%81%84%E3%81%AA%E3%82%82%E3%81%AE%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%A6%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B/","title":"MinIOでローカルにS3みたいなものを作って開発する"},{"content":"本記事はQrunchからの転載です。\n 概要 自然言語界隈では非常によく話題になるBERTですが、BERTを使った文生成を実装してみたので今回はその話をします。BERTの事前学習モデルが文生成のタスクで使えたら、比較的少なめの学習データでもそれっぽく文生成できたりしないかなぁと思ってやってみました。\n実験ではポケモンの説明文を学習させて、生成させてみました。ちなみに自分はポケモンはルビー・サファイアで止まってますので、あんまりポケモンは分からないです。（他に面白そうな題材が見つからず…。遊戯王の通常モンスターの説明文でやりたかったんですが、データ数が700弱と少なすぎて断念。）\n参考にした論文：BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model\n使用した事前学習モデル：BERT日本語Pretrainedモデル\n実装したソースコード：https://github.com/opqrstuvcut/BertMouth\nBERTでの文生成 学習 学習は以下のようなネットワークを使っておこないます。  \nネットワークへの入力となる各トークンはサブワードになります。\n例えば今回のように京都大学の事前学習モデルを利用する場合には、「何日だってなにも食べなくても元気 ！背中のタネ にたくさん栄養があるから元気だ！」という文はJuman++で形態素解析された後、サブワードに分割され、\n何/日/だって/なに/##も/食べ/なくて/も/元気/！/背中/の/タ/##ネ/に/たくさん/栄養/が/ある/から/元/##気/##だ/！ となります。\n上記のネットワークを使って、ランダムにマスクした部分のサブワードの確率が予測できるように、以下の手順を繰り返して学習をすすめていきます。\n ある文がN個のトークンから構成されているときに、ランダムに1つのトークンを[MASK]に置き換える（上の図の例だと2番目のトークンがこれに該当します）。 1つのトークンを[MASK]に置き換えたトークン列をBERTに与える。 BERTの出力のうち、[MASK]に対応するトークンの出力O[MASK]に対して全結合層とsoftmaxを適用する（softmaxの結果が全サブワードの出現確率になります）。 求められた[MASK]に対応する出現確率のうち、正解となるサブワードの確率が高くなるように、クロスエントロピーを用いて最適化する。  予測 予測は次のようにギブスサンプリングを使います。\n 長さNのトークン列を初期化する。 以下を適当な回数繰り返す。  次を全トークンに対しておこなう。  i番目(i=1,\u0026hellip;,N)のトークンを[MASK]で置き換え、学習したネットワークに入力する。 出現確率が最大のサブワードで[MASK]のトークンを置換する。      トークン列の初期化には全サブワードから一様分布に従ってサンプリングしていますが、人間が適当な文を入れてあげてもいいですし、色々やりようはあるかと思います。\n実験 データ 学習には https://wiki.ポケモン.com/wiki/ポケモン一覧 のポケモンの説明文から、漢字が使われている文のみを利用しています。訓練データに使われたのは4730文で、例えば以下のような文が含まれます。\n 生まれたときから 背中に 不思議な タネが 植えてあって 体と ともに 育つという。 トレーナーとの 絆が パワーの 源。 ジェット機を しのぐ 飛行能力を 誇る。  こんな感じのポケモンの説明文を自動で生成できたら面白いなぁと思ったので、このデータでやってみました。うまく行けば架空のポケモンが作れますね！\n結果 学習したモデルで予測した結果を示します。ちなみに予測するときにサブワードの数をあらかじめ指定しますが、以下の例ではサブワードの数は20です。\n生成文1: 弱い獲物を一度捕まえると止まらない。毎日１８時間鳴くチビノーズ。弱い獲物をいたぶっているのか、猟奇的な感じがします。\n生成文2: この姿に変化して連れ去ることでお腹を自在に操るピィができるのだ。お腹を自由に操る…？化して連れ去るあたりは悪いポケモン感が出ていていいですね。\n生成文3: ボールのように引っ張るため１匹。だが１匹ゆらゆら数は少ない。ちょっと解釈が難しいです。孤高の存在？\n生成文4: 化石から復活した科学者を科学力で壊し散らす生命力を持つポケモン。科学力で科学者に勝利するインテリポケモン。\n生成文5: ただ絶対に捕まえないので傷ついた相手には容赦しない。なぜだか。これは解釈が難しいですが、恐ろしいポケモン感がでてますね。「なぜだか。」がいいアクセントです。\nまとめ それっぽい文はできたけども、意味があまり通らない文が多いかなという印象です。とりあえず学習データが少ないので、文が多い他のデータで実験します。気力のある方はぜひ自分でデータを用意して、学習してみて結果を教えて欲しいです！\nおまけ 今回自分が使った京都大学の事前学習モデルを利用して学習する場合は、以下の手順で学習データを用意できます。\n  文を集めてきて、次のようなフォーマットのテキストファイルに保存する。\n文1 文2 ︙ 文N   juman++、pyknp、mojimojiをインストールする。pyknpとmojimojiはpipでOKです。\n  レポジトリにあるpreprocess.pyを次のように実行して、形態素解析と前処理をおこなう。\n python ./preprocess.py \\ --input_file 1で作ったテキストファイルのパス \\ --output_file 出力先のテキストファイルのパス \\ --model xxx/jumanpp-2.0.0-rc2/model/jumandic.jppmdl（jumanのモデルのパスが通っている場合は不要）   出力されたファイルを訓練データと検証データに適当に分割する。\n  ","date":"2019-11-07T11:42:23+09:00","image":"https://opqrstuvcut.github.io/blog/p/bert%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%81%AE%E8%AA%AC%E6%98%8E%E6%96%87%E7%94%9F%E6%88%90/729b98aa8f9032f789244aa4e870b844_hua413e8993b42675a600325789a072244_97816_120x120_fill_box_smart1_2.png","permalink":"https://opqrstuvcut.github.io/blog/p/bert%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%81%AE%E8%AA%AC%E6%98%8E%E6%96%87%E7%94%9F%E6%88%90/","title":"BERTでおこなうポケモンの説明文生成"}]