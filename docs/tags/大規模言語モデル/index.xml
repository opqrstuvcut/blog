<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>大規模言語モデル on MatLoverによるMatlab以外のブログ</title>
        <link>https://opqrstuvcut.github.io/mblog/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/</link>
        <description>Recent content in 大規模言語モデル on MatLoverによるMatlab以外のブログ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja-jp</language>
        <lastBuildDate>Mon, 28 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://opqrstuvcut.github.io/mblog/tags/%E5%A4%A7%E8%A6%8F%E6%A8%A1%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>「Hidden in plain sight： VLMs overlook their visual representations」の論文紹介</title>
        <link>https://opqrstuvcut.github.io/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</link>
        <pubDate>Mon, 28 Jul 2025 00:00:00 +0000</pubDate>
        
        <guid>https://opqrstuvcut.github.io/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/</guid>
        <description>&lt;img src="https://opqrstuvcut.github.io/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/thm.png" alt="Featured image of post 「Hidden in plain sight： VLMs overlook their visual representations」の論文紹介" /&gt;&lt;p&gt;今回紹介するのは
&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2506.08008&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Hidden in plain sight: VLMs overlook their visual representations&lt;/a&gt;
です.&lt;/p&gt;
&lt;p&gt;テキストの生成というよりも画像が中心となるタスクに対し、オープンソースのVisual Language Modelの性能について調査した論文になっています.
DINOやCLIPをLLMに組み込んだマルチモーダルモデルは、単体のViT系のモデルよりも性能が&lt;strong&gt;大きく下がる&lt;/strong&gt;ことを示しています.&lt;/p&gt;
&lt;p&gt;例えば、次の図では左の2枚の画像が与えられ、上の画像の「Ref」と書かれている点と同じ点は下の画像のA~Dの4つの点のどれか？というのを当てる問題を解くことを考えます.&lt;br&gt;
DINOやCLIP単体によって問題を解いたとき、DINOでは80%、CLIPでは60%程度のAccuracyでしたが、VLMを用いるとチャンスレート（適当に答えたときの性能）よりも低くなってしまいます.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig1.png&#34;
	width=&#34;1337&#34;
	height=&#34;824&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig1_hu2620225380482079590.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig1_hu5169817889437821056.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;性能比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;162&#34;
		data-flex-basis=&#34;389px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;llavaによるvlmの実現&#34;&gt;LLaVAによるVLMの実現
&lt;/h2&gt;&lt;p&gt;まず、VLMはどのように実現されているかの話になります.&lt;br&gt;
本論文で扱われている&lt;a class=&#34;link&#34; href=&#34;https://github.com/TRI-ML/prismatic-vlms&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;LLaVA&lt;/a&gt;ではDINOのようなVisualモデルから得られた画像のトークン列をLLMのembeddingの空間にマッピングするようなProjector層を追加しています.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/llava_arch.png&#34;
	width=&#34;1607&#34;
	height=&#34;542&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/llava_arch_hu504353733132962031.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/llava_arch_hu16042321218087387222.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLaVAの構成&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;296&#34;
		data-flex-basis=&#34;711px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;LLaVAでのファインチューニングは次の2段階の処理から構成されるようです.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Projector層単体ののファインチューニング&lt;/li&gt;
&lt;li&gt;EndToEndのファインチューニング&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;比較実験の方法&#34;&gt;比較実験の方法
&lt;/h2&gt;&lt;p&gt;LLaVAはEndToEndでファインチューニングしていますが、本論文ではProjector層のファインチューニングした場合のVLMとVisualモデルの比較を中心におこなっています.これは、VLMのVisualモデルの重みを固定しておくことで、VLMとVisualモデル単体との正確な比較をおこなうようにするためです.&lt;br&gt;
ただし、VLM用にEndToEndでファインチューニングされた既存のオープンソースのVisualモデルでさえも性能悪化の傾向があることを示すため、 QwenやPhi-3などでも一部の実験をおこなっています.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;タスク&#34;&gt;タスク
&lt;/h2&gt;&lt;p&gt;以下では扱っているタスクの一覧を示します.&lt;br&gt;
タスクの具体例をあらわす画像は論文のなかのVisualモデルとVLMが間違った例を用いています.&lt;/p&gt;
&lt;h3 id=&#34;カメラ距離を推定するタスク&#34;&gt;カメラ距離を推定するタスク
&lt;/h3&gt;&lt;p&gt;どちらのBounding Boxのほうがカメラに近いかを判定するタスク.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/depth_estimation.png&#34;
	width=&#34;1320&#34;
	height=&#34;264&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/depth_estimation_hu10820540876376682905.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/depth_estimation_hu10060460141892101579.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;カメラ距離&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;500&#34;
		data-flex-basis=&#34;1200px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLMにはPromptとBounding Box付きの画像を入力し、どちらのBounding Boxがカメラに近いかを出力させる.&lt;/li&gt;
&lt;li&gt;Visualモデルはそのままだと解くことができないため、NYUv2という深度を推定するタスクのデータセットを用いてVisualモデルにDPT Headを追加して訓練する.
&lt;ul&gt;
&lt;li&gt;DPT Headは奥行きの推定の出力部分&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;視覚的な類似箇所を推定するタスク&#34;&gt;視覚的な類似箇所を推定するタスク
&lt;/h3&gt;&lt;p&gt;2枚の画像から視覚的に似ている箇所を見つけるタスク.
&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/semantic_correspondence.png&#34;
	width=&#34;1320&#34;
	height=&#34;264&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/semantic_correspondence_hu15590774341794558653.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/semantic_correspondence_hu8339495262947362836.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;類似箇所&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;500&#34;
		data-flex-basis=&#34;1200px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reference画像に1つの点があり、もう一枚の画像にはA,B,C,Dの4つの点を用意する.&lt;/li&gt;
&lt;li&gt;VLMにはどの点が対応するかを出力させる.&lt;/li&gt;
&lt;li&gt;Visualモデルではそのまま解くことはできないため、Reference画像の点の特徴量と一番類似度が高くなる特徴量に対応する点をA~Dから選ぶ.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;機能的な類似箇所を推定するタスク&#34;&gt;機能的な類似箇所を推定するタスク
&lt;/h3&gt;&lt;p&gt;2枚の画像から機能的に似ている箇所を見つけるタスク.
&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/affordance.png&#34;
	width=&#34;1328&#34;
	height=&#34;258&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/affordance_hu8934180305198273882.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/affordance_hu9127482930411852116.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;機能的類似箇所&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;514&#34;
		data-flex-basis=&#34;1235px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;問題の解き方は1つ前のタスクと同じです.&lt;/p&gt;
&lt;h3 id=&#34;同じ位置を推定するタスク&#34;&gt;同じ位置を推定するタスク
&lt;/h3&gt;&lt;p&gt;2枚の照明条件や視点が異なる画像から同じ位置を見つけるタスク
&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/low_level_match.png&#34;
	width=&#34;1317&#34;
	height=&#34;264&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/low_level_match_hu6550707789194667419.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/low_level_match_hu8248087818981904298.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;同定&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;498&#34;
		data-flex-basis=&#34;1197px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;問題の解き方は1つ前のタスクと同じです.&lt;/p&gt;
&lt;h3 id=&#34;最も似ていない3dオブジェクトを推定するタスク&#34;&gt;最も似ていない3Dオブジェクトを推定するタスク
&lt;/h3&gt;&lt;p&gt;4枚の画像から4枚の画像から4枚の画像から4枚の画像から最も似ていない3Dオブジェクトの画像を選択するタスク.
&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/3d.png&#34;
	width=&#34;1328&#34;
	height=&#34;309&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/3d_hu17128352951129553648.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/3d_hu2140973221106283333.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;3D&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;429&#34;
		data-flex-basis=&#34;1031px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VLMは4つのなかで一番似ていない画像を出力させる.&lt;/li&gt;
&lt;li&gt;VisualモデルはVisualモデルの[CLS]トークン部分の特徴量同士の類似度をもとに選択する.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;似ている画風の絵画を選択するタスク&#34;&gt;似ている画風の絵画を選択するタスク
&lt;/h3&gt;&lt;p&gt;与えられた画像に似ている画風の絵画を2枚のなかから選択するタスク.
&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/art.png&#34;
	width=&#34;1317&#34;
	height=&#34;259&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/art_hu17384315567729850040.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/art_hu16838940929286436389.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;art&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;508&#34;
		data-flex-basis=&#34;1220px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;VLMは3つの画像を与えて、似ている画風の画像を出力させる.&lt;/li&gt;
&lt;li&gt;Visualモデルの場合、画像のパッチの各特徴量からグラム行列を作ると画像のstyleを表現できることを利用し、Reference画像と比較対象画像のグラム行列の二乗誤差が小さいものを似ている画風であると選択する.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;実験結果&#34;&gt;実験結果
&lt;/h2&gt;&lt;h3 id=&#34;visualモデルとprojector層をftしたvlmの性能比較&#34;&gt;Visualモデルとprojector層をFTしたVLMの性能比較
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig2.png&#34;
	width=&#34;1327&#34;
	height=&#34;450&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig2_hu11232206505773662476.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig2_hu14443638137478442914.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;VisualモデルとVLMの比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;294&#34;
		data-flex-basis=&#34;707px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualモデル単体に比べてVLMは性能が悪化し、チャンスレートよりも低くなりうる.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;オープンソースのvlmとvisualモデル部分の性能比較&#34;&gt;オープンソースのVLMとVisualモデル部分の性能比較
&lt;/h3&gt;&lt;p&gt;他のVLMでのVisualモデルとの性能比較.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig3.png&#34;
	width=&#34;1330&#34;
	height=&#34;429&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig3_hu10875643566285980402.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig3_hu15147632360257238445.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;オープンソースモデルとの比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;310&#34;
		data-flex-basis=&#34;744px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;この結果でも、基本的にはVisualモデル単体のほうが性能が高い.
&lt;ul&gt;
&lt;li&gt;InternVLの3D Objectの問題のように、そもそもVisualモデルの性能が低い場合には改善することもある.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;以上の結果から、VLMは全く入力画像を正しく参照できていないかもしれない&amp;hellip;&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;目隠ししたケースでの性能&#34;&gt;目隠ししたケースでの性能
&lt;/h3&gt;&lt;p&gt;VLMが画像を参照できているのか確認をするため、通常の状態での出力の分布と入力画像を真っ白の画像にして問題を解かせたときの出力の分布を比較する.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig4.png&#34;
	width=&#34;1330&#34;
	height=&#34;385&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig4_hu8002847656754150763.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig4_hu13136841656957264549.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;目隠ししたケースでの出力分布&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;345&#34;
		data-flex-basis=&#34;829px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;入力画像のある場合（青）と目隠ししたケース（オレンジ）で出力の分布に大きな違いがない.&lt;/li&gt;
&lt;li&gt;もし、入力画像が真っ白である場合には出力がランダムになりそうだが、実際には大きな偏りがある.
&lt;ul&gt;
&lt;li&gt;LLMにバイアスがあり、それがどちらの分布にもあらわれているを思われる.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;以上の結果から、モデルは画像を適切に参照して出力ができておらず、もしかするとVisualモデルの情報が途中で失われている？&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;中間層の特徴量の比較&#34;&gt;中間層の特徴量の比較
&lt;/h3&gt;&lt;p&gt;VLMの層の途中でVisualモデルの情報が失われていないかを確認するための実験.
各層の特徴量ををもとにして類似箇所を見つけるタスクや位置の推定タスクを解く. もしうまく解けるようであれば、特徴量にVisualモデルからの情報が伝搬できている.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig5.png&#34;
	width=&#34;787&#34;
	height=&#34;625&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig5_hu9068197456343288497.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig5_hu11956313488245988528.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;特徴量比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;125&#34;
		data-flex-basis=&#34;302px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;グラフの領域内の左部分（白い領域）はVisualモデルの情報をそのまま利用している部分だが、そこから性能が落ち込んでいる様子はない.
&lt;ul&gt;
&lt;li&gt;つまりVisualモデルの情報が失われているわけではない.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;問題・モデルによっては最終層に近いところで大きな性能悪化が見られる.
&lt;ul&gt;
&lt;li&gt;自然言語の回答の生成の優先度が高くなり 画像の情報が失われる？&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Art StyleのIN-1kでは徐々に性能があがっているため、モデルによっては層を経ることで質の高い特徴量を作り出せている.
&lt;ul&gt;
&lt;li&gt;それにも関わらず先程のFigure3によると、モデルにテキストを生成させて回答させると高くても55%程度のAccuracyになる.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;プロンプトチューニング&#34;&gt;プロンプトチューニング
&lt;/h3&gt;&lt;p&gt;プロンプトによって性能を改善できるのかを実験.
テキストのプロンプトの前に学習可能なembeddingを1、5、10トークン追加する.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig6.png&#34;
	width=&#34;1347&#34;
	height=&#34;323&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig6_hu2712944692912299940.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig6_hu13701780070495995266.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;プロンプトチューニング&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;417&#34;
		data-flex-basis=&#34;1000px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;プロンプトにembeddingを1つ追加したときは性能が少し改善&lt;/li&gt;
&lt;li&gt;1つより増やしても良い効果は得られていない&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;llm側のファインチューニング&#34;&gt;LLM側のファインチューニング
&lt;/h3&gt;&lt;p&gt;LoRAによるLLM部分のFTによって性能を改善できるのかを実験.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig7.png&#34;
	width=&#34;1347&#34;
	height=&#34;400&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig7_hu2183467831082103688.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig7_hu14804726074471620347.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;LLMファインチューニング&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;336&#34;
		data-flex-basis=&#34;808px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;LLMのFT後は精度が向上（3Dの問題はあまり精度があがらないが、LoRAのパラメーター追加が少なすぎた？）.&lt;/li&gt;
&lt;li&gt;ViT部分やprojector層のFTはあまり効果がない.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;また、FT前後でのAttentin Mapの差を可視化したのが以下の図.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig8.png&#34;
	width=&#34;716&#34;
	height=&#34;609&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig8_hu2244249150732014588.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/fig8_hu13900222547004782885.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Attention Mapの差&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;117&#34;
		data-flex-basis=&#34;282px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;FT後は参照すべき箇所のAttentionが大きくなる.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;出力の分布&#34;&gt;出力の分布
&lt;/h3&gt;&lt;p&gt;以下で定義されるTotal Variation距離を正解ラベルの分布とモデルの出力分布から計算.&lt;/p&gt;
&lt;p&gt;$$
{\rm TV} := \frac{1}{2} \sum_{i=1}^n |P(x_i) - Q(x_i)|.
$$&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;各ラベルの頻度の差を取っており、2つの分布が近いほど0に近づく.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/tab2.png&#34;
	width=&#34;1342&#34;
	height=&#34;399&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/tab2_hu15232865863818876518.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/hidden-in-plain-sight-vlms-overlook-their-visual-representations%E3%81%AE%E8%AB%96%E6%96%87%E7%B4%B9%E4%BB%8B/tab2_hu8513353824623184623.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;出力の分布&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;336&#34;
		data-flex-basis=&#34;807px&#34;
	
&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;全体の傾向として、Original（元のVLM）はTV距離が大きいが、LLMのFTによってTV距離が0に近づいており、モデルの出力分布が正解ラベルの分布に近づくことができている.&lt;/li&gt;
&lt;li&gt;ViTではほとんど改善されないが、projectorのFTでは改善されている.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想
&lt;/h2&gt;&lt;p&gt;本論文ではVLLMが明確に苦手とするタスクがあり、なかなか改善が難しいという結果でした.
近年のAI活用ブームを考えると、従来のモデルをVLMで気軽に置き換えるというようにはいかないことが示されたのは有意義な内容かと思います.&lt;/p&gt;
</description>
        </item>
        <item>
        <title>拡散言語モデルのLLaDA</title>
        <link>https://opqrstuvcut.github.io/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/</link>
        <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
        
        <guid>https://opqrstuvcut.github.io/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/</guid>
        <description>&lt;img src="https://opqrstuvcut.github.io/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/thm.png" alt="Featured image of post 拡散言語モデルのLLaDA" /&gt;&lt;h1 id=&#34;bertを拡張した生成モデル拡散型llmlladaの概要と可能性&#34;&gt;BERTを拡張した生成モデル？拡散型LLM「LLaDA」の概要と可能性
&lt;/h1&gt;&lt;p&gt;2025年に入り、拡散モデルを用いた大規模言語モデル（LLM）が注目されています.特に「Gemini Diffusion」や「LLaDA（Large Language Diffusion with mAsking）」といった新しいアプローチは、従来の自己回帰型（autoregressive）モデルとは異なる性質を持ち、今後のLLMのあり方を変える可能性すらあります.
提案手法のLLaDAとLLaMAを比較したものが以下で、提案手法は遜色ない性能が出ています.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig1.png&#34;
	width=&#34;568&#34;
	height=&#34;558&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig1_hu17096554497865023231.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig1_hu9554779636632027206.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;性能比較top&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;101&#34;
		data-flex-basis=&#34;244px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;本記事では、拡散モデルベースのLLMであるLLaDAについて、その背景、構造、実験結果などを解説します.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;自己回帰型モデルの限界&#34;&gt;自己回帰型モデルの限界
&lt;/h2&gt;&lt;p&gt;従来のLLM（例：GPT系）は自己回帰型モデルに分類され、トークンを一つずつ順番に生成していきます.しかし、この方式には次のような課題があります：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;逐次処理のため推論効率が悪い&lt;/li&gt;
&lt;li&gt;「Reversal Curse」に弱い（参考：&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/2309.12288%ef%bc%89&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;THE REVERSAL CURSE: LLMS TRAINED ON “A IS B” FAIL TO LEARN “B IS A”&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Reversal Curseは次の例のようにトム・クルーズの親については回答できても、メアリー・リー・ファイファーの子どもは誰かを答えることができないという問題です.学習データにはそういったデータがないため、このようなことが起こるようです.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/reversal_curse.png&#34;
	width=&#34;957&#34;
	height=&#34;327&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/reversal_curse_hu2553009147931841247.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/reversal_curse_hu8828465981890160767.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Reversal Curse&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;292&#34;
		data-flex-basis=&#34;702px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;従来のllmのアプローチ&#34;&gt;従来のLLMのアプローチ
&lt;/h2&gt;&lt;p&gt;LLMでは一般に次の左式か右式の問題を解けるようにモデルのパラメーター$\theta$を学習していきます.&lt;/p&gt;
&lt;p&gt;$$
\max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x) \Leftrightarrow \min_\theta {\rm KL}(p_{data}(x)||p_\theta(x)).
$$&lt;/p&gt;
&lt;p&gt;特に自己回帰モデルの場合は、過去のトークンをもとにして次のトークンを予測する問題を解く形になっています.&lt;/p&gt;
&lt;p&gt;$$
p_\theta(x) = p_\theta(x^1) \prod_{i=2}^L p_\theta(x^i|x^1,\dots,x^{i-1}).
$$&lt;/p&gt;
&lt;h2 id=&#34;lladaのアプローチ拡散モデル型のllm&#34;&gt;LLaDAのアプローチ：拡散モデル型のLLM
&lt;/h2&gt;&lt;p&gt;LLaDAは、自己回帰ではなく&lt;strong&gt;拡散モデル&lt;/strong&gt;のアプローチを採用しています.これはBERTのようなマスク予測タスクに近く、以下のような構成です.&lt;/p&gt;
&lt;h3 id=&#34;事前学習pretraining&#34;&gt;事前学習（Pretraining）
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_1.png&#34;
	width=&#34;344&#34;
	height=&#34;310&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_1_hu8317583821321791884.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_1_hu7632591432292500537.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;事前学習の概要&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;110&#34;
		data-flex-basis=&#34;266px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;事前学習ではBERTのようにマスクされた単語を当てるタスクを解きます. ただし、BERTは15%をマスクするようにしていましたが、提案手法では0~100%のランダムな割合だけマスクするようになっています.&lt;/p&gt;
&lt;p&gt;損失関数は次の通りです：&lt;/p&gt;
&lt;p&gt;$$
\mathcal{L}(\theta) := -\mathbb{E}_{t,x_0,x_t} \left[\frac{1}{t} \sum_{i=1}^L \textbf{1}[x_t^i =M]\log p_\theta(x_0^i|x_t) \right].
$$&lt;/p&gt;
&lt;p&gt;この損失関数を小さくできるモデルはたしかに言語を学んでいると言えそうですし、実際BERTは分類問題や固有表現抽出などで高い性能を出すことができます.&lt;br&gt;
しかし、言語モデルとしてもともと解きたかった問題&lt;/p&gt;
&lt;p&gt;$$
\max_{\theta} \mathbb{E}_{p_{data}(x)} \log p_\theta(x)
$$&lt;/p&gt;
&lt;p&gt;についてもうまくいくモデルなのかは謎です.&lt;br&gt;
これについては、&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/2408.08252&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;既存研究&lt;/a&gt;で以下が成り立つことがわかっています.&lt;/p&gt;
&lt;p&gt;$$
\mathbb{E}_{p_{data}(x_0)}[\log p_\theta(x_0)] \geq -\mathcal{L}(\theta).
$$&lt;/p&gt;
&lt;p&gt;マスクされた問題に関する損失関数にマイナスを付けた式により下から最大化したい式を抑えられることをあらわしているので、損失関数を小さくできるほど言語モデルの意味で良いモデルで良いモデルと言えます.&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning&#34;&gt;Fine Tuning
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_2.png&#34;
	width=&#34;337&#34;
	height=&#34;232&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_2_hu13976377202842427562.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_2_hu18101824364018976670.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;Fine Tuningの概要&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;145&#34;
		data-flex-basis=&#34;348px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;事前学習後はFine Tuningをして命令に従えるように学習をしていきます.&lt;br&gt;
データセットとしてプロンプト（指令）と期待される出力のペアが用意されていて、モデルには出力部分の一部をマスクして入力をし、マスクされた部分を予測するように学習させます.&lt;/p&gt;
&lt;p&gt;論文で用いているデータセットには以下の問題が含まれているようです.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;コード生成&lt;/li&gt;
&lt;li&gt;数学問題の解答&lt;/li&gt;
&lt;li&gt;会話応答&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;この学習により、LLaDAもGPTのような指令に従うような機能を獲得できます.&lt;/p&gt;
&lt;h3 id=&#34;推論inference&#34;&gt;推論（Inference）
&lt;/h3&gt;&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_3.png&#34;
	width=&#34;358&#34;
	height=&#34;315&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_3_hu13358073431359741007.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig2_3_hu3799801697783653053.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;推論の概要&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;113&#34;
		data-flex-basis=&#34;272px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;ここまで学習しても、困るのが推論です（ちなみに昔紹介したBERTで文を生成する方法の&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/abs/1902.04094&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;論文&lt;/a&gt;ではトークンを順番にマスクして生成というのを繰り返すギブスサンプリングの形を取っていました）.&lt;/p&gt;
&lt;p&gt;LLaDAの推論は以下のようにして徐々にキレイな文章を生成していきます.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;出力部分をすべてマスク&lt;/li&gt;
&lt;li&gt;マスクされたトークンを推論&lt;/li&gt;
&lt;li&gt;ランダムなトークンを一部 or 確信度が低いトークンを選択して再度マスク&lt;/li&gt;
&lt;li&gt;所定回数まで2～3を繰り返す&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一部のトークンを固定してそれ以外を再生成といったプロセスを繰り返しますので、ノイズを徐々に取り除いていく拡散モデルのようなことをしているイメージにはなっています.
このように推論することで、文全体を一気に生成するよりも安定した品質が得られているのだと思います.&lt;/p&gt;
&lt;p&gt;なお、推論したい部分全体に対して上記の処理をしていく方法の他、semi-autoregressiveという推論したい部分をいくつかのBlockに分けて、左のBlockから上記の処理をおこなっていく方法も提案されています. Blockを小さくしていくと、より自己回帰モデルに近いものになっていきます.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig4.png&#34;
	width=&#34;994&#34;
	height=&#34;414&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig4_hu5570279578201171015.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig4_hu11527428040134031267.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;semi-autoregressive&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;240&#34;
		data-flex-basis=&#34;576px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;実験&#34;&gt;実験
&lt;/h2&gt;&lt;h3 id=&#34;スケーラビリティ&#34;&gt;スケーラビリティ
&lt;/h3&gt;&lt;p&gt;まず投下計算量（モデルのサイズ、学習トークン数）に対して順当に性能が向上するかを確認した結果が以下になります（ちなみにFLOPsはモデルのパラメーター数を$N$、学習データのトークン数を$D$としたときに$6ND$で計算されています）.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig3.png&#34;
	width=&#34;1087&#34;
	height=&#34;512&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig3_hu18239460491744951665.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig3_hu1191963074837339478.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;スケーラビリティ&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;212&#34;
		data-flex-basis=&#34;509px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;オレンジの提案手法でもスケーラビリティが成り立っており、MMLU（一般知識を問う問題）やGSM8K（算数の文章問題）では自己回帰型モデルよりも優位な結果が得られています.&lt;/p&gt;
&lt;h3 id=&#34;他モデルとの比較&#34;&gt;他モデルとの比較
&lt;/h3&gt;&lt;h4 id=&#34;事前学習モデル&#34;&gt;事前学習モデル
&lt;/h4&gt;&lt;p&gt;以下は事前学習モデルでの各タスクの推論性能をあらわしています.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab1.png&#34;
	width=&#34;1166&#34;
	height=&#34;688&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab1_hu2890582493287940261.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab1_hu13953731144681541690.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;性能比較&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;169&#34;
		data-flex-basis=&#34;406px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;学習データが異なるのですが、LLaDAはLLaMA2と比較するとほとんどのタスクで上回っています.LLaMA3とはほぼ互角かやや劣る感じですね.とはいえ、拡散言語モデルの可能性を十分に感じさせる結果です.&lt;br&gt;
論文では特に言及がなかったと思うのですが、こうやって比較するとQwenは性能高いですね. 全然勝てていません.&lt;/p&gt;
&lt;h4 id=&#34;ファインチューニング済モデル&#34;&gt;ファインチューニング済モデル
&lt;/h4&gt;&lt;p&gt;ファインチューニングした場合の性能比較した結果は以下のとおりです.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab2.png&#34;
	width=&#34;1157&#34;
	height=&#34;555&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab2_hu10804090436290878284.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab2_hu15849663389628836230.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;性能比較FT&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;500px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;ファインチューニングしたもの同士で比較するとLLaMA3には劣っているかなという印象ですが、一部のタスクでは上回ることができています. MMLUでは性能が下がっているのですが、Fine Tuning用のデータセットと合っていない可能性が指摘されています.&lt;/p&gt;
&lt;h3 id=&#34;reversal-curseへの耐性&#34;&gt;Reversal Curseへの耐性
&lt;/h3&gt;&lt;p&gt;中国語の詩のデータセットを作り、与えられた詩の前後の句を当てる問題を解かせた結果が以下になります.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab3.png&#34;
	width=&#34;479&#34;
	height=&#34;188&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab3_hu15674018539611421611.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab3_hu14618242785069038937.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;ReversalCurseの結果&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;254&#34;
		data-flex-basis=&#34;611px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;入力された詩の後の句を当てる問題（Forward）はGPTとQwenは高い精度で当てることができていますが、前の句を当てる問題（Reversal）は半分以下しか当たらなくなります. 一方で、提案手法はあまり性能を落とさずに推論できています（とはいえ少し精度が低くはなっていますので解決とは言いづらいですね）.&lt;/p&gt;
&lt;p&gt;論文にはあまり考察がなかったと記憶しているのですが、提案手法はBidirectionalな処理をおこなうため、前後問わずに詩の周辺にはあるトークンの並びが出やすいという傾向を得ることができているのかもしれません.&lt;/p&gt;
&lt;h3 id=&#34;サンプリング戦略とその影響&#34;&gt;サンプリング戦略とその影響
&lt;/h3&gt;&lt;p&gt;LLaDAでは以下のようなトークン再マスク戦略が検討されました：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ランダム&lt;/li&gt;
&lt;li&gt;確信度ベース&lt;/li&gt;
&lt;li&gt;Block単位の逐次生成（semi-autoregressive remasking）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;それらを比較した結果が以下のとおりです.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab5.png&#34;
	width=&#34;1161&#34;
	height=&#34;231&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab5_hu4181500823610003072.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab5_hu11447350925398506949.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;サンプリング戦略&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;502&#34;
		data-flex-basis=&#34;1206px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;Fine Tuningまで考えると、低い確信度を再度マスク &amp;amp; semi-autoregressiveが良いという結果になっています.&lt;br&gt;
semi-autoregressiveを使わずに低い確信度を再度マスクする方法を採用した場合に、ファインチューニング済モデルでは非常に低い性能になっています. ファインチューニングのときに|EOS|トークンでトークン列にパディングをしており、そこの確信度が低いため|EOS|以外の部分のトークンが再マスクされず文の改善がおこなわれないのが理由のようです.
これに対して、semi-autoregressiveを用いて左のBlockから順に生成していくことでパディング部分の悪さを回避できています.&lt;/p&gt;
&lt;h3 id=&#34;推論のサンプリングのステップ数&#34;&gt;推論のサンプリングのステップ数
&lt;/h3&gt;&lt;p&gt;提案手法はサンプリング回数を増やせばより性能が良くなりそうですが、実際どうなのでしょうか？それをあらわしたのが次の図です.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig5.png&#34;
	width=&#34;1042&#34;
	height=&#34;388&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig5_hu6699693169680570119.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/fig5_hu9964007407806442367.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;サンプリングステップ数&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;268&#34;
		data-flex-basis=&#34;644px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;横軸が対数スケールなのですが、ステップ数（NFEs）を増やすごとに性能が上がっていくのが分かります. とはいえ1024回もモデルの推論を実行するのは計算コストが大きすぎますね.&lt;/p&gt;
&lt;h3 id=&#34;推論の過程&#34;&gt;推論の過程
&lt;/h3&gt;&lt;p&gt;推論の過程をあらわしたのが次の図で、色が薄いほど推論のステップのなかで先にトークンが固定され、濃いほどあとの方にトークンが固定されたことをあらわします.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab4_1.png&#34;
	width=&#34;1071&#34;
	height=&#34;271&#34;
	srcset=&#34;https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab4_1_hu9053918660955307375.png 480w, https://opqrstuvcut.github.io/mblog/mblog/posts/%E6%8B%A1%E6%95%A3%E8%A8%80%E8%AA%9E%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AEllada/tab4_1_hu18108364345437381024.png 1024w&#34;
	loading=&#34;lazy&#34;
	
		alt=&#34;推論過程&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;395&#34;
		data-flex-basis=&#34;948px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;少しおもしろいのが答えの72よりも後に埋まっている部分があることです（同じように計算式の答えよりも後に式が埋まっていたりしますね）.&lt;br&gt;
人間の思考とは異なる結果にはなっているのですが、再度サンプリングしても同じトークンが得られている（=実はステップによってあまり改善が必要でない例）ということもあるかと思うので、これだけだとちょっと考察しづらいですね.&lt;br&gt;
ただ、ぼやっとした考えみたいなものを明確にしていっていると捉えると人間に近いとも言えるのかもしれません.&lt;/p&gt;
&lt;h2 id=&#34;感想&#34;&gt;感想
&lt;/h2&gt;&lt;p&gt;LLaDAのアイデア自体はBERTに近く、マスク予測の延長線上にあります.&lt;br&gt;
BERTでも文章が作れるのでは？と思っていましたが、世の中的にはなかなか芽が出ていなかったところにこういった手法が出たことは嬉しく思います.&lt;br&gt;
最近になってようやくこの種の手法がうまくいったのは計算量やデータ量の違いなんでしょうか. 自己回帰モデル型で培われたノウハウのおかげなのかもしれませんね.&lt;/p&gt;
&lt;p&gt;Gemini Diffusionは非常に高速で話題になりましたが、今回の提案手法だとサンプリングが多く速度は速くないはずなので、Gemini Diffusionがどうやっているのか気になるところです（実際、論文では推論速度については言及が特にないはずです）.素のモデルの性能が非常に高くてサンプリングステップが少ないのかもしれませんね.&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
