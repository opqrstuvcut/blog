<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>寄与 on MatLoverによるMatlab以外のブログ</title>
        <link>http://localhost:50000/blog/tags/%E5%AF%84%E4%B8%8E/</link>
        <description>Recent content in 寄与 on MatLoverによるMatlab以外のブログ</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>ja-jp</language>
        <lastBuildDate>Wed, 12 Feb 2020 00:23:01 +0900</lastBuildDate><atom:link href="http://localhost:50000/blog/tags/%E5%AF%84%E4%B8%8E/index.xml" rel="self" type="application/rss+xml" /><item>
        <title>モデルの予測結果を説明するLIMEの理論</title>
        <link>http://localhost:50000/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/</link>
        <pubDate>Wed, 12 Feb 2020 00:23:01 +0900</pubDate>
        
        <guid>http://localhost:50000/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/</guid>
        <description>&lt;img src="http://localhost:50000/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/1341b73a17da593ffc43cebc86969604.jpg" alt="Featured image of post モデルの予測結果を説明するLIMEの理論" /&gt;&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;モデルの予測結果を説明する方法として&lt;strong&gt;LIME&lt;/strong&gt;があります。
LIMEはディープラーニングに限らず、任意のモデルに対して予測結果を適用することができます。
また手法としては結構有名かと思います。&lt;/p&gt;
&lt;p&gt;今回はそんなLIMEの理論について説明します。&lt;/p&gt;
&lt;p&gt;論文：&lt;a class=&#34;link&#34; href=&#34;https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;“Why Should I Trust You?” Explaining the Predictions of Any Classifie&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;limeの戦略&#34;&gt;LIMEの戦略
&lt;/h1&gt;&lt;p&gt;任意のモデル$f$に入力$x \in \mathbb{R}^d$が与えられたときの予測結果$f(x)$への特徴量の寄与を求めることを考えます。&lt;/p&gt;
&lt;p&gt;LIMEでは$x$近傍（近傍については後述）に対しては$f$と同じような予測をすることができる、かつ解釈が容易なモデル$g$を求めます。
例えば$g$が線形モデルの場合には、$g$の各係数を見ることで特徴量の寄与を得ることが可能です。あるいは$g$が決定木であれば、人間でもある程度容易にモデルの解釈が可能です。ですから、このようなモデル$g$を$f$の代わりに使って、予測結果の解釈をしようというモチベーションです。
ただし、LIMEでは$g$には特徴量の値が$0$か$1$となるベクトル$x&amp;rsquo;$が入力として与えられるものとします。これは何らかのルールで$x$の要素と$x&amp;rsquo;$の要素が対応づいているとします。ここも詳細をあとで述べます。
以上のように、解釈が難しいモデル$f$を解釈が容易なモデル$g$に落とし込むことがLIMEのやりたいことになります。&lt;/p&gt;
&lt;p&gt;実際にどうやって$g$を求めるのかといえば、次式のようになります。
$${\rm argmin_{g \in G}} \ L(f, g, \pi_x) + \Omega(g).$$&lt;/p&gt;
&lt;p&gt;ここで、&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$L$は損失関数です。$x$近傍で$g$の予測値が$f$の予測値に近いと、小さくなるように$L$を定義します。&lt;/li&gt;
&lt;li&gt;$\pi_x$は損失関数で使われる重みで、$x$の近傍点が$x$から遠いほど小さい値を取るようにします。詳細は後述する線形モデルの項を参照。&lt;/li&gt;
&lt;li&gt;$\Omega$はモデルの複雑さとなります。決定木を使う場合には木の深さであったり、線形モデルの場合には非ゼロの重みの数になります。モデルを解釈するためには、モデルはシンプルな方が良いため、$\Omega$を加えることで$g$をなるべく人間にやさしいモデルにしてあげます。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;まだ色々と詳細を述べていないため、わからないところは多々あると思いますが、上式は&lt;strong&gt;なるべくシンプルなモデルで$x$の近傍で$f$と近似する$g$を見つける&lt;/strong&gt;といったことを意味します。
この局所的に近似された$g$が得られれば、$x$近傍での特徴量が$g$へ与える寄与がわかる、つまり$f$へ与える寄与が近似的にはわかります。&lt;/p&gt;
&lt;p&gt;次に画像の場合のケースについて、詳細に踏み込みます。&lt;/p&gt;
&lt;h1 id=&#34;画像に対する線形モデルでのlime&#34;&gt;画像に対する線形モデルでのLIME
&lt;/h1&gt;&lt;h2 id=&#34;superpixel&#34;&gt;superpixel
&lt;/h2&gt;&lt;p&gt;画像にLIMEを適用する場合、まず次のように入力画像をsuperpixelに分割し、領域ごとに寄与を求めていきます。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/1341b73a17da593ffc43cebc86969604.jpg&#34;
	width=&#34;1400&#34;
	height=&#34;672&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/1341b73a17da593ffc43cebc86969604_hu8446067134062836465.jpg 480w, http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/1341b73a17da593ffc43cebc86969604_hu12011469338926740137.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;208&#34;
		data-flex-basis=&#34;500px&#34;
	
&gt;
引用元：https://towardsdatascience.com/understanding-how-lime-explains-predictions-d404e5d1829c&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;実際には上記のようにある程度細かく領域を分けますが、以下では例として扱いやすいように次のような画像を考えて、粗く領域を分けていきます（左がオリジナルのくまモンで、右がsuperpixelに分割されたくまモンです）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/ad1fa258f844d5a4ad33e45d75dcf7da.png&#34;
	width=&#34;171&#34;
	height=&#34;294&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/ad1fa258f844d5a4ad33e45d75dcf7da_hu9434594275459747719.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/ad1fa258f844d5a4ad33e45d75dcf7da_hu5114722451893555332.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;58&#34;
		data-flex-basis=&#34;139px&#34;
	
&gt;&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/cc5a03cf0c2b50e3217a99edddcc002b.jpg&#34;
	width=&#34;171&#34;
	height=&#34;294&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/cc5a03cf0c2b50e3217a99edddcc002b_hu9645411743597279984.jpg 480w, http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/cc5a03cf0c2b50e3217a99edddcc002b_hu6040191805966188847.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;58&#34;
		data-flex-basis=&#34;139px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;各領域を$g$に与える入力$x&amp;rsquo;$の各要素に対応させます。例えば1番の領域が$x&amp;rsquo;$の1番目の要素、2番が2番目の要素のようにします。その上で、$x&amp;rsquo;$の各要素が1のときには対応する領域のピクセルが$x$と同じピクセル値、0のときにはその領域がグレーで埋められた画像と対応していると考えます。
具体的には
$$x&amp;rsquo; = [0, 0, 1, 1, 0,0,0,0]$$
としたとき、3番目と4番目だけが1ですので、この$x&amp;rsquo;$に対応した画像は次のようになります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/158a3d802d662d2e95988553c4d83e5d.jpg&#34;
	width=&#34;171&#34;
	height=&#34;294&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/158a3d802d662d2e95988553c4d83e5d_hu15376020573375360564.jpg 480w, http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/158a3d802d662d2e95988553c4d83e5d_hu3072560797674580192.jpg 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;58&#34;
		data-flex-basis=&#34;139px&#34;
	
&gt;&lt;/p&gt;
&lt;h2 id=&#34;近傍のサンプリング&#34;&gt;近傍のサンプリング
&lt;/h2&gt;&lt;p&gt;LIMEでは $x$の近傍のサンプリングをおこないます。
画像の場合に近傍とはどうなるんでしょうか？直感的には謎じゃないでしょうか。&lt;/p&gt;
&lt;p&gt;LIMEの場合には分割された領域のうち、適当な個数（個数もランダムに決めますが、個数の下限は決めておきます）をそのままにし、それ以外をグレーに置き換える処理をします。
$x&amp;rsquo;$の話でいえば、適当な個数の要素については1とし、それ以外は0とする処理に等しいです。&lt;/p&gt;
&lt;p&gt;このようにして得られた画像を$x$の近傍として扱います。またこのようにして近傍を得ることを、近傍のサンプリングとします。
先程示した$x&amp;rsquo;$に対応した画像も$x$の近傍になります。&lt;/p&gt;
&lt;h2 id=&#34;線形モデルのケース&#34;&gt;線形モデルのケース
&lt;/h2&gt;&lt;p&gt;$g$が線形モデルの場合には$g(z&amp;rsquo;)$は次のようになります。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/e25d1447ff223231a0d857396c39ef6f.png&#34;
	width=&#34;1335&#34;
	height=&#34;377&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/e25d1447ff223231a0d857396c39ef6f_hu345399191377022762.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/e25d1447ff223231a0d857396c39ef6f_hu5359498291438263020.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;354&#34;
		data-flex-basis=&#34;849px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;線形モデルの係数（寄与）を求めるため、次のように損失関数$L$を定義します。
$$  L(f, g, \pi_x) = \sum_{z,z&amp;rsquo;∈Z}\pi_x(z) (f(z) − g(z&amp;rsquo;))^2.$$
ここで$\pi_x$は以下のとおりです。
$$ \pi_x = \exp(−D(x, z)^2/\sigma^2).$$
$z$は$x$近傍の画像をあらわし、$z&amp;rsquo;$は先程まで説明していた（$z$に対応する）$x&amp;rsquo;$と同じものです。$Z$はサンプリングされた$z$と$z&amp;rsquo;$のペアになります。&lt;/p&gt;
&lt;p&gt;上式の意味合いとしては、近傍画像$z$の学習済みモデルでの予測値$f(z)$と解釈が容易なモデル$g(z&amp;rsquo;)$が近い値になるように$g$を学習していきます。&lt;/p&gt;
&lt;p&gt;また、$\pi_x$の存在のため、$z$が$x$に近ければ（=近傍が入力画像に近い）二乗誤差$(f(z) − g(z&amp;rsquo;))^2$が$L$に与える影響は大きいですが、一方で$z$が$x$と大きく異なれば（=近傍が入力画像と大きく異なる）、$(f(z) − g(z&amp;rsquo;))^2$が$L$に与える影響が小さくなります。
より$x$に近い$z$に関しては$g(z&amp;rsquo;)$が良く$f(x)$に近似されるべきですので、このように重み付けされているのは分かる話かと思います。&lt;/p&gt;
&lt;p&gt;なお論文中ではLasso回帰として${\rm argmin_{g \in G}} \ L(f, g, \pi_x) + \Omega(g)$を解いています。&lt;/p&gt;
&lt;h1 id=&#34;limeの実験結果&#34;&gt;LIMEの実験結果
&lt;/h1&gt;&lt;p&gt;実験結果は他の方のブログなどで散々書かれていますので、そちらを参考ください（力尽きました）。&lt;/p&gt;
</description>
        </item>
        <item>
        <title>ディープラーニングのモデルの特徴量の寄与を求めるDeepLift</title>
        <link>http://localhost:50000/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</link>
        <pubDate>Thu, 19 Dec 2019 02:03:01 +0900</pubDate>
        
        <guid>http://localhost:50000/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/</guid>
        <description>&lt;p&gt;本記事はQrunchからの転載です。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;ディープラーニングのモデルに対する特徴量の寄与を求める方法の1つである、DeepLiftについて今回は説明します。&lt;/p&gt;
&lt;p&gt;参考文献：&lt;a class=&#34;link&#34; href=&#34;https://arxiv.org/pdf/1704.02685.pdf&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Learning Important Features Through Propagating Activation Differences&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;従来法の問題点&#34;&gt;従来法の問題点
&lt;/h1&gt;&lt;p&gt;DeepLiftを提案している論文では、以下の2つが従来手法の問題点として挙げられています。&lt;/p&gt;
&lt;h2 id=&#34;saturation-problem&#34;&gt;saturation problem
&lt;/h2&gt;&lt;p&gt;saturation problemは勾配が0であるような区間では寄与が0になってしまう問題です。
従来手法には勾配を利用する手法が多いですが、そのような手法ではsaturation problemが発生してしまいます。
以下の図をご覧ください。
&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/a78fcdb1dc3c5d2431c1ab31da893c9d.png&#34;
	width=&#34;1689&#34;
	height=&#34;1125&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/a78fcdb1dc3c5d2431c1ab31da893c9d_hu12908979449694638319.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/a78fcdb1dc3c5d2431c1ab31da893c9d_hu5562535822996353406.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;図中の関数は$y = 1 - {\rm ReLU(1 - x)}$で、この関数を1つのネットワークとして考えてみます。
この関数では$x &amp;lt; 1$では勾配が$1$となり、$x&amp;gt;1$では勾配が$0$になります。
入力が$x=0$の場合に比べれば、$x=2$の場合は出力値が1だけ大きくなるため、寄与は$x=0$の場合よりも大きくなって欲しいです。しかしながら、寄与=勾配$\times$入力とする寄与の計算方法の場合、
$x = 0 $では残念ながら寄与が等しく0になってしまいます。
このようにReLUによって勾配が0になってしまうことは、Integrated Gradientsの提案論文のなかでも同様に問題として挙げられています。&lt;/p&gt;
&lt;h2 id=&#34;discontinuous-gradients&#34;&gt;discontinuous gradients
&lt;/h2&gt;&lt;p&gt;2つ目に挙げられている問題がdiscontinuous gradientsです。これも下図をご覧ください。
&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/b29d1ab9a442911e96451ac4cccdbc63.png&#34;
	width=&#34;2002&#34;
	height=&#34;545&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/b29d1ab9a442911e96451ac4cccdbc63_hu13833951256203028933.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/b29d1ab9a442911e96451ac4cccdbc63_hu13394467446217612706.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;367&#34;
		data-flex-basis=&#34;881px&#34;
	
&gt;
左から、ネットワークをあらわしている関数$y={\rm ReLU(x - 10)}$、その勾配、寄与=勾配$\times $入力です。
このような関数に対しては計算される寄与値が$x=10$で不連続となり、$x=10$までは寄与が全く無いのに、$x=10$を超えると突然寄与の値が$10$を超えるようになります。
入力値のちょっとした差で寄与が大きく変わるのは良くないですね。&lt;/p&gt;
&lt;h1 id=&#34;deeplift&#34;&gt;DeepLift
&lt;/h1&gt;&lt;p&gt;前述した2つの問題を解決するDeepLiftのアイディアと適用結果について述べていきます。DeepLift以外にも、&lt;a class=&#34;link&#34; href=&#34;https://qrunch.net/@opqrstuvcut/entries/FKxqQpXc0lhh3LMn&#34;  target=&#34;_blank&#34; rel=&#34;noopener&#34;
    &gt;Integrated Gradients&lt;/a&gt;がこれら2つの問題を解決していますが、求まった寄与が直感的ではない場合があります。このことは適用結果で示します。&lt;/p&gt;
&lt;p&gt;なお、DeepLiftで利用されているアイディアの1つとして、RevealCancel Ruleというものがありますが、書くのが大変になりそうなので省略します。&lt;/p&gt;
&lt;h2 id=&#34;deepliftのアイディア&#34;&gt;DeepLiftのアイディア
&lt;/h2&gt;&lt;p&gt;DeepLiftはIntegrated GradientsやSHAPと同様に、基準となる点を決めておき、そこから入力$x$がどれだけ異なるか、また基準点と$x$のネットワークの出力がどれだけ異なるかをもとにして寄与値を計算していきます。
この基準となる点を$x_1^0, \cdots, x_n^0$としておきます。&lt;/p&gt;
&lt;p&gt;ディープラーニングで使われる計算は線形変換と非線形変換の2つに分けられ、DeepLiftではこれによって次のように寄与の計算方法が変わってきます。&lt;/p&gt;
&lt;h3 id=&#34;linear-rule&#34;&gt;Linear Rule
&lt;/h3&gt;&lt;p&gt;まず線形変換の方からです。線形変換には全結合層、畳み込み層が該当します。&lt;/p&gt;
&lt;p&gt;入力（あるいはある隠れ層の出力）$x_1,\cdots, x_n$から次の層のあるニューロン$y$が、重み$w_i$とバイバス$b$を用いて次のようにあらわされるとします。
$$y =  \sum_{i=1}^N w_i x_i + b$$
基準点$x_1^0, \cdots, x_n^0$でも同様に
$$y^0 =  \sum_{i=1}^N w_i x_i^0 + b$$
となります。&lt;/p&gt;
&lt;p&gt;このとき、基準点$x_1^0, \cdots, x_n^0$に対して、$x_1,\cdots, x_n$における$y$の変化量は
$$ \Delta y =\sum_{i=1}^N w_i  \Delta x_i $$
となります。ここで$\Delta y = y - y^0, \Delta x_i = x_i - x_i^0$です。&lt;/p&gt;
&lt;p&gt;DeepLiftではこの変化量に着目し、各入力$x_i$に対する$y$への寄与度$C_{\Delta x_i \Delta y} $を計算していきます。具体的には次のようになります。
$$ C_{\Delta x_i \Delta y} = w_i \Delta x_i .$$&lt;/p&gt;
&lt;p&gt;つまり、入力$x_i$が基準点に比べてどれだけ$y$の変化に影響を及ぼしたかによって寄与が決まります。&lt;/p&gt;
&lt;h3 id=&#34;rescale-rule&#34;&gt;Rescale Rule
&lt;/h3&gt;&lt;p&gt;次に活性化関数で用いられる非線形変換を扱っていきます。
非線形変換のときも線形変換の場合と同様にして考え、基準点に対するニューロンの出力からどれだけ変化を及ぼしたかによって、寄与を決定します。
ただしReLUやtanhなどは1変数$x$を入力としますから、線形変換の場合とは異なり、
$$C_{\Delta x \Delta y} = \Delta y $$です。&lt;/p&gt;
&lt;h2 id=&#34;saturation-problemとdiscontinuous-gradientsの解決&#34;&gt;saturation problemとdiscontinuous gradientsの解決
&lt;/h2&gt;&lt;p&gt;Linear RuleとRescale Ruleの2つを定義しましたが、このルールに則って寄与を計算することで、前述した2つの問題を解決することができます（どちらもRescale Rule絡みになりますが）。&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;saturation problem&lt;!-- raw HTML omitted --&gt;
以下の図のように、DeepLiftでは勾配が0になる状況でも寄与は0になりません。
&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/7d9d6c0a6fa52841fa18e2cf4cb227a8.png&#34;
	width=&#34;1689&#34;
	height=&#34;1125&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/7d9d6c0a6fa52841fa18e2cf4cb227a8_hu15296595552791101390.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/7d9d6c0a6fa52841fa18e2cf4cb227a8_hu11322379920257949130.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;150&#34;
		data-flex-basis=&#34;360px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;&lt;!-- raw HTML omitted --&gt;discontinuous gradients&lt;!-- raw HTML omitted --&gt;
以下の3列目がDeepLiftでの寄与をあらわしたグラフです。DeepLiftでは寄与が不連続になりません。
&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/f2103e478f29951e3faa9d398d626a56.png&#34;
	width=&#34;1724&#34;
	height=&#34;470&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/f2103e478f29951e3faa9d398d626a56_hu15324739139615279590.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/f2103e478f29951e3faa9d398d626a56_hu5960859445899261866.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;366&#34;
		data-flex-basis=&#34;880px&#34;
	
&gt;&lt;/p&gt;
&lt;p&gt;非常に単純なアイディアですが、問題にあがっていた2つを解決することができました。&lt;/p&gt;
&lt;h2 id=&#34;連鎖律&#34;&gt;連鎖律
&lt;/h2&gt;&lt;p&gt;ここまでで扱ってきた内容は、入力を線形変換したときの寄与、あるいは入力を非線形変換したときの寄与の計算になります。
それでは、入力に線形変換と非線形変換を順番に適用するときには、入力の最終的な出力に対する寄与はどのようにして求めると良いでしょうか。またディープラーニングのように層が複数あるようなケースではどうやって計算すれば良いでしょうか。
DeepLiftでは次のmultiplierとそれに対する連鎖律を導入することで、この計算を可能にしています。&lt;/p&gt;
&lt;p&gt;まず、multiplier $m_{\Delta x \Delta y}$の定義は以下のようになります。
$$ m_{\Delta x \Delta y} = \frac {C_{\Delta x \Delta y}}{\Delta x}.$$
これは$\partial y/ \partial x$と似たような形式になっています。特にRescale ruleのときには$C_{\Delta x \Delta y}=\Delta y$ですから、意味合いは近いものがあります。&lt;/p&gt;
&lt;p&gt;次に連鎖律の定義です。
ネットワークへの入力を$x_1,\cdots,x_n$、隠れ層のニューロンを$y_1,\cdots, y_\ell$、出力層のある1つのニューロンを$z$とします。このとき、multiplierに対して次のように連鎖律を定義します。
$$ m_{\Delta x_i \Delta z} = \sum_{j=1}^\ell m_{\Delta x_i \Delta y_j} m_{\Delta y_j \Delta z}.$$&lt;/p&gt;
&lt;p&gt;これは丁度ディープラーニングでの計算で使われる連鎖律と同じものです。つまり、
$$ \frac{\partial z}{\partial x_i} = \sum_{j=1}^\ell \frac{\partial z}{\partial y_j}  \frac{\partial y_j}{\partial x_i} $$
と同じ形式です。
ただし、multiplierの連鎖律は導かれるものではなく、定義であることに注意が必要です。&lt;/p&gt;
&lt;p&gt;multiplierの連鎖律を使うことで、backpropagationのようにして任意の層に対する任意の層へのmultiplierが求まります。こうして求まったmultiplierに対して基準点からの差をかけ合わせれば寄与が求まります。さきほどの連鎖律の話に出てきた変数の定義をそのまま使うと、
$$ C_{\Delta x_i \Delta z} = m_{\Delta x_i \Delta z}   \Delta x_i $$
が$x_i$が$z$への寄与になります。&lt;/p&gt;
&lt;h2 id=&#34;deepliftの適用結果&#34;&gt;DeepLiftの適用結果
&lt;/h2&gt;&lt;p&gt;MNISTに適用した結果を示します。
&lt;img src=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/64a618e3bf36cb2953ac208966e42b90.png&#34;
	width=&#34;1346&#34;
	height=&#34;1074&#34;
	srcset=&#34;http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/64a618e3bf36cb2953ac208966e42b90_hu2447569265231029651.png 480w, http://localhost:50000/blog/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/64a618e3bf36cb2953ac208966e42b90_hu1380855239694972600.png 1024w&#34;
	loading=&#34;lazy&#34;
	
	
		class=&#34;gallery-image&#34; 
		data-flex-grow=&#34;125&#34;
		data-flex-basis=&#34;300px&#34;
	
&gt;
1つの行が1つの手法をあらわしています（DeepLiftはRevealCancelとありますが、これは今回説明を省いたアイディアです）。1列目がオリジナルの画像で、2列目がCNNによって計算された「8」である確率への寄与でをあらわします。明るい部分が正の寄与で、暗いところが負の寄与になります。ちなみに基準点となる入力は全ピクセル値を0とした真っ黒な画像です。3列目は「3」である確率への寄与です。また4列目はオリジナルの画像から「3」である確率への寄与が高いピクセルを抜き出しているものです。
上2つの手法はピクセル間での寄与の差があまり明確ではありません。また4列目をみてみると、勾配と入力の積を寄与とした方法やIntegrated Gradientsよりも、「3」と判定するために必要なピクセルへはっきりと高い寄与を割り当てることができています。&lt;/p&gt;
&lt;h1 id=&#34;deepliftとintegrated-gradients&#34;&gt;DeepLiftとIntegrated Gradients
&lt;/h1&gt;&lt;p&gt;DeepLiftとIntegrated Gradientsは論文の中でお互いの問題点を指摘しあっています。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;DeepLiftの提案論文の主張：
Integrated Gradientsは直感的でない寄与の割当がおこる。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Integrated Gradientsの提案論文の主張：
DeepLiftはmultiplierの連鎖律の部分が数学的に問題がある。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;SHAPでも上記2つの手法を利用した計算が可能です。どちらが良いのかは悩ましいですが、結果が直感的になりやすいのはDeepLift、数学的に理論がしっかりしているのがIntegrated Gradientsという感じでしょうか（あとは実装しやすいのはIntegrated Gradientsとか計算量が少ないのはDeepLiftなどの観点もありますね）。&lt;/p&gt;
</description>
        </item>
        
    </channel>
</rss>
