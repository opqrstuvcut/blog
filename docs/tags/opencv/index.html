<!DOCTYPE html>
<html lang="ja" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>OpenCV | MatLoverによるMatlab以外のブログ</title>
<meta name="keywords" content="">
<meta name="description" content="">
<meta name="author" content="">
<link rel="canonical" href="https://opqrstuvcut.github.io/blog/tags/opencv/">
<link crossorigin="anonymous" href="/blog/assets/css/stylesheet.fc220c15db4aef0318bbf30adc45d33d4d7c88deff3238b23eb255afdc472ca6.css" integrity="sha256-/CIMFdtK7wMYu/MK3EXTPU18iN7/MjiyPrJVr9xHLKY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://opqrstuvcut.github.io/blog/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://opqrstuvcut.github.io/blog/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://opqrstuvcut.github.io/blog/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://opqrstuvcut.github.io/blog/apple-touch-icon.png">
<link rel="mask-icon" href="https://opqrstuvcut.github.io/blog/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" type="application/rss+xml" href="https://opqrstuvcut.github.io/blog/tags/opencv/index.xml">
<link rel="alternate" hreflang="ja" href="https://opqrstuvcut.github.io/blog/tags/opencv/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  
    
      
    
  

<meta property="og:title" content="OpenCV" />
<meta property="og:description" content="" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://opqrstuvcut.github.io/blog/tags/opencv/" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="OpenCV"/>
<meta name="twitter:description" content=""/>

</head>

<body class="list" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://opqrstuvcut.github.io/blog/" accesskey="h" title="MatLoverによるMatlab以外のブログ (Alt + H)">MatLoverによるMatlab以外のブログ</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://opqrstuvcut.github.io/blog/" title="Home">
                    <span>homeHome</span>
                </a>
            </li>
            <li>
                <a href="https://opqrstuvcut.github.io/blog/archives" title="Archives">
                    <span>archivesArchives</span>
                </a>
            </li>
            <li>
                <a href="https://opqrstuvcut.github.io/blog/search" title="Search (Alt &#43; /)" accesskey=/>
                    <span>searchSearch</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main"> 
<header class="page-header">
  <h1>
    OpenCV
  </h1>
</header>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">いつの間にかOpenCVのVideoCaptureが正しく向きに対応できるようになっていた
    </h2>
  </header>
  <div class="entry-content">
    <p>昔の話 OpenCVのVideoCaptureを使っていると、あれって思うことがありました。
動画によって、読み込まれたフレームの向きが正しかったり、90度回転していたりするんですよね。特にスマートフォンで撮影した動画で問題が起きていました。
もちろん、一般的な動画プレーヤーで再生すると正しく表示されるような動画です。
この原因としては、動画には向きをあらわすメタデータが含まれているのですが、昔のOpenCVのVideoCaptureだとそれを無視していたためです。
これへの対応としてexiftoolあたりを使ってメタデータを読み込んで、自分でフレームの回転を補正する必要がありました。
現在の話 最近OpenCVの新しめのバージョンを使っていたところ、なぜだか動画のフレームが正しく読み込まれるようになっていました。
なんと、実は2020年の秋くらいからOpenCV側でメタデータを読み込んで回転を補正するようになっていました！めちゃくちゃ良いアップデート！
詳しくはこちらのissue(https://github.com/opencv/opencv/issues/15499 )を見ていただければと思います。
versionが4.5からは間違いなくこの機能が使えそうですので、動画を扱う人は新しめのOpenCVを使いましょう。</p>
  </div>
  <footer class="entry-footer"><span title='2022-12-13 00:00:00 +0000 UTC'>12月 13, 2022</span></footer>
  <a class="entry-link" aria-label="post link to いつの間にかOpenCVのVideoCaptureが正しく向きに対応できるようになっていた" href="https://opqrstuvcut.github.io/blog/posts/%E3%81%84%E3%81%A4%E3%81%AE%E9%96%93%E3%81%AB%E3%81%8Bopencv%E3%81%AEvideocapture%E3%81%8C%E6%AD%A3%E3%81%97%E3%81%8F%E5%90%91%E3%81%8D%E3%81%AB%E5%AF%BE%E5%BF%9C%E3%81%A7%E3%81%8D%E3%82%8B%E3%82%88%E3%81%86%E3%81%AB%E3%81%AA%E3%81%A3%E3%81%A6%E3%81%84%E3%81%9F/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">動画データから前景と背景を分離する
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
画像から前景と背景を分けるのは以前に取り上げたのですが、動画でもOpenCVで前景と背景をわけることが可能です。ここでいう前景は動いている物体を指します。
前景と背景を分離する難しさ 動画から前景と背景を分離するアルゴリズムを自分で実装するのは結構大変です。 最も単純なアルゴリズムは背景だけが写っている画像を撮っておいて、運用時には背景画像とリアルタイムに取得された画像との差分を取るというのが考えられます。 ただしこのやり方だと照明環境は一定にしないといけないのですが、問題設定によってはそうできなかったりします。また背景だけの画像を撮るのが難しい場合もあります。
問題の難しさから、リッチな処理をしたくなるのですが、変に処理をすると計算時間が伸びていく可能性もあります。
OpenCVでやってみる OpenCVのBackgroundSubtractorMOG2を使うと、簡単に照明の変化にも適応する手法を利用できます。背景画像を撮る必要もありません。 BackgroundSubtractorMOG2では背景と前景を分離するために混合ガウス分布を利用しています。 混合ガウス分布の学習はいつするの？という話ですが、これはリアルタイムに更新されていきます。リアルタイムで更新するので照明変化などにも対応できるわけですね。
今回のテスト用の動画としてこちらを利用させていただきました。 道路を車がビュンビュン走っています。
次のようにしてBackgroundSubtractorMOG2を利用できます。
import cv2 import numpy as np cap = cv2.VideoCapture(&#34;ex.mp4&#34;) fgbg = cv2.createBackgroundSubtractorMOG2(history=60, detectShadows=False) masks = [] kernel = np.ones((5, 5), np.uint8) while True: ret, frame = cap.read() if not ret: break fgmask = fgbg.apply(frame) fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel) masks.append(fgmask) cap.release() cv2.destroyAllWindows() createBackgroundSubtractorMOG2に渡している引数ですが、history=60とすることで、直近の60フレームだけをモデルに考慮させているようなイメージです（正確にそうなるわけではないはずですが）。 また、detectShadows=Trueの場合には影も検出できるのですが、不要なのでFalseにしています。この機能を切っておいたほうが少し早くなります。
fgbg.apply(frame)の返り値が前景の検出結果（mask画像）になります。 ちなみに、検出された結果にオープニング処理を入れてノイズを減らしています。今回の動画ではオープニング処理を入れないと次のように結構ノイズが拾われてしまいます。
検出されたマスクにオープニング処理も入れた結果が次のとおりです（GIFが動かない場合はクリックしてみてください）。
コード全体 import cv2 import numpy as np cap = cv2....</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-20 11:04:00 +0900 JST'>8月 20, 2020</span></footer>
  <a class="entry-link" aria-label="post link to 動画データから前景と背景を分離する" href="https://opqrstuvcut.github.io/blog/posts/%E5%8B%95%E7%94%BB%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E5%89%8D%E6%99%AF%E3%81%A8%E8%83%8C%E6%99%AF%E3%82%92%E5%88%86%E9%9B%A2%E3%81%99%E3%82%8B/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">抽出した輪郭の描画
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
OpenCVのfindContoursで見つけた輪郭はdrawContoursで簡単に描画できます。 次のようにして使えます。
drawed = cv2.drawContours(img, contours=contours, contourIdx=-1, color=(255, 0, 0), thickness=10, lineType=8, hierarchy=hierarcies, maxLevel=1) 引数の意味はそれぞれ次のとおりです。必須なのはcolorまでです。
引数 意味 contours findContoursで見つかった輪郭 contourIdx 描画する輪郭のインデックスを指定する（-1だと全て描画） color 描画する輪郭の色 thickness 描画する輪郭の太さ lineType 4、8、cv2.LINE_AAのどれかを指定し、後のほうがきれいに描画される hierarchy findContoursで見つかった輪郭の階層構造 maxLevel 描画する最大の階層を指定する maxLevelを1にしたときと、2にしたときの違いを次に示します。
maxLevel=1 maxLevel=2 maxLevelが2のときには外側の輪郭の中まで輪郭が描画されていますね。</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-17 11:03:00 +0900 JST'>8月 17, 2020</span></footer>
  <a class="entry-link" aria-label="post link to 抽出した輪郭の描画" href="https://opqrstuvcut.github.io/blog/posts/%E6%8A%BD%E5%87%BA%E3%81%97%E3%81%9F%E8%BC%AA%E9%83%AD%E3%81%AE%E6%8F%8F%E7%94%BB/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">findContoursで輪郭の検出
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
画像から物体の輪郭を見つけたくなることが多々あります。 そんなときにもOpenCVを利用することができます。
findContoursで輪郭抽出 次の画像から輪郭の抽出をおこなうことを考えます。
最初に次のように二値化しておきます。
_, bi_img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY &#43; cv2.THRESH_OTSU) これに対して次のようにfindContoursを適用します。
contours, hierarcies = cv2.findContours(bi_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) 第二引数は輪郭の取り出し方を指定しており、cv2.RETR_EXTERNALは一番外側の輪郭だけを取り出します。ここに指定できる方法の比較は後でおこないます。 第三引数は輪郭の近似方法をあらわします。例えば、cv2.CHAIN_APPROX_SIMPLEにすると、返ってくる点の数が大きく減ります。cv2.CHAIN_APPROX_TC89_L1にすると返ってくる点の数をうまい具合に減らしてくれますが、他に比べると計算量がかかります。 返り値の1つめが輪郭を格納したリストです。２つめが輪郭の階層構造をあらわしています。 細かく言うと、輪郭の方は、点のリストが1つの輪郭をあらわし、それらのリストが格納されています。 階層構造の方は、輪郭ごとに１つの階層構造をあらわす4つの要素をもつリストが存在します。各要素の0番目は次の輪郭のインデックス、1番目は前の輪郭のインデックス、2番目は子の輪郭のなかで1番目のインデックス、3番目は親の輪郭のインデックスをあらわします。親と子が何かといえば、親はみている輪郭を囲んでいる輪郭のことで、子は中にある輪郭のことです。 見つかった輪郭を次のように描画してみます。
drawed = cv2.drawContours(np.stack([img, img, img], axis=-1), contours, -1, (255, 0, 0), 10) plt.imshow(drawed) plt.show() 描画の結果は以下のとおりです。
輪郭の取り出し方を変えてみる 先程は輪郭の取り出し方にcv2.RETR_EXTERNALを指定しました。これは一番外側の輪郭しか取れません。 次にちゃんと階層構造をもった結果を返すようにしてみます。これにはcv2.RETR_TREEを指定します。
contours, hierarcies = cv2.findContours(bi_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) 他にもcv2.RETR_LISTやcv2.RETR_CCOMPなどがありますが、hierarciesの中の階層構造の情報の持ち方が変わってきます。</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-16 17:56:36 +0900 JST'>8月 16, 2020</span></footer>
  <a class="entry-link" aria-label="post link to findContoursで輪郭の検出" href="https://opqrstuvcut.github.io/blog/posts/findcontours%E3%81%A7%E8%BC%AA%E9%83%AD%E3%81%AE%E6%A4%9C%E5%87%BA/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">テンプレートマッチングで画像から物体をみつける
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
カメラを固定しておいて、何らかの被写体を取り続けるということはよくある問題設定です。 ただし、被写体の位置が毎回少しズレるということも多々あります。 そんなときにテンプレートマッチングを使うことができます。
テンプレートマッチングについて テンプレートマッチングではテンプレート画像と呼ばれるものを事前に用意しておきます。 そして、検出したいものが写っている画像の左上の領域から順にテンプレート画像とどれくらい似ているかを計算していきます。 このようにして、テンプレート画像とよく似た領域を検出するというのがテンプレートマッチングです。
OpenCVでテンプレートマッチング 次の左の画像をテンプレート画像として、右から同じ物体を検出してみます。
テンプレートマッチングは次のようにしておこなえます。
res = cv2.matchTemplate(img, template, cv2.TM_CCORR_NORMED) cv2.TM_CCORR_NORMEDは類似度の計算の方法です。 選択肢は複数あり、手法によって精度と計算時間が変わります。 詳細はこちらをご確認ください。
返り値には各位置での類似度が格納されています。
TM_CCORR_NORMEDの場合には大きな値ほど、似ていますので明るい部分がもっともテンプレートとマッチしたことをあらわします。
この部分の画像を次のように切り抜いてみます。
_, max_val, _, max_loc = cv2.minMaxLoc(res) height, width = template.shape plt.imshow(img[max_loc[1]: max_loc[1] &#43; height, max_loc[0]: max_loc[0] &#43; width]) plt.show() 結果は以下のとおりです。 バッチリできていることがわかります。</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-15 11:09:00 +0900 JST'>8月 15, 2020</span></footer>
  <a class="entry-link" aria-label="post link to テンプレートマッチングで画像から物体をみつける" href="https://opqrstuvcut.github.io/blog/posts/%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%8B%E3%82%89%E7%89%A9%E4%BD%93%E3%82%92%E3%81%BF%E3%81%A4%E3%81%91%E3%82%8B/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">minMaxLocで最大と最小の位置を楽に取得
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
行列の最大値、最小値はNumPyのmaxやmin、またそれらのインデックスはargmaxやargminを使えば取得できるのですが、OpenCVでは一発ですべて取得できます。
min_val, max_val, min_idx, max_idx = cv2.minMaxLoc(np.array([[1, 2, 3], [4, 5, 6]])) print(min_val, max_val, min_idx, max_idx) この出力は以下のとおりですが、それぞれ最小値、最大値、最小値の位置、最大値の位置をあらわします。位置は$(x,y)$をあらわしていますので、行列でいえば、（列、行）の順に格納されています。
(1.0, 6.0, (0, 0), (2, 1)) </p>
  </div>
  <footer class="entry-footer"><span title='2020-08-11 11:04:00 +0900 JST'>8月 11, 2020</span></footer>
  <a class="entry-link" aria-label="post link to minMaxLocで最大と最小の位置を楽に取得" href="https://opqrstuvcut.github.io/blog/posts/minmaxloc%E3%81%A7%E6%9C%80%E5%A4%A7%E3%81%A8%E6%9C%80%E5%B0%8F%E3%81%AE%E4%BD%8D%E7%BD%AE%E3%82%92%E6%A5%BD%E3%81%AB%E5%8F%96%E5%BE%97/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">compHistでヒストグラム比較をいろいろなやり方でおこなう
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
画像処理の領域では画像から特徴量をあらわすヒストグラムを生成することがよくあります。 特徴量としてヒストグラムを生成するということは、比較をすることもよくあるということで、今回はヒストグラムの比較を扱います。
compHistによるヒストグラムの比較の仕方 次のようにしてヒストグラムの比較をおこないます。
cv2.compareHist(hist_1, hist_2, method) hist_1とhist_2はヒストグラムをあらわすNumPy arrayです。 methodは比較方法をあらわし、以下のようなものがあります。
方法 概要 cv2.HISTCMP_CORREL ピアソンの相関係数 cv2.HISTCMP_CHISQR カイ二乗検定 cv2.HISTCMP_KL_DIV KLダイバージェンス cv2.HISTCMP_INTERSECT 交差法 cv2.HISTCMP_BHATTACHARYYA バタチャリア距離 それぞれの違いは式を見ればわかるという話もありますが、ぱっと分かるように数値的な違いを見ていきます。
比較方法の一覧 次のようなヒストグラムを対象にして各比較方法の違いをみてみます。 結果は次のとおりです。
比較方法 2と2 1と2 2と1 2と3 1と3 HISTCMP_CORREL 1.0 0.22 0.22 -0.22 -0.87 HISTCMP_CHISQR 0.0 10.13 9.11 11.78 18.0 HISTCMP_KL_DIV 0.0 245.6 228.07 234.31 447.40 HISTCMP_INTERSECT 18.0 8.0 8.0 4.0 0.0 HISTCMP_BHATTACHARYYA 0.0 0.73 0.73 0.82 1.0 手法によって、完全一致は大きい値になるのか、小さい値になるのか、また最大値と最小値はあるのかといったところも違うので、注意が必要です。
なお、利用したコードは以下のとおりです。
import numpy as np import pandas as pd import matplotlib....</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-10 13:20:47 +0900 JST'>8月 10, 2020</span></footer>
  <a class="entry-link" aria-label="post link to compHistでヒストグラム比較をいろいろなやり方でおこなう" href="https://opqrstuvcut.github.io/blog/posts/comphist%E3%81%A7%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AF%94%E8%BC%83%E3%82%92%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E3%81%AA%E3%82%84%E3%82%8A%E6%96%B9%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">OpenCVのヒストグラムの計算はNumPyより断然速い
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
画像処理や集計、機械学習では何かとヒストグラムを計算するケースがありますね。
これに伴い、ヒストグラムを計算できるライブラリは色々あるかと思いますが、OpenCVでもヒストグラムを計算する機能をもっています。 NumPyでもヒストグラムの計算できるじゃない、と思いますが、実はOpenCVの方がNumPyのヒストグラムよりも断然速いです。今回はその辺りの比較もおこなっていきます。
OpenCVのヒストグラム せっかくOpenCVを使うので、以下の画像の画素値のヒストグラムを計算してみます。
OpenCVでのヒストグラムの計算は以下のようにおこなえます。
hist = cv2.calcHist(images=[img], channels=[0], mask=None, histSize=[256], ranges=(0, 256)) imagesにはヒストグラムの計算のもととなる画像をリストの形式で渡します。 channelsには画像のチャネルのうち、どれを用いてヒストグラムを計算するかを指定します。いまはグレースケールで1チャネルしかないため、0を指定しています。カラー画像のときにはBGRの3チャネルなので、channelに対応する0~2のどれかを指定します。 maskには画像と同じサイズの1チャネルのマスクを与えることで、ヒストグラムを計算する領域を制限できます。 histSizeにはヒストグラムのbinの数を与えます。 rangesにはヒストグラムの下限と上限を指定します。厳密には(0,256)を与えるということは$[0, 256)$のような区間をあらわすことに注意してください。 結果を以下のように描画してみます。
plt.bar(range(len(hist)), hist.ravel()) plt.ylabel(&#34;freq&#34;) plt.xlabel(&#34;val&#34;) plt.show() NumPyとの比較 cv2.calcHistによって得たヒストグラムと全く同じヒストグラムをNumPyを用いて得ることができます。 具体的には次のようにします。
numpy_hist, bin_edges = np.histogram(img.ravel(), bins=256, range=(0, 255)) さて、速度はどれくらい違うかという話になりますが、%%timeitによって測定した結果が以下のとおりです。
方法 timeitの結果 cv2.calcHist 2.95 ms ± 186 µs per loop np.histogram 109 ms ± 7.22 ms per loop 36倍程度OpenCVのほうが速いことがわかります。 全然違うのでびっくりしますね。</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-10 11:03:00 +0900 JST'>8月 10, 2020</span></footer>
  <a class="entry-link" aria-label="post link to OpenCVのヒストグラムの計算はNumPyより断然速い" href="https://opqrstuvcut.github.io/blog/posts/opencv%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E3%81%AE%E8%A8%88%E7%AE%97%E3%81%AFnumpy%E3%82%88%E3%82%8A%E6%96%AD%E7%84%B6%E9%80%9F%E3%81%84/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Grabcutsで背景と猫を分離したい
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
次のような画像があったとします。
ここから猫だけ抽出したいときに、ツールを使えば少し手間はかかりますが、切り取れると思います。
実はOpenCVのGrabcutsを使えば非常に簡単にそれが実現できます。 （ディープラーニング使えばできるよね？はおいておいて）
Grabcutsを使ってみる 矩形を指定 最初に猫を囲うような矩形を指定する方法を試していきます。 OpenCVのGrabcutsは以下のように利用できます。
bgd_model = np.zeros((1, 65), np.float64) fgd_model = np.zeros((1, 65), np.float64) rect = (0, 30, 300, 120) mask = np.zeros(img.shape[:2], np.uint8) cv2.grabCut(img, mask, rect, bgd_model, fgd_model, 10, cv2.GC_INIT_WITH_RECT) 各引数の意味は以下のとおりです。
maskの詳細は一旦おいておきます。 rectは猫を囲う矩形をあらわし、$(x,y,w,h)$の形式のタプルです。 bgd_modelとfgd_modelは内部で利用する変数なのですが、わざわざ外から与える必要があります。 なぜかといえば、grabCut関数を適用したあとに、同じ画像に再度grabCutを適用したいケースがあるのですが、そういったときに同じbgd_modelとfgd_modelを使い回す必要があるためです。 そのため、外から変数を与えられるようになっています。 6つめの引数の10とあるのは、アルゴリズムの反復回数です。 最後のcv2.GC_INIT_WITH_RECTは指定した矩形をもとに前景である猫を抽出してくださいと指定しているflagです。 分割された領域の情報はmaskに格納されます。 maskに格納される値は以下のような意味になります。
0は確実に背景 1は確実に前景 2は多分背景 3は多分前景 以下のようにして抽出された前景を抽出します。
def plot_cut_image(img, mask): cut_img = img * np.where((mask==1) | (mask==3), 1, 0).astype(np.uint8)[:, :, np.newaxis] plt.imshow(cut_img[:, :, ::-1]) plt.show() 上手く猫だけを抽出できていますね。...</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-09 11:00:00 +0900 JST'>8月 9, 2020</span></footer>
  <a class="entry-link" aria-label="post link to Grabcutsで背景と猫を分離したい" href="https://opqrstuvcut.github.io/blog/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/"></a>
</article>

<article class="post-entry tag-entry"> 
  <header class="entry-header">
    <h2 class="entry-hint-parent">Watershedで領域検出
    </h2>
  </header>
  <div class="entry-content">
    <p>本記事はQrunchからの転載です。
Watershedと呼ばれる方法を使うと、指定したマーカーの情報と画像のエッジから画像中の領域の分割をおこなってくれます。 マーカーとしては、この位置は領域1、この位置は領域2それ以外は背景だよといった感じの情報を与えます。
実際にOpenCVでやってみましょう。
OpenCVでWatershed 次の画像にWaterShedを適用してみます。
いま、4つの物体が写っていますので、これを4つの領域と背景に分けることを考えます。 マーカーは以下のように指定します。
marker = np.zeros((504, 378), np.int32) marker[90:130, 100:130] = 1 marker[230:270, 125:180] = 2 marker[120:150, 250:280] = 3 marker[280:310, 290:320] = 4 markerに代入した1~4の値がそれぞれの物体上にくるようにしています。 マーカーの位置と画像を重ねると次のようになります。
OpenCVのWatershedは次のようにして実行できます。
res = cv2.watershed(img, marker) 返り値には領域を分割した結果をあらわす行列が格納されています。 行列のサイズは画像と同じになっていて、各要素の値はその座標がどの領域かを示した値が入っています。 描画してみると以下のようになります。
3つはちゃんと領域が分割できています。 白いボトルは上手くいきませんでした。エッジがあまり取れていないのかもしれないです。</p>
  </div>
  <footer class="entry-footer"><span title='2020-08-08 11:10:00 +0900 JST'>8月 8, 2020</span></footer>
  <a class="entry-link" aria-label="post link to Watershedで領域検出" href="https://opqrstuvcut.github.io/blog/posts/watershed%E3%81%A7%E9%A0%98%E5%9F%9F%E6%A4%9C%E5%87%BA/"></a>
</article>
<footer class="page-footer">
  <nav class="pagination">
    <a class="next" href="https://opqrstuvcut.github.io/blog/tags/opencv/page/2/">次へ&nbsp;&nbsp;»
    </a>
  </nav>
</footer>
    </main>
    
<footer class="footer">
        <span>&copy; 2024 <a href="https://opqrstuvcut.github.io/blog/">MatLoverによるMatlab以外のブログ</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
