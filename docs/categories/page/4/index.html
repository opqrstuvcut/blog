<!DOCTYPE html>
<html lang="ja-jp"
  x-data
  :class="$store.darkMode.class()"
  :data-theme="$store.darkMode.theme()">
  <head><script src="/blog/livereload.js?mindelay=10&amp;v=2&amp;port=50000&amp;path=blog/livereload" data-no-instant defer></script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>


カテゴリー一覧 | MatLoverによるMatlab以外のブログ</title>

    
<link href="/blog/favicon.ico" rel="shortcut icon" type="image/x-icon" />


<link rel="canonical" href="http://localhost:50000/blog/categories/" />



<meta name="author" content="" />
<meta name="description" content="" />



<meta name="generator" content="Hugo 0.133.0">


<meta property="og:url" content="http://localhost:50000/blog/categories/">
  <meta property="og:site_name" content="MatLoverによるMatlab以外のブログ">
  <meta property="og:title" content="Categories">
  <meta property="og:locale" content="ja_jp">
  <meta property="og:type" content="website">




  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Categories">


<link rel="stylesheet" href="/blog/css/output.css" />




     

    

    <script defer src="https://cdn.jsdelivr.net/npm/alpinejs@3.x.x/dist/cdn.min.js"></script>
  </head>

  <body x-data="{
    flip: false,
  }">
    
    <div id="dream-global-bg"></div>

    
<nav class="mt-4 lg:mt-8 py-4">

  
  <div class="container flex justify-between px-4">
  
    <section class="flex items-center gap-4">
      <div class="avatar cursor-pointer hover:online" @click="flip = !flip" title="Flip it!">
        <div class="h-10 rounded-full">
          <img src="/blog/" alt="" />
        </div>
      </div>

      
    </section>

    <div class="dropdown dropdown-end sm:hidden">
      <div tabindex="0" role="button" class="btn btn-ghost btn-square" aria-label="Select an option">
        <ion-icon name="menu" class="text-2xl"></ion-icon>
      </div>
      <ul class="dropdown-content menu w-36 bg-base-100 rounded-box z-[1] shadow-md">
        


























<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/blog/categories" title="カテゴリー一覧">
    <ion-icon name="grid"></ion-icon>
    カテゴリー一覧
  </a>
</li>




<li>
  <a class="inline-flex items-center p-2 cursor-pointer" href="/blog/tags" title="タグ一覧">
    <ion-icon name="pricetags"></ion-icon>
    タグ一覧
  </a>
</li>






      </ul>
    </div>
    <section class="hidden sm:flex sm:items-center sm:gap-2 md:gap-4">
      

      
      

      

      
      





      
      





      
      





      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/blog/categories" title="カテゴリー一覧">
  <ion-icon class="group-hover:text-primary-content" name="grid"></ion-icon>
</a>


      
      
<a class="group inline-flex items-center p-2 rounded-full cursor-pointer hover:bg-primary" href="/blog/tags" title="タグ一覧">
  <ion-icon class="group-hover:text-primary-content" name="pricetags"></ion-icon>
</a>


      

      

      
    </section>
  </div>
</nav>


    <div class="flip-container" :class="{ 'flip-it': flip }">
      <div class="flipper">
        <div class="front">
          <div class="container">
            


<div class="flex flex-col md:flex-row mt-4">

  <div class="md:basis-[200px] lg:basis-[300px] p-4 prose dark:prose-invert">
    <h1 class="text-2xl">カテゴリー一覧</h1>
    <p class="text-sm">27カテゴリー</p>
    <div class="flex flex-wrap gap-4 mt-6">
      
      <a href="/blog/categories/app-runner/" class="badge h-6 no-underline hover:badge-primary">
        App Runner
      </a>
      
      <a href="/blog/categories/aws/" class="badge h-6 no-underline hover:badge-primary">
        AWS
      </a>
      
      <a href="/blog/categories/chrome/" class="badge h-6 no-underline hover:badge-primary">
        Chrome
      </a>
      
      <a href="/blog/categories/deep-learning/" class="badge h-6 no-underline hover:badge-primary">
        Deep Learning
      </a>
      
      <a href="/blog/categories/docker/" class="badge h-6 no-underline hover:badge-primary">
        Docker
      </a>
      
      <a href="/blog/categories/ecs/" class="badge h-6 no-underline hover:badge-primary">
        ECS
      </a>
      
      <a href="/blog/categories/feature-importance/" class="badge h-6 no-underline hover:badge-primary">
        Feature Importance
      </a>
      
      <a href="/blog/categories/flutter/" class="badge h-6 no-underline hover:badge-primary">
        Flutter
      </a>
      
      <a href="/blog/categories/gcp/" class="badge h-6 no-underline hover:badge-primary">
        GCP
      </a>
      
      <a href="/blog/categories/gpu/" class="badge h-6 no-underline hover:badge-primary">
        GPU
      </a>
      
      <a href="/blog/categories/kubernetes/" class="badge h-6 no-underline hover:badge-primary">
        Kubernetes
      </a>
      
      <a href="/blog/categories/opencv/" class="badge h-6 no-underline hover:badge-primary">
        OpenCV
      </a>
      
      <a href="/blog/categories/opencv%E7%94%9F%E6%B4%BB/" class="badge h-6 no-underline hover:badge-primary">
        OpenCV生活
      </a>
      
      <a href="/blog/categories/pandas/" class="badge h-6 no-underline hover:badge-primary">
        Pandas
      </a>
      
      <a href="/blog/categories/python/" class="badge h-6 no-underline hover:badge-primary">
        Python
      </a>
      
      <a href="/blog/categories/sql/" class="badge h-6 no-underline hover:badge-primary">
        SQL
      </a>
      
      <a href="/blog/categories/tensorflow/" class="badge h-6 no-underline hover:badge-primary">
        TensorFlow
      </a>
      
      <a href="/blog/categories/uwsgi/" class="badge h-6 no-underline hover:badge-primary">
        UWSGI
      </a>
      
      <a href="/blog/categories/vim/" class="badge h-6 no-underline hover:badge-primary">
        Vim
      </a>
      
      <a href="/blog/categories/vim%E3%83%97%E3%83%A9%E3%82%B0%E3%82%A4%E3%83%B3/" class="badge h-6 no-underline hover:badge-primary">
        Vimプラグイン
      </a>
      
      <a href="/blog/categories/%E7%94%BB%E5%83%8F%E5%87%A6%E7%90%86/" class="badge h-6 no-underline hover:badge-primary">
        画像処理
      </a>
      
      <a href="/blog/categories/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92/" class="badge h-6 no-underline hover:badge-primary">
        機械学習
      </a>
      
      <a href="/blog/categories/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86/" class="badge h-6 no-underline hover:badge-primary">
        自然言語処理
      </a>
      
      <a href="/blog/categories/%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92/" class="badge h-6 no-underline hover:badge-primary">
        深層学習
      </a>
      
      <a href="/blog/categories/%E6%95%B0%E5%AD%A6/" class="badge h-6 no-underline hover:badge-primary">
        数学
      </a>
      
      <a href="/blog/categories/%E6%95%B0%E5%80%A4%E8%A8%88%E7%AE%97/" class="badge h-6 no-underline hover:badge-primary">
        数値計算
      </a>
      
      <a href="/blog/categories/%E7%B5%B1%E8%A8%88/" class="badge h-6 no-underline hover:badge-primary">
        統計
      </a>
      
    </div>
  </div>
  
  <div class="divider divider-vertical md:divider-horizontal px-4 md:px-0"></div>
  
  <div class="flex-1">
    

    
    <div class="dream-grid">
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/opencv%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E3%81%AE%E8%A8%88%E7%AE%97%E3%81%AFnumpy%E3%82%88%E3%82%8A%E6%96%AD%E7%84%B6%E9%80%9F%E3%81%84/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default3_hu11811107914568380876.webp" type="image/webp" />
      <img src="/blog/images/default3.jpg" alt="/images/default3.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">OpenCVのヒストグラムの計算はNumPyより断然速い</h2>

    <p class="date text-base-content/60">
    
      月曜日, 8月 10, 2020
    
    </p>

    本記事はQrunchからの転載です。
画像処理や集計、機械学習では何かとヒストグラムを計算するケースがありますね。
これに伴い、ヒストグラムを計算できるライブラリは色々あるかと思いますが、OpenCVでもヒストグラムを計算する機能をもっています。 NumPyでもヒストグラムの計算できるじゃない、と思いますが、実はOpenCVの方がNumPyのヒストグラムよりも断然速いです。今回はその辺りの比較もおこなっていきます。
OpenCVのヒストグラム せっかくOpenCVを使うので、以下の画像の画素値のヒストグラムを計算してみます。
OpenCVでのヒストグラムの計算は以下のようにおこなえます。
hist = cv2.calcHist(images=[img], channels=[0], mask=None, histSize=[256], ranges=(0, 256)) imagesにはヒストグラムの計算のもととなる画像をリストの形式で渡します。 channelsには画像のチャネルのうち、どれを用いてヒストグラムを計算するかを指定します。いまはグレースケールで1チャネルしかないため、0を指定しています。カラー画像のときにはBGRの3チャネルなので、channelに対応する0~2のどれかを指定します。 maskには画像と同じサイズの1チャネルのマスクを与えることで、ヒストグラムを計算する領域を制限できます。 histSizeにはヒストグラムのbinの数を与えます。 rangesにはヒストグラムの下限と上限を指定します。厳密には(0,256)を与えるということは$[0, 256)$のような区間をあらわすことに注意してください。 結果を以下のように描画してみます。
plt.bar(range(len(hist)), hist.ravel()) plt.ylabel(&#34;freq&#34;) plt.xlabel(&#34;val&#34;) plt.show() NumPyとの比較 cv2.calcHistによって得たヒストグラムと全く同じヒストグラムをNumPyを用いて得ることができます。 具体的には次のようにします。
numpy_hist, bin_edges = np.histogram(img.ravel(), bins=256, range=(0, 255)) さて、速度はどれくらい違うかという話になりますが、%%timeitによって測定した結果が以下のとおりです。
方法 timeitの結果 cv2.calcHist 2.95 ms ± 186 µs per loop np.histogram 109 ms ± 7.22 ms per loop 36倍程度OpenCVのほうが速いことがわかります。 全然違うのでびっくりしますね。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default4_hu11774824972063855820.webp" type="image/webp" />
      <img src="/blog/images/default4.jpg" alt="/images/default4.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">Grabcutsで背景と猫を分離したい</h2>

    <p class="date text-base-content/60">
    
      日曜日, 8月 9, 2020
    
    </p>

    本記事はQrunchからの転載です。
次のような画像があったとします。
ここから猫だけ抽出したいときに、ツールを使えば少し手間はかかりますが、切り取れると思います。
実はOpenCVのGrabcutsを使えば非常に簡単にそれが実現できます。 （ディープラーニング使えばできるよね？はおいておいて）
Grabcutsを使ってみる 矩形を指定 最初に猫を囲うような矩形を指定する方法を試していきます。 OpenCVのGrabcutsは以下のように利用できます。
bgd_model = np.zeros((1, 65), np.float64) fgd_model = np.zeros((1, 65), np.float64) rect = (0, 30, 300, 120) mask = np.zeros(img.shape[:2], np.uint8) cv2.grabCut(img, mask, rect, bgd_model, fgd_model, 10, cv2.GC_INIT_WITH_RECT) 各引数の意味は以下のとおりです。
maskの詳細は一旦おいておきます。 rectは猫を囲う矩形をあらわし、$(x,y,w,h)$の形式のタプルです。 bgd_modelとfgd_modelは内部で利用する変数なのですが、わざわざ外から与える必要があります。 なぜかといえば、grabCut関数を適用したあとに、同じ画像に再度grabCutを適用したいケースがあるのですが、そういったときに同じbgd_modelとfgd_modelを使い回す必要があるためです。 そのため、外から変数を与えられるようになっています。 6つめの引数の10とあるのは、アルゴリズムの反復回数です。 最後のcv2.GC_INIT_WITH_RECTは指定した矩形をもとに前景である猫を抽出してくださいと指定しているflagです。 分割された領域の情報はmaskに格納されます。 maskに格納される値は以下のような意味になります。
0は確実に背景 1は確実に前景 2は多分背景 3は多分前景 以下のようにして抽出された前景を抽出します。
def plot_cut_image(img, mask): cut_img = img * np.where((mask==1) | (mask==3), 1, 0).astype(np.uint8)[:, :, np.newaxis] plt.imshow(cut_img[:, :, ::-1]) plt.show() 上手く猫だけを抽出できていますね。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/watershed%E3%81%A7%E9%A0%98%E5%9F%9F%E6%A4%9C%E5%87%BA/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default1_hu18298318514717008910.webp" type="image/webp" />
      <img src="/blog/images/default1.jpg" alt="/images/default1.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">Watershedで領域検出</h2>

    <p class="date text-base-content/60">
    
      土曜日, 8月 8, 2020
    
    </p>

    本記事はQrunchからの転載です。
Watershedと呼ばれる方法を使うと、指定したマーカーの情報と画像のエッジから画像中の領域の分割をおこなってくれます。 マーカーとしては、この位置は領域1、この位置は領域2それ以外は背景だよといった感じの情報を与えます。
実際にOpenCVでやってみましょう。
OpenCVでWatershed 次の画像にWaterShedを適用してみます。
いま、4つの物体が写っていますので、これを4つの領域と背景に分けることを考えます。 マーカーは以下のように指定します。
marker = np.zeros((504, 378), np.int32) marker[90:130, 100:130] = 1 marker[230:270, 125:180] = 2 marker[120:150, 250:280] = 3 marker[280:310, 290:320] = 4 markerに代入した1~4の値がそれぞれの物体上にくるようにしています。 マーカーの位置と画像を重ねると次のようになります。
OpenCVのWatershedは次のようにして実行できます。
res = cv2.watershed(img, marker) 返り値には領域を分割した結果をあらわす行列が格納されています。 行列のサイズは画像と同じになっていて、各要素の値はその座標がどの領域かを示した値が入っています。 描画してみると以下のようになります。
3つはちゃんと領域が分割できています。 白いボトルは上手くいきませんでした。エッジがあまり取れていないのかもしれないです。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/%E7%94%BB%E5%83%8F%E3%81%AE%E8%B7%9D%E9%9B%A2%E5%A4%89%E6%8F%9B%E3%82%92%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default2_hu4245215999118357849.webp" type="image/webp" />
      <img src="/blog/images/default2.jpg" alt="/images/default2.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">画像の距離変換をおこなう</h2>

    <p class="date text-base-content/60">
    
      金曜日, 8月 7, 2020
    
    </p>

    本記事はQrunchからの転載です。
画像に対する距離変換とは、グレースケールの画像において、ピクセルから最も近い0の値をもつピクセルまでの距離を求めたものです。
早速OpenCVで試してみます。
OpenCVで距離変換 次のようにして距離変換をおこなえます。
dist = cv2.distanceTransform(img, distanceType=cv2.DIST_L2, maskSize=5 ) distanceTypeに距離の計算方法を指定します。DIST_L2はユークリッド距離です。 maskSizeには最も近い0の値をもつピクセルまでの距離の近似値を計算するときに使うmaskの大きさを指定します。maskSize=5の例でいえば、maskをあらわす$5\times5$の行列の各要素にはmaskの中心からの距離が格納されています。このmaskを使うことで、正確に距離を計算するよりも速く距離（の近似値）が計算できます。
結果は以下のとおりです。
入力画像 距離変換適用（明るいほど距離大） 背景が0の値をもつので、そこまでの距離が反映されています。窓の中心や、猫の顔の中心は背景から遠いので、大きな値をもっています。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/floodfill%E3%81%A7%E9%A0%98%E5%9F%9F%E3%81%AB%E8%89%B2%E3%82%92%E5%A1%97%E3%82%8B/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default1_hu18298318514717008910.webp" type="image/webp" />
      <img src="/blog/images/default1.jpg" alt="/images/default1.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">floodFillで領域に色を塗る</h2>

    <p class="date text-base-content/60">
    
      木曜日, 8月 6, 2020
    
    </p>

    本記事はQrunchからの転載です。
OpenCVのfloodFillを使うことで、選んだ点の周辺の似たような色のピクセルを塗りつぶすことができます。
使い方 次のようにしてfloodFillを利用できます。
mask = np.zeros((img.shape[0] + 2, img.shape[1] + 2), dtype=np.uint8) res = cv2.floodFill(img, mask=mask, seedPoint=(400, 700), newVal=(0, 0, 255), loDiff=30, upDiff=30) まずmaskですが、入力画像の$(x,y)$がmaskの$(x+1, y+1)$に対応し、maskの値が0でないところは塗りつぶされません。入力画像に比べて縦横が2ピクセルずつ大きいので、元の画像の周辺に1ピクセルずつpaddingができたようなイメージですね。 seedPointに指定した座標が塗りつぶしの処理の起点になります。 newValに塗りつぶす色を指定します。 seedPointに指定したピクセルの値からloDiffを引いた値とseedPointに指定したピクセルの値にupDiffを加えた値の間に入っているピクセルをseedPointの隣から順に塗りつぶしていきます。
結果は以下のとおりです。
入力画像 floodFillの結果 

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/hough%E5%A4%89%E6%8F%9B%E3%81%A7%E5%86%86%E3%82%92%E6%A4%9C%E5%87%BA/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default3_hu11811107914568380876.webp" type="image/webp" />
      <img src="/blog/images/default3.jpg" alt="/images/default3.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">Hough変換で円を検出</h2>

    <p class="date text-base-content/60">
    
      水曜日, 8月 5, 2020
    
    </p>

    本記事はQrunchからの転載です。
Hough変換は直線を検出する方法として前回紹介したのですが、Hough変換を応用することで、円の検出も行えます。
OpenCVで円の検出 次の画像から円を検出してみます。
円の検出は以下のようにおこないます。
hough_circle = cv2.HoughCircles(img, method=cv2.HOUGH_GRADIENT, dp=1, minDist=5, param1=100, param2=80) HoughLinesと異なり、画像はグレースケールの状態で渡せば、なかでエッジ検出をおこなってくれます。
methodには手法を指定しますが、HOUGH_GRADIENTしかないようです。
dpには分解能を指定しています。1にすると画像の解像度と同じ分解能をもちます。
minDistには円同士の最小の距離を指定します。これより近いと2つの円として認識されません。
param1はCanny法のしきい値の上限、param2は円上にあると判定されたエッジの点の数に対するしきい値です。
結果は以下のとおりです。
大まかには円が検出できていることがわかります。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/hough%E3%83%8F%E3%83%95%E5%A4%89%E6%8F%9B%E3%81%A7%E7%9B%B4%E7%B7%9A%E3%82%92%E8%A6%8B%E3%81%A4%E3%81%91%E3%82%88%E3%81%86/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default1_hu18298318514717008910.webp" type="image/webp" />
      <img src="/blog/images/default1.jpg" alt="/images/default1.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">Hough（ハフ）変換で直線を見つけよう</h2>

    <p class="date text-base-content/60">
    
      火曜日, 8月 4, 2020
    
    </p>

    本記事はQrunchからの転載です。
Hough変換は画像から直線をみつける方法です。
簡単な原理 入力として2値画像を考えます。 Hough変換では候補となる直線を用意し、直線上にいくつ0でないピクセルがあるかを数えます。 このピクセルの個数が指定したしきい値以上であった場合、その候補の直線は正しい直線として扱います。
なお、OpenCVでは直線の候補は以下のように$(\rho, \theta)$による極座標系であらわされています。 $$ \rho = x \cos \theta + y \sin \theta .$$ $\rho$は原点からの直線の距離、$\theta$は直線の角度をあらわします。
$\theta$が0でないとしたとき、上式をちょっと変形することで見慣れた形の方程式になるかと思います。 $$ y = \frac{\rho}{\sin\theta} - x \frac{\cos \theta}{\sin \theta}. $$
わざわざ極座標系であらわす理由はなにかというと、$y=ax+b$ような直線に対してy軸に平行な直線を考えるときに、傾きが$\infty$の直線となり扱いづらくなることを防ぐためです。 極座標系ですと、無理なくy軸に平行な直線を扱うことができます。
OpenCVで試してみる 次の画像に対してHough変換を適用します。
Hough変換にかける前に、Canny法でエッジを抽出しておきます。
canny = cv2.Canny(img, threshold1=50, threshold2=100, apertureSize=3, L2gradient=True) Canny法の結果に対して、次のようにHough変換を適用できます。
hough_lines = cv2.HoughLines(canny, rho=5, theta=0.01, threshold=300) rhoとthetaはそれぞれの軸方向の直線の候補の分解能になります。小さいほどたくさんの直線が見つかるかと思います。thresholdに直線の候補を採用するかを決めるしきい値を指定します。 また、min_thetaとmax_thetaで見つかる直線のthetaの最小値、最大値を決めることもできます。
検出された直線のパラメータ$(\rho, \theta)$は以下のようにして変換して、画像に直線として書き込んでいます。
t = 3000 for params in hough_lines: rho, theta = params[0] a = np.cos(theta) b = np.

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/canny%E6%B3%95%E3%81%A7%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default3_hu11811107914568380876.webp" type="image/webp" />
      <img src="/blog/images/default3.jpg" alt="/images/default3.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">Canny法でエッジ検出</h2>

    <p class="date text-base-content/60">
    
      月曜日, 8月 3, 2020
    
    </p>

    本記事はQrunchからの転載です。
エッジ検出の方法として、Canny法というものがあります。 SobelフィルタやLaplacianフィルタもエッジ検出ができるわけですが、Canny法を使うとより正確に輪郭を検出することが可能です。
Canny法の簡単な原理 勾配の計算 Canny法では画像を平滑化したあとに、Sobelフィルタによって勾配を計算します。 OpenCVでは勾配の大きさは以下の2つのうちのどちらかで計算がなされます。$G_x$と$G_y$はそれぞれ$x$方向、$y$方向の勾配です。
2ノルムの場合 $$ \rm{grad}=\sqrt{G_x^2 + G_y^2}. $$ 1ノルムの場合 $$ \rm{grad}= |G_x| + |G_y|. $$ 2ノルムのほうが正確ですが、計算量では1ノルムのほうが優れています。
極大値を求める 次に、計算された勾配から、勾配の極大値を求めます。こうすることで、余計な箇所がエッジとして検出されるのを防ぎます。
しきい値処理 最後に、しきい値処理でエッジとして扱うかどうかを決めます。 Canny法のしきい値は2つあり、1つはこの値より大きければエッジとすると決めるためのもの、もう1つはこの値よりも小さければエッジではないと決めるためのものです。 じゃあ2つのしきい値の間はどうなるの？という話ですが、隣接しているピクセルがエッジと判定されていれば、エッジと判定するようにし、そうでなければエッジではないと判定します。 単純なしきい値でのエッジの判定よりも、より柔軟ですね。
ただし、しきい値が非常に重要になることが容易に想像できます。
OpenCVでCanny法をためす Canny法は以下のようにして実行できます。
canny = cv2.Canny(img, threshold1=10, threshold2=50, apertureSize=3, L2gradient=True) threshold1がしきい値の小さい方で、threshold2がしきい値の大きい方です。apertureSizeにSobelフィルタのサイズを指定しています。また勾配の大きさに2ノルムを使う場合にはL2gradientをTrueにします。
結果を以下に示します。
元画像 canny（2ノルム） canny（1ノルム） 2ノルムのほうがきれいにエッジが取れている気がします。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E5%B9%B3%E5%9D%A6%E5%8C%96/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default3_hu11811107914568380876.webp" type="image/webp" />
      <img src="/blog/images/default3.jpg" alt="/images/default3.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">ヒストグラム平坦化</h2>

    <p class="date text-base-content/60">
    
      日曜日, 8月 2, 2020
    
    </p>

    本記事はQrunchからの転載です。
今日はヒストグラム平坦化を扱います。
ヒストグラム平坦化はコントラストが偏っているような画像を補正します。 結果として、コントラストがある程度平坦化された結果が得られます。
処理の中身としては、実際には画像のピクセル値の累積分布関数で写像したうえで、最大値と最小値が広がるように調整してあげるというイメージです。
OpenCVでヒストグラム平坦化 次の画像にヒストグラム平坦化を適用してみます。このままだと全くみえません。
この画像の画素値のヒストグラムは以下のとおりです。だいぶ偏ってますね。
ヒストグラム平坦化は次のようにしておこなえます。めちゃくちゃ簡単です。
res = cv2.equalizeHist(img) ちゃんと見えるようになりましたね。
この画像の画素値のヒストグラムは以下のとおりです。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
      <div class="w-full md:w-1/2 lg:w-1/3 p-4 dream-column">
        <a class="card card-compact bg-base-100 hover:bg-base-content/10 shadow-xl cursor-pointer dark:border dark:border-base-content/30" href="/blog/posts/non-local-means-denoising%E3%81%A7%E3%83%8E%E3%82%A4%E3%82%BA%E9%99%A4%E5%8E%BB/">
  
  <figure>
    
    
    <picture>
      <source srcset="/blog/images/default2_hu4245215999118357849.webp" type="image/webp" />
      <img src="/blog/images/default2.jpg" alt="/images/default2.jpg" />
    </picture>
    
  </figure>
  

  <div class="card-body">
    <h2 class="card-title">Non-Local Means Denoisingでノイズ除去</h2>

    <p class="date text-base-content/60">
    
      土曜日, 8月 1, 2020
    
    </p>

    本記事はQrunchからの転載です。
Non-Local Means Denoisingのアイデア 今回はノイズ除去を扱うのですが、特にガウスノイズを考えます。 これは平均が0となるノイズですので、着目しているピクセルにある意味で似ているピクセルを画像中から探してきて、それらの平均を取れば、ノイズの影響が消えたピクセルが得られるはずです。 これがNon-Local Means Denoisingのアイデアになります。
似ているピクセルをどう定義するか Non-Local Means Denoisingでは着目しているピクセルの値自体ではなく、着目しているピクセルの周辺の値同士の差分を取ることで、似ているかどうかを考えます。 この考えから定義されるピクセル$p$と$q$間の距離は以下のようになります。 $$ d^2(B(p, f), B(q,f)) = \frac{1}{3(2f + 1)^2} \sum_{c=1}^3 \sum_{j \in B(0, f)} (I_c(p+j) - I_c(q+j))^2. $$ ここで$B(p,f)$は着目しているピクセル$p$のサイズの周辺のピクセルで、サイズが$(2f + 1) \times (2f + 1)$となっています。$I_c(p+j)$が周辺ピクセルの$c$番目のchannelの値をあらわします。
平均値の取り方 先程定義した距離を使って以下のような重みを計算します。 $$ w(p,q) = e^{-\max(d^2 - 2\sigma^2, 0) / h^2}. $$ $\sigma^2$はノイズの分散になります（OpenCVの関数で実行するときには特にこれを指定しないので、上手く処理されている？）。$h$は与えるパラメーターで、大きいほど$w$の値に差がつきづらくなります。 距離$d^2$が小さいと$w$が1に近い値を取り、$d^2$が大きいほど$w$は小さい値になります。 この$w$を重みとしたピクセル値の重み付き平均を取ることがNon-Local Means Denoisingでの処理になります。
この重み付き平均をとることで、似ているピクセルは強く考慮されますが、似ていないピクセルはほとんど影響を与えないため、似ているピクセルだけでの平均が取れるような計算処理になっています。
なお、すべてのピクセル同士で距離$d^2$を計算すると、当然計算量が大変なことになります。 このため、実際には着目しているピクセルの周辺のどこまでを考慮するかを指定します。
OpenCVでやってみる OpenCVでNon-Local Means Denoisingをやってみます。
次の左の画像にノイズをのせて右の画像を生成しました。
これに対して次のようにして、Non-Local Means Denoisingを適用します。
denoised = cv2.fastNlMeansDenoisingColored(img, h=3, templateWindowSize=7, searchWindowSize=21) hはさきほどの重みで出てきた$h$と同じで、templateWindowSizeは$d^2$の計算で使われる$f$と同じで、searchWindowSizeは着目しているピクセルの周辺をどこまで考慮するかをあらわします。 ちなみに、fastNlMeansDenoisingという関数もありますが、カラー画像に対してはfastNlMeansDenoisingColoredが良いらしいです。

    <div class="card-actions justify-between items-center mt-4">
      <div class="flex items-center">
  
  <span>@</span>
  

  <span itemprop="author" itemscope itemtype="https://schema.org/Person">
  
    <span itemprop="name"></span>
  
  </span>
</div>


      <div class="inline-flex items-center">
        <ion-icon name="time" class="mr-1"></ion-icon>
        <span>1分で読めます</span>
      </div>
    </div>
  </div>
</a>

      </div>
    
    </div>
    

    
    <div class="flex justify-center pt-8">
      <div class="join grid grid-cols-2">
  
  <a href="/blog/categories/page/3/" title=前へ class="join-item btn btn-outline btn-sm">
    前へ
  </a>
  
  
  <a href="/blog/categories/page/5/" title="次へ" class="join-item btn btn-outline btn-sm">
    次へ
  </a>
  
</div>

    </div>
    
  </div>
</div>



            
            <footer class="flex justify-between items-center gap-2 px-4 py-12">
            
              <div>
  
  <p>© 2024 MatLoverによるMatlab以外のブログ</p>
  

  
  <p class="text-sm">
    🌱
    <span class="text-base-content/60">
      Powered by <a class="hover:underline" href="https://gohugo.io/" target="_blank">Hugo</a> with theme
      <a class="hover:underline" href="https://github.com/g1eny0ung/hugo-theme-dream" target="_blank">Dream</a>.</span
    >
  </p>
  
</div>

              <div
  x-data="{ icons: [
    { name: 'moon', status: 'y' },
    { name: 'sunny', status: 'n' },
    { name: 'desktop', status: 'auto' }
  ] }"
  class="flex items-center h-[32px] px-2 gap-2 border border-base-300 rounded-full"
>
  <template x-for="icon in icons">
    <div
      role="button"
      tabindex="0"
      :aria-label="'Select ' + icon.name + ' mode'"
      class="group inline-flex justify-center items-center p-1 rounded-full cursor-pointer hover:bg-primary"
      :class="$store.darkMode.icon() === icon.name && 'bg-primary'"
      @click="$store.darkMode.toggle(icon.status)"
    >
      <ion-icon
        :name="`${icon.name}-outline`"
        class="group-hover:text-primary-content"
        :class="$store.darkMode.icon() === icon.name && 'text-primary-content'"
      >
      </ion-icon>
    </div>
  </template>
</div>

            </footer>
          </div>
        </div>
        <div class="back">
          <div class="container">
            
            <div class="dream-grid">
  

  

  
</div>

            
          </div>
        </div>
      </div>
    </div>

    <script>
  window.lightTheme =  null 
  window.darkTheme =  null 
</script>

<script src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
<script src="/blog/js/grid.js"></script>

<script src="/blog/js/main.js"></script>

    





    

    
    
  
    
      
    
  


    

    <script type="module" src="https://unpkg.com/ionicons@7.4.0/dist/ionicons/ionicons.esm.js"></script>
    <script nomodule src="https://unpkg.com/ionicons@7.4.0/dist/ionicons/ionicons.js"></script>
  </body>
</html>
