[{"content":"問題発生時の状況 AWSのECS上にFastAPI + uvicornの構成でのサーバーをたてました。内容的には普通のREST APIです。\nとりあえずは順調に動作していたのですが、たまにコンテナが再起動しているっぽいけどなんだろうと思って調べていたところ、メモリ使用量が次のようになっていました。\n 時間経過でメモリ使用量が勝手に増えているような振る舞いです。 実装上はこんなことにならないはず…。\n解決方法 調べてみると、同じようなissueが存在していました。\n https://github.com/tiangolo/fastapi/issues/1624 https://github.com/encode/uvicorn/issues/1226  uvicorn側のissueをみると、uvicornのバージョンが0.17.0でこのような問題が起こるようです。\n利用しているバージョンがちょうど0.17.0でしたので、0.17.6へとバージョンしてみると、見事に解決しました！\nバージョンアップ後のメモリ使用量が以下のグラフの右端の平らな部分です。時間経過でメモリ使用量が増えるようなことがなくなっています。\n ","date":"2022-03-20T00:00:00Z","image":"https://opqrstuvcut.github.io/blog/posts/fastapi-uvicorn%E3%81%AE%E6%A7%8B%E6%88%90%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E6%99%82%E9%96%93%E7%B5%8C%E9%81%8E%E3%81%A7%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E9%87%8F%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B%E3%81%A8%E3%81%8D/memory_usage_ng_hu72596f761d14dd628728bdb4e15cee9a_105746_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/fastapi-uvicorn%E3%81%AE%E6%A7%8B%E6%88%90%E3%81%AE%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E6%99%82%E9%96%93%E7%B5%8C%E9%81%8E%E3%81%A7%E3%83%A1%E3%83%A2%E3%83%AA%E4%BD%BF%E7%94%A8%E9%87%8F%E3%81%8C%E5%A2%97%E3%81%88%E3%82%8B%E3%81%A8%E3%81%8D/","title":"FastAPI + uvicornの構成のサーバーで時間経過でメモリ使用量が増えるとき"},{"content":"NVIDIAのGPUのdriver更新手順 色々手順はあると思いますが、1つのやり方のメモです。\n 古いドライバを削除しておく  公式からCUDA Toolkitをダウンロードしてインストールした場合は次で削除できるはず。 $ cd /usr/local/cuda-x/bin $ sudo cuda-uninstaller $ sudo nvidia-uninstall  もしapt-getを使って古いドライバを入れていたら次のコマンドで消え去るはず。nvidia containerが入っている場合はそれも消えるので、嫌な人は注意。 sudo apt-get remove --purge nvidia\\* libnvidia-\\*    CUDA Toolkitのダウンロードとインストール（Installer Typeはrunfileが一番ラク）  CUDA 11.2ならここの手順に従うhttps://developer.nvidia.com/cuda-11.2.1-download-archive 最新版のCUDAはここの手順に従うhttps://developer.nvidia.com/cuda-downloads   nvidia-smiコマンドを実行して動けばOK  ","date":"2021-12-11T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/nvidia%E3%81%AEgpu%E3%81%AEdriver%E3%81%AE%E6%9B%B4%E6%96%B0/","title":"NVIDIAのGPUのdriverの更新"},{"content":"Individual Conditional Expectation(ICE)は任意のモデルのある特徴量に対するデータごとの挙動を確認する手法です。\n例えば、ある特定のデータのある特徴量が大きくなるにつれ、モデルの出力がどういった変化をするかを見ます。\nPDPの記事を先に見ると、理解がはやいかと思います。\nPartial Dependence Plotの解説記事\nIndividual Conditional Expectationの概要 ICEは冒頭に述べたとおりなので、あまり細かい話をする必要がないのですが、Partial Dependence Plot(PDP)との違いを述べておきます。\nPDPはデータの集合の全体に対して、ある特徴量の値を順に変化させていき、そのときのモデルの出力の平均値をみる方法でした。\n一方で、ICEはモデルの出力の平均値を取らず、データごとに変化をみます。そのため、PDPだと一本の曲線がプロットできますが、ICEではデータの数だけ曲線がプロットできます。\nIndividual Conditional Expectationはの実験 kaggleのtitanicの問題でIndividual Conditional Expectationを試してみます。 モデルはLightGBMの勾配ブースティング法を利用しています。\n年齢に対するIndividual Conditional Expectation 年齢を$0,5,10,\\cdots,65$と変化させてみた結果が以下のとおりです。縦軸はタイタニックに乗った乗客の生存確率の予測値です。1つ1つの曲線が1つの乗客に対応します。\nこれを見ると、傾向として年齢が大人になるくらいまでは、年齢とともに生存確率が下がっていきます。これは直感に合った結果です。 変わったところでいくと、生存確率が年齢の変化とともに変わらない人がいます。\n生存確率が0.7以上であり続けた人のデータを軽く確認したところ、性別は全員女性でした。PDPのときもそうでしたが、女性の生存確率が高いモデルになっているのがここからもわかります。\nまた、ICEでは左端の値をすべてのデータで揃えることで見やすくすることがあります。\n各データごとに、0歳のときの予測値でそれぞれの予測値を引いてみた結果が以下のとおりです。 データの変化の比較がしやすくなりましたね。\n実装 実装は次のとおりです。\nimport pandas as pd from typing import List def individual_conditional_expectation(model, x:pd.DataFrame, target:str, candidates:List) -\u0026gt; np.ndarray: replaced_x = x.copy() ice_vals = np.empty((len(x), len(candidates)), dtype=float) for i, replaced_val in enumerate(candidates): replaced_x[target] = replaced_val preds = model.predict(replaced_x) ice_vals[:, i] = preds return ice_vals candidates = range(0, 70, 5) target = \u0026#34;Age\u0026#34; ice = individual_conditional_expectation(model, train_x, target=target, candidates=candidates) まとめ PDPのようにICEも実装が簡単で、わかりやすい結果が得られます。\n","date":"2021-10-19T00:00:00Z","image":"https://opqrstuvcut.github.io/blog/posts/individual-conditional-expectation/ice_age_huc878420bf5c6d32d2382db7211c8390f_92859_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/individual-conditional-expectation/","title":"Individual Conditional Expectation"},{"content":"Partial Dependence Plotは任意のモデルのある特徴量に対するglobalな挙動を確認できる手法です。\n例えば、特徴量が大きくなるにつれ、モデルの出力がどういった変化をするかがわかります。\nPartial Dependence Plotの概要 学習済みのモデル$f$へ入力する特徴量$x$のうち、$i$番目の特徴量の変化に対する$f$の出力の挙動の変化を確認したいとします。\nこのとき、次のようにモデルの出力の期待値を計算します。 $$ E_{X_C}[f(x_i, X_C)] = \\int p(X_C) f(x_i, X_C) dX_C .$$ ここで$X_C$は$x_i$以外の特徴量になっていまして、$x_i$の値だけを固定し、それ以外の特徴量について積分をしています。\nこうすることで、$x_i$の値により、おおよそどれくらいの出力の差が出るのかがわかります。\n注意点として、$x_i$と$X_C$は独立でなければいけません。\n独立でないときには$x_i$は$X_C$の関数としてあらわされるため、期待値の計算において$X_C$の変化にともない、$x_i$が変化することになります。こうなると、$x_i$が固定という前提と一致しなくなります。\nPartial Dependence Plotの実験 kaggleのtitanicの問題でPartial Dependence Plotを試してみます。 モデルはLightGBMの勾配ブースティング法を利用しています。\n期待値の計算は訓練データの特徴量$X_{C_{j}},j=1,2,\\cdots,n$を用いて以下のように近似値を利用しています。 $$ E_{X_C}[f(x_i, X_C)] = \\int p(X_C) f(x_i, X_C) dX_C \\approx \\frac{1}{n} \\sum_{j=1}^n f(x_i,X_{jC}) .$$\n年齢に対するPartial Dependence Plot 年齢を$0,5,10,\\cdots,65$と変化させてみた結果が以下のとおりです。\n年齢が低いほうが、生存しやすかった傾向が読み取れます。また35歳にピークがありますので、なにか理由がありそうです。例えば、年齢があがるほど客室のクラスが良くなりやすいのかもしれません（そうだとすると年齢と客室のクラスは独立ではないのかという話になりますので、実際にはここの考察が必要になりそうです）。\n性別に対するPartial Dependence Plot 性別についても結果を示します。\n女性のほうが生存しやすかったという傾向が見て取れます。\n実装 実装は次のとおりです。\nimport pandas as pd from typing import List def get_partial_dependence_func_val(model, x:pd.DataFrame, target:str, candidates:List) -\u0026gt; List[float]: expecteds = [] replaced_x = x.copy() for replaced_val in candidates: replaced_x[target] = replaced_val preds = model.predict(replaced_x) expecteds.append(preds.mean()) return expecteds candidates = range(0, 70, 5) target = \u0026#34;Age\u0026#34; ppd = get_partial_dependence_func_val(model, train_x, target=target, candidates=candidates) まとめ Partial Dependence Plotは実装が簡単で、わかりやすい結果が得られます。\nただし、特徴量間が独立でないときは仮定が崩れますので、注意が必要です。\n","date":"2021-10-14T00:00:00Z","image":"https://opqrstuvcut.github.io/blog/posts/partial-dependence-plot/ppd_sex_hu8ddb742a3403f96d393fc87767c04275_6382_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/partial-dependence-plot/","title":"Partial Dependence Plot"},{"content":"AWSのSQSを使うときにちょっと困るのが、アプリの不具合等でDead Letter Queueに送られたメッセージをもとのQueueに戻したいケースです。 調べた感じでは、通常のAWS CLIでは簡単にはできなさそうです。\n理想的には、送信元と送信先のQueueを指定さえすればメッセージを全部送れるような仕組みがあるといいなぁ…と思っていたところ、すばらしい実装を見つけました。 それがSQS Message Moverです。\n使い方はとても簡単です。READMEに書いてあることをやれば簡単に動きます。\n  インストール\nmacの方はbrewで入れても良いと思いますが、自分は次のコマンドで入れました（brew installより待たなくて済んだ説はあります）。\n$ curl https://raw.githubusercontent.com/mercury2269/sqsmover/master/install.sh | sudo sh   credentialsへのキーの指定\nこれは~/.aws/credentialsにaccess keyとsecret keyを指定します（知っている方は多いかと思いますが）。\n[default] aws_access_key_id = \u0026lt;YOUR_ACCESS_KEY_ID\u0026gt; aws_secret_access_key = \u0026lt;YOUR_SECRET_ACCESS_KEY\u0026gt;   コマンドを実行\n次のように送りたいメッセージをもつQueueと送り先のQueueの名前を指定するだけです。\n$ sqsmover --source=\u0026lt;送り元のQueue名\u0026gt; --destination=\u0026lt;送り先のQueue名\u0026gt; 次のような標準出力が確認できるかと思います。\n• Source queue URL: https://sqs.ap-northeast-1.amazonaws.com/xxx/queue_from • Destination queue URL: https://sqs.ap-northeast-1.amazonaws.com/xxx/queue_to • Approximate number of messages in the source queue: x • Starting to move messages... |████████████████████████████████████████| 100% • Done. Moved x messages   とてもお手軽なので同じ状況の人にはおすすめです。\n","date":"2021-06-05T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/aws%E3%81%AEdead-letter-queue%E3%81%AE%E3%83%A1%E3%83%83%E3%82%BB%E3%83%BC%E3%82%B8%E3%82%92%E3%82%82%E3%81%A8%E3%81%AEqueue%E3%81%AB%E6%88%BB%E3%81%99/","title":"AWSのDead Letter QueueのメッセージをもとのQueueに戻す"},{"content":"BERTの系列でCharacterレベルでのembedding手法であるCANINEが提案され、これに似たような手法が盛んになるのではという考えのもと論文を読んだメモを書いておきます。 CANINEってなんて読むべきなんでしょう？\n論文はこちら：https://arxiv.org/pdf/2103.06874.pdf\nエンコーダーのアーキテクチャ CANINEのアーキテクチャは以下のようになっています。 以下では各々の詳細について述べます。\n入力の作り方 文字列から数値列への変換 エンコーダーへの入力は文字単位でおこないます。\n各文字はunicodeの番号に変換され、それがエンコーダーの入力になります。Pythonであれば、ord関数を使うだけで良いです。\nunicodeを使うことで、簡単に入力文を数値列に変換できるうえ、各文字にIDを振って辞書を作成するような手間が不要になります。\n文字のembedding 文字はunicodeの番号に変換されたあと、embedding（ベクトル）に変換されます。 BERTなどはsubwordに対応したベクトルを参照すれば良いですが、CANINEの場合に同じことをしようとすると、14万3000個の文字ごとに768次元のベクトルを用意する必要があるために難しいです。 このため、CANINEではword hash embedding trickというものを利用します。\nこれは、ある文字のunicodeの番号を$x_i$としたとき、次のようにベクトルを生成します。\n$$\\bold{e}_i = \\oplus_k^K {\\rm LOOKUP}_k(\\mathcal{H}_k(x_i)\\ \\% \\ B, d')$$\nここで${\\rm LOOKUP}_k(x, d)$はベクトルの一覧の中から、与えられた値$x$に対応した$d$次元のベクトルを返す関数をあらわします（つまり$\\mathbb{R}^{B \\times d'}$のサイズの行列の特定行を返すような関数）。また$\\oplus$\tはベクトルの結合を、$\\mathcal{H}_k$はハッシュ関数を、$B$は与えられた自然数をあらわします。論文中では$K=8, B=16k, d'=768/K(=96)$となっています。\nunicodeの番号のハッシュ値に応じて得られた96次元のベクトルを結合することで、768次元のベクトルを生成しています。この処理によって生成されうるベクトルの種類は$16 \\times 32 \\times \\dots \\times 2048 \\approx 1.1529215 \\times 10^{18}$なので、豊富な表現力をもつこととなります。\nダウンサンプリング BERTでも同じことがいえますが、文字単位で入力を与えると入力の数が多くなるため計算量が多くなってしまいます。Transformerで行われる行列積は入力長の二乗のオーダーの計算量になるため、入力長を小さくすることは計算量削減に大きく寄与します。 そのためCANINEではダウンサンプリングを用いて、後続のネットワークへの入力を少なくする方法を提案しています。\nダウンサンプリングは以下のようにおこなわれます。\n 文字のembeddingに対してblock単位でのTransformerを1度だけ適用する。  これは128字単位で文字を区切り、その中でself-attentionを実行することを指します。blockに区切ることで計算量削減ができます。   strideのサイズが4のConvolutionを実行する。  1つめのTransformerで文字レベルのembeddingから局所的な情報を得ており、そのあとにstrideが4のConvolutionを実行することで、情報を集約して入力長を1/4に減らすことができます。\n論文では最大で2048字を入力できるようにしていますが、strideのサイズが4のConvolutionを利用することで後続の処理には最大で512個のシーケンスが与えられることになります。\nダウンサンプリング後のシーケンスは、BERTなどのようにTransformerを重ねたネットワークへ与えられます。\nアップサンプリング 固有表現抽出やQAなどのタスクを解くために、入力と同じ長さの出力が必要になります（分類問題は[CLS]に対応するトークンを利用すれば良い）。 このため、次のようにしてアップサンプリングをおこない、入力と同じ長さの出力を得ます。\n Transformerの出力のシーケンスをダウンサンプリングのstrideの分だけ複製し、ダウンサンプリング前の入力長と一致するようにする。  出力のシーケンスが$(o_1,o_2,\\dots)$のときに$(o_1, o_1,o_1,o_1, o_2,o_2,o_2,o_2,\\dots)$とすることを指しているはず。   1のシーケンスとダウンサンプリングでのblock単位でのself-attentionでの出力を結合する。  つまり、各ベクトルは高度な文脈情報と局所的な文脈情報をもつことになります。   結合されたシーケンスへConvolutionを適用することで倍になった次元を結合前の次元に戻す（論文ではkernel sizeは4）。 最後にTransformerを一度適用する。  学習 CANINEの事前学習のタスクには文字単位とsubword単位がありますが、性能は問題によって少しだけ変わります。 各タスクの詳細は以下のとおりです。\n文字単位のタスク 入力されるテキストを単語単位で選択し、その単語を文字単位でmaskし、maskされた文字を予測するタスクです。\n予測するときには文字を左から順に予測するなどのような順番が決まっておらず、maskされているなかからランダムな順番で予測をすることになります。\nサブワード単位のタスク こちらのタスクではsubword単位で選択し、そのsubwordを文字単位でmaskし、maskされたsubwordを予測するタスクです。\n予測するときにはmaskしたsubwordに含まれる文字に対応する出力をランダムに選択し、その出力からsubwordを予測します。\n実験 実験はTYDI QAデータセットを用いておこなわれています。\n他手法との比較 mBERTとの比較が次のテーブルになります。 CANINE-Sはサブワード単位のタスクで事前学習したとき、CANINE-Cは文字単位のタスクで事前学習したときをあらわします。一番右の2つの列はタスクの性能で、F1スコアで計算されているため大きいほうが良いです。 結果を見るとmBERTに大きな差をつけていることがわかります。 また、Example/secの列をみると、mBERTは文字単位の入力にすると非常に推論が遅いですが、CANINEはそれに比べると非常に速いです。とはいえ、mBERTのsubword単位の入力の場合に比べると当然遅いです。\nablation study ablation studyです。 個人的に気になったところとしては、ダウンサンプリングのときによりstrideをもっと大きくすると計算量が減るのですが、この結果をみるとstrideを大きくすることで結構性能が下がってしまっています。これは残念。 embeddingの次元も小さくすると性能が結構下がっていますね。\n","date":"2021-04-13T00:00:00Z","image":"https://opqrstuvcut.github.io/blog/posts/canine%E3%81%AE%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%83%A1%E3%83%A2/fig1_hu05317f9eff6b540e6631244be0e9c68b_212946_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/canine%E3%81%AE%E8%AB%96%E6%96%87%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A0%E3%83%A1%E3%83%A2/","title":"CANINEの論文を読んだメモ"},{"content":"画像分類モデルを作っているときに予測精度をあげるのに役に立ったなぁという方法の一覧のメモです。 簡単にできるものから順に紹介しているつもりです。\nConvolutionとFC層との橋渡しにはGlobal Averaging Poolingを使う ネットにある転移学習の例をコピペすると、畳み込み層の出力を1次元にするところでFlattenを使ってたりします。 でも実はGlobal Averaging Poolingを使ったほうが精度が良くなるかもしれません。\n精度を改善するのもそうですが、モデルのパラメーターを大きく減らせることも非常に大きい恩恵だったりもします。\nEfficientNetはNoisy Student版を使う 転移学習に素のEfficientNetを利用している方は多いと思いますが、Noisy Stundent版の重みを用いて転移学習することでさらに性能があがるかもしれません。\nTensorFlowであれば、こちらのレポジトリが利用可能です。 https://github.com/qubvel/efficientnet\nLabel Smoothingを使う 1か0かのハードラベルではなく、ソフトラベルを使って過学習を抑える方法です。 TensorFlowだと、簡単に使えます。\ntf.keras.losses.categorical_crossentropy(y, y_hat, label_smoothing=0.1) Learning Rate Schedulerを使う 学習率のスケジューラーを利用してみると、精度が良くなるかもしれません。\n例えば、lossが下がりきったタイミングで学習率を0.1倍にしてみると、lossが少し落ちたりします。\n性能が変わらないことも多いので、あまり期待しないほうが良いかも。\nRandAugmentを使う RandAugmentは各画像に対して、ランダムにAugmentをいくつか利用する方法です。 RandAugmentのパラメーターはいくつのAugmentを利用するかと各Agumentでのパラメーターを制御するための値の2つのみになります。そのため、Augmentに関するパラメーターのグリッドサーチも一応可能です。\n\u0026ldquo;パラメーターがたったの2つでいいの！？\u0026ldquo;と普通の人は効果を疑うかと思いますが、実際に試してみるとかなりうまくいきました。\n使うときには、imgaugなどのライブラリを利用すると良いかと思います。次のようにしてRandAugmentを利用可能です。\nimport imgaug.augmenters as iaa images = iaa.RandAugment(n=2, m=30)(images=images) 論文によると、AutoAugmentよりも性能が良いという結果が出ています。\nPyTorch用の実装もネットで適当に探せばすぐに見つかります。\nGridMaskを使う GridMaskはAugmentの一つで、Cutoutと同じような種類のAugmentになります。 Cutoutと違い、Grid状のMaskをかける方法になります。\n問題によっては結構性能があがりました。\n実装はググると色々見つかります。\n","date":"2021-03-13T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E8%AA%8D%E8%AD%98%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E6%80%A7%E8%83%BD%E3%82%92%E3%81%82%E3%81%92%E3%82%8B%E3%81%9F%E3%82%81%E3%81%AEtips/","title":"画像認識モデルの性能をあげるためのTips"},{"content":"ライブストリーミングや動画の配信するためにm3u8とtsファイルを利用するケースがあります。 tsファイルは細切れになった小さい動画になっており、m3u8ファイルはそれらの情報をもっているプレイリストになります。\nm3u8ファイルが手元にあるorm3u8のurlを知っている場合には、Pythonのライブラリのm3u8を使えば簡単にtsファイルをdownloadできます。\nm3u8のInstall インストールは簡単で、pipを使うだけです。\npip install m3u8 tsファイルのdownload tsファイルのダウンロードはm3u8とurllibを組み合わせておこなえます。\nimport urllib.request import m3u8 playlist = m3u8.load(m3u8のパス) for i, segment in enumerate(playlist.segments): # tsファイルのパス uri = segment.absolute_uri urllib.request.urlretrieve(uri, f\u0026#34;{i}.ts\u0026#34;) ","date":"2021-01-31T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/m3u8%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%A8ts%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%81%AEdownload/","title":"m3u8ファイルとtsファイルのdownload"},{"content":"Google Chromeのバージョンをあげたあと、ImageDownloaderなどを使っているときに次のようなエラーメッセージがでることがあります。\nMessage: session not created: This version of ChromeDriver only supports Chrome version 86 最後の86のところはChromeのバージョンによって変わります。\nこれはChromeDriverのバージョンとGoogle Chromeのバージョンが異なっていることによって起きるエラーです。 そのため、ChromeDriverのバージョンをあげればよいです。\n例えば、Homebrewを使ってChromeDriverを入れている場合には次を実行します。\n$ brew upgrade chromedriver ","date":"2021-01-31T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/this-version-of-chromedriver-only-supports-chrome-version-xx%E3%81%A8%E3%81%AA%E3%81%A3%E3%81%9F%E3%81%A8%E3%81%8D/","title":"This version of ChromeDriver only supports Chrome version xxとなったとき"},{"content":"問題設定 次のような設定でサンプリングをしたいことはよくあると思います。\n3つのデータがあり、それぞれに重みがつけられているとする。 それぞれ、データ1の重みは10、データ2の重みは20、データ3の重みは30である。 このときに各データの重みと全体の重みの和の比を確率としてサンプリングをしたい。 つまり、データ1は10/60、データ2は20/60、データ3は30/60の確率でサンプリングすることになる。 シンプルな方法 さきほどの問題設定のとき、簡単にサンプリングする方法は正規化された重みの和を順に足していき、一様分布からサンプリングした乱数がその和を超えたときのデータを取得するという方法です。 手順は次のようになります。\n 0~1の乱数を生成する。 i=0、sum=0とし、生成した乱数をsumが超えるまで以下を実行する。  i番目のデータの重みを全体の重みの和で割る。 1.で計算した重みをsumに足す。   i番目のデータをサンプリングされたデータとする。  この方法は簡単に実装でき、理解も容易ですが、データの重みを順に足していくため、$O(n)$の計算量がかかります。\nSum Tree Sum Treeを用いれば、計算量のオーダーを$O(\\log n)$にできます。\n概要 木の構成 Sum Treeでは二分木を作成し、各ノードがもつ重みを用いてサンプリングの処理をおこないます。 各ノードの重みはそのノードにぶら下がっている葉の重みの和になります。\n図をもちいて具体例を示します。\nデータは5つで重みはそれぞれ5、20、30、5、40としたときには以下のような木が作られます。 緑色の丸はノード、オレンジは葉になります。葉の数値はデータの重み、ノードの数値はノードにぶら下がっている葉の重みの和です。\nサンプリング サンプリングはシンプルです。\n 0からrootノードの重みの間の乱数を生成する。これをvとおく。 rootノードから葉にたどり着くまで、以下の処理を繰り返す。  左の子の重みがv以上ならば、左の子のノードに移動する。 左の子の重みがv以上でなければ、右の子のノードに移動する。またv=v-（左の子の重み）とする。   たどり着いた葉をサンプリングされたデータとする。  これだけだとよく分からないと思うので次から例をみていきます。\nサンプリングの例1 1つめのサンプリングの例は次のとおりです。乱数が70のときは以下の赤のような経路を通ります。 処理をおっていくと、はじめにrootノードが100という重みをもっているので、70を生成したときは右の子ノードに移ります。 このとき、左側の子ノードの重みを生成した値から引くことで、右側のノードの重み(40)以下の値になることが保証されます。 ノードの重みで引いた値が10になるため、次に左側のノードにいき、結果として40の重みをもつデータがサンプリングされています。\nこの葉にたどり着く確率は$$\\frac{40}{100} \\times \\frac{40}{40} \\times \\frac{40}{40} = \\frac{40}{100}$$となりますので、狙い通り40%の確率でサンプリングできます。\nまた、見てわかるように、二分木を使うことで葉の重みを個別には見ずに和を利用するため、計算量を減らすことができています。\nサンプリングの例2 また別の例として乱数が50のときも示します。 重み30の葉にたどり着いていますが、そうなる確率は $$\\frac{60}{100} \\times \\frac{35}{60} \\times \\frac{30}{35} = \\frac{30}{100}$$です。\nサンプルコード Python実装を最後に示します。\nimport numpy as np from typing import List, Optional, Union class Node: def __init__(self, weight, parent): self.weight: int = weight self.left: Optional[Node, Leaf] = None self.right: Optional[Node, Leaf] = None self.parent: Optional[Node, Leaf] = parent class Leaf: def __init__(self, parent): self.weight = 0 self.val = 0 self.parent: Optional[Node] = parent class SumTree: def __init__(self, max_leaves): self._root: Node = Node(weight=0, parent=None) self._max_leaves: int = max_leaves self._leaf_index: int = 0 self._reached_limit: bool = False self._all_leaves: List[Leaf] = [] self._init_tree() def _init_tree(self): hierarchy = int(np.ceil(np.log2(self._max_leaves))) pre_hierarchies = [self._root] for _ in range(1, hierarchy): new_hierarchies = [] for pre_hierarchy_node in pre_hierarchies: left_node = Node(0, paent=pre_hierarchy_node) right_node = Node(0, parent=pre_hierarchy_node) pre_hierarchy_node.left = left_node pre_hierarchy_node.right = right_node new_hierarchies.append(left_node) new_hierarchies.append(right_node) pre_hierarchies = new_hierarchies for node in pre_hierarchies: node.left = Leaf(parent=node) node.right = Leaf(parent=node) self._all_leaves.append(node.left) self._all_leaves.append(node.right) def _update_node_weight(self, node: Node): while node: weight_sum = 0 if node.left: weight_sum += node.left.weight if node.right: weight_sum += node.right.weight node.weight = weight_sum node = node.parent def __len__(self): return self._max_leaves if self._reached_limit else self._leaf_index def __getitem__(self, item): if item \u0026gt;= self.__len__(): raise Exception(\u0026#34;index out of range\u0026#34;) if not self._reached_limit: idx = item else: idx = self._leaf_index + item if idx \u0026gt;= self._max_leaves: idx -= self._max_leaves return self._all_leaves[idx] def add(self, val, weight): target_leaf = self._all_leaves[self._leaf_index] target_leaf.val = val target_leaf.weight = weight self._leaf_index += 1 if self._leaf_index \u0026gt;= self._max_leaves: self._leaf_index = 0 self._reached_limit = True self._update_node_weight(target_leaf.parent) def sample(self): target_val = np.random.random() * self._root.weight target_node = self._root while True: if target_node.left.weight \u0026gt; target_val: target_node = target_node.left else: target_val -= target_node.left.weight target_node = target_node.right if isinstance(target_node, Leaf): break return target_node 次のように使用します。\nst = SumTree(max_leaves=10) st.add(\u0026#34;a\u0026#34;, 5) st.add(\u0026#34;b\u0026#34;, 20) st.add(\u0026#34;c\u0026#34;, 30) st.add(\u0026#34;d\u0026#34;, 5) st.add(\u0026#34;e\u0026#34;, 40) st.sample() 次のようにして、サンプリングが上手くいっているかどうかを確認できます。\ncount_dict = {\u0026#34;a\u0026#34;: 0, \u0026#34;b\u0026#34;: 0, \u0026#34;c\u0026#34;: 0, \u0026#34;d\u0026#34;: 0, \u0026#34;e\u0026#34;: 0} for i in range(100000): count_dict[st.sample().val] += 1 print(count_dict) # {\u0026#39;a\u0026#39;: 4941, \u0026#39;b\u0026#39;: 19941, \u0026#39;c\u0026#39;: 29892, \u0026#39;d\u0026#39;: 5234, \u0026#39;e\u0026#39;: 39992} ","date":"2021-01-17T00:00:00Z","image":"https://opqrstuvcut.github.io/blog/posts/sum-tree%E3%81%A7%E9%87%8D%E3%81%BF%E3%81%AB%E3%81%9D%E3%81%A3%E3%81%A6%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%99%E3%82%8Bpython%E5%AE%9F%E8%A3%85/sum_tree_example_hu4339e54fcdc467e3bb2cb12deb8a6e1b_32825_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/sum-tree%E3%81%A7%E9%87%8D%E3%81%BF%E3%81%AB%E3%81%9D%E3%81%A3%E3%81%A6%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AA%E3%83%B3%E3%82%B0%E3%81%99%E3%82%8Bpython%E5%AE%9F%E8%A3%85/","title":"Sum Treeで重みにそってサンプリングする（Python実装）"},{"content":"Workload Identityについて Compute Engineのホストからは何もせずにGCPのサービスにアクセス可能ですが、GKEを利用しているときに、クラスタ内のコンテナからStorageなどのサービスにどうやってアクセスするかという話がでてきます。\nGCPのサービスアカウントのキーをコンテナ内に配置できればよいですが、それを実現すると今度はセキュリティ上の問題があらわれたりします。\nこういった問題を解決してくれるのがWorkload Identityです。Workload IdentityではKubernetesサービスアカウントというものとGCPのサービスアカウントの紐付けをおこない、KubernetesサービスアカウントでGCPのサービスを利用できるようにします。\n手順は基本的には公式に従えばいいですが、ここでは簡素化したものを載せておきます。\n手順   プロジェクト用のサービスアカウントを作成します。サービスアカウントに適切な権限を設定しておく必要あるので、注意しておこないます。下記で利用される\u0026quot;GSA\u0026quot;はこのサービスアカウントの名称をあらわしています。\n  GCPのコンソールかgcloudコマンドのいずれかでクラスターを作成します。このときworkload identityの有効化を必ずおこないます。 既存のクラスターに対しては次でWorkload Identityの有効化ができるようです（未確認）。\n$ gcloud container clusters update \u0026lt;クラスタ名\u0026gt; \\  --workload-pool=\u0026lt;プロジェクトID\u0026gt;.svc.id.goog   terminal上で、作成したクラスターを操作できるようにします。例えば次のようなコマンドを実行します。\n$ gcloud container clusters get-credentials \u0026lt;作成したクラスタ名\u0026gt; \\  --zone \u0026lt;asia-northeast1-aなどのzoneの指定\u0026gt;   Kubernetes Service Account（KSA）を作成します。\n$ kubectl create serviceaccount --namespace default \\  \u0026lt;KSAの名称（適当につける）\u0026gt; 次のようなmanifestを作成し、podを作成しておくでもOKなはずです。\napiVersion:v1kind:ServiceAccountmetadata:name:\u0026lt;KSAの名称\u0026gt;  GSAがSAと紐付けできるように権限を付与します。\n$ gcloud iam service-accounts add-iam-policy-binding \\  --role roles/iam.workloadIdentityUser \\  --member \u0026#34;serviceAccount:\u0026lt;プロジェクトID\u0026gt;.svc.id.goog[default/\u0026lt;KSAの名称\u0026gt;]\u0026#34; \\  \u0026lt;GSAの名称\u0026gt;@\u0026lt;プロジェクトID\u0026gt;.iam.gserviceaccount.com   最後に次を実行。\n$ kubectl annotate serviceaccount \\  --namespace default \\  \u0026lt;KSAの名称\u0026gt; \\  iam.gke.io/gcp-service-account=\u0026lt;GSAの名称\u0026gt;@\u0026lt;プロジェクトID\u0026gt;.iam.gserviceaccount.com   ","date":"2021-01-16T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/gcp%E3%81%AEworkload-identity%E3%81%A7gke%E3%81%A8gcp%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%81%A8%E3%81%AE%E9%80%A3%E6%90%BA%E3%82%92%E5%AE%89%E5%85%A8%E3%81%AB%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/","title":"GCPのWorkload IdentityでGKEとGCPサービスとの連携を安全におこなう"},{"content":"たまにTensorBoardを使うときに、ホスト環境などにTensorBoardを入れるより、それ用のコンテナをたてたくなったので、そのメモです。\nDocker Imageは以下のとおり。\nFROM python:3.8 RUN pip install tensorflow WORKDIR /logs ENTRYPOINT [\u0026quot;tensorboard\u0026quot;, \u0026quot;--logdir\u0026quot;, \u0026quot;/logs\u0026quot;, \u0026quot;--host\u0026quot;, \u0026quot;0.0.0.0\u0026quot;] 次のような感じでdocker buildとdocker runします。\n$ docker build -t tensorboard . $ docker run -it --rm -p 10000:6006 -v $PWD/logs:/logs tensorboard -vに指定するhost側のlogのディレクトリのパスと-pに指定するportは適当に変更する。\n","date":"2020-11-18T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/tensorboard%E3%81%AEdocker-image/","title":"TensorBoardのDocker Image"},{"content":"最近では実験用の環境なんかもDockerコンテナ上に用意することも多いです。\ndocker-composeも使うわけですが、たまにdocker-composeのbuildがいつになってもはじまらないことがあります。\nBuilding xxxがずっと表示されて、そこから進まないわけです。\n以前も同じケースに出くわしたのにすぐ思い出せなかったので、備忘録的に残りしておきます（似た記事はネットにたくさんありますが）。\ndocker-composeのbuildがはじまらない原因 学習に使うデータのように重いファイルを置いてあるディレクトリでdocker-composeをおこなうのが原因です。なぜこれが原因でbuildがはじまらないかといえば、Docker Imageのbuildをするときに、Dockerfileがあるディレクトリ上のデータはすべてDockerのデーモンに渡されるためです。\n全部のファイルをデーモンに渡そうとするので、学習データなんかがDockerfileと同じディレクトリ上にあると、それらの重たいファイルも渡そうとしてしまい、いつになってもbuildが進まないわけですね。\n対処方法  .dockerignoreにデーモンに渡してほしくないディレクトリ、ファイルを指定する ファイルパスを工夫する（でもdockerignoreを使うのが一番いいんじゃないでしょうか）  ","date":"2020-11-08T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/docker-compose%E3%81%AEbuild%E3%81%8C%E3%81%AF%E3%81%98%E3%81%BE%E3%82%89%E3%81%AA%E3%81%84%E3%81%A8%E3%81%8D/","title":"docker-composeのbuildがはじまらないとき"},{"content":"GPUサーバー上でTensorFlowを動かすアプリを作成し、nginxとの間にはuWSGIを挟む構成にしていたところ、次のエラーが出てしまいました。\nFailed to get device properties, error code: 3 他の記事の引用になってしまいますが、エラーメッセージ自体をググっても解決できなかったので、メモ程度に載せておきます。\n原因 確かとは断言できないのですが、次の記事にかかれていることが怪しいと推測しました。 https://keng000.hatenablog.com/entry/2020/05/05/092425\nつまり、マルチスレッドでのモデルの読み込み方が良くないのかと。\n対処 記事に書かれている通り、uWSGIのiniファイルで次のように追記しました。\n[uwsgi] lazy-apps = true とりあえずこれで解決しました。\n","date":"2020-11-08T00:00:00Z","permalink":"https://opqrstuvcut.github.io/blog/posts/gpu%E3%82%B5%E3%83%BC%E3%83%90%E3%83%BC%E3%81%A7%E3%81%AEtensorflow-uwsgi%E3%81%A7failed-to-get-device-properties-error-code-3/","title":"GPUサーバーでのTensorFlow + uWSGIでFailed to get device properties, error code: 3"},{"content":"本記事はQrunchからの転載です。\n 画像から前景と背景を分けるのは以前に取り上げたのですが、動画でもOpenCVで前景と背景をわけることが可能です。ここでいう前景は動いている物体を指します。\n前景と背景を分離する難しさ 動画から前景と背景を分離するアルゴリズムを自分で実装するのは結構大変です。 最も単純なアルゴリズムは背景だけが写っている画像を撮っておいて、運用時には背景画像とリアルタイムに取得された画像との差分を取るというのが考えられます。 ただしこのやり方だと照明環境は一定にしないといけないのですが、問題設定によってはそうできなかったりします。また背景だけの画像を撮るのが難しい場合もあります。\n問題の難しさから、リッチな処理をしたくなるのですが、変に処理をすると計算時間が伸びていく可能性もあります。\nOpenCVでやってみる OpenCVのBackgroundSubtractorMOG2を使うと、簡単に照明の変化にも適応する手法を利用できます。背景画像を撮る必要もありません。 BackgroundSubtractorMOG2では背景と前景を分離するために混合ガウス分布を利用しています。 混合ガウス分布の学習はいつするの？という話ですが、これはリアルタイムに更新されていきます。\u0008リアルタイムで更新するので照明変化などにも対応できるわけですね。\n今回のテスト用の動画としてこちらを利用させていただきました。 道路を車がビュンビュン走っています。\n次のようにしてBackgroundSubtractorMOG2を利用できます。\nimport cv2 import numpy as np cap = cv2.VideoCapture(\u0026#34;ex.mp4\u0026#34;) fgbg = cv2.createBackgroundSubtractorMOG2(history=60, detectShadows=False) masks = [] kernel = np.ones((5, 5), np.uint8) while True: ret, frame = cap.read() if not ret: break fgmask = fgbg.apply(frame) fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel) masks.append(fgmask) cap.release() cv2.destroyAllWindows() createBackgroundSubtractorMOG2に渡している引数ですが、history=60とすることで、直近の60フレームだけをモデルに考慮させているようなイメージです（正確にそうなるわけではないはずですが）。 また、detectShadows=Trueの場合には影も検出できるのですが、不要なのでFalseにしています。この機能を切っておいたほうが少し早くなります。\nfgbg.apply(frame)の返り値が前景の検出結果（mask画像）になります。 ちなみに、検出された結果にオープニング処理を入れてノイズを減らしています。今回の動画ではオープニング処理を入れないと次のように結構ノイズが拾われてしまいます。\n検出されたマスクにオープニング処理も入れた結果が次のとおりです（GIFが動かない場合はクリックしてみてください）。\n コード全体 import cv2 import numpy as np cap = cv2.VideoCapture(\u0026#34;ex.mp4\u0026#34;) fgbg = cv2.createBackgroundSubtractorMOG2(history=60, detectShadows=False) masks = [] kernel = np.ones((5, 5), np.uint8) while True: ret, frame = cap.read() if not ret: break fgmask = fgbg.apply(frame) fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel) masks.append(fgmask) cv2.imshow(\u0026#39;frame\u0026#39;, fgmask) k = cv2.waitKey(30) if k == ord(\u0026#39;q\u0026#39;): break cap.release() cv2.destroyAllWindows() fourcc = cv2.VideoWriter_fourcc(\u0026#34;m\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;v\u0026#34;) writer = cv2.VideoWriter(\u0026#34;res.mp4\u0026#34;, fourcc, 30, tuple(masks[0].shape[::-1]), 0) for mask in masks: writer.write(mask) writer.release() ","date":"2020-08-20T11:04:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E5%8B%95%E7%94%BB%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E5%89%8D%E6%99%AF%E3%81%A8%E8%83%8C%E6%99%AF%E3%82%92%E5%88%86%E9%9B%A2%E3%81%99%E3%82%8B/cbb562e9eaee89cb60ac8ec0be592a80_hu079f0db8015dc5d47a1ebef470e8ebdd_27984_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E5%8B%95%E7%94%BB%E3%83%87%E3%83%BC%E3%82%BF%E3%81%8B%E3%82%89%E5%89%8D%E6%99%AF%E3%81%A8%E8%83%8C%E6%99%AF%E3%82%92%E5%88%86%E9%9B%A2%E3%81%99%E3%82%8B/","title":"動画データから前景と背景を分離する"},{"content":"本記事はQrunchからの転載です。\n OpenCVでは二値画像から結合している領域の抽出をおこなうことができます。 こういうのは自分で実装すると大変なので、大変助かりますね。\nconnectedComponets 次の二値画像を考えます。 領域として取り出したいのは2つの白い部分です。\nconnectedComponetsを使って簡単にこの2つの領域を抽出できます。\nn_labels, labels = cv2.connectedComponents(bi_img) n_labelsはラベル付けされた領域の数です。 labelsには入力画像と同じサイズの行列が入っており、それぞれの座標の値がその位置での領域のラベルをあらわします。\nラベルごとに色付けしてみると、次のようになります。\ncolored_img = np.zeros(bi_img.shape + (3,), dtype=np.uint8) for i in range(1, n_labels): colored_img[labels == i] = [np.random.randint(0, 256), np.random.randint(0, 256), np.random.randint(0, 256)] plt.imshow(colored_img) plt.show() ","date":"2020-08-18T11:00:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/connectedcomponents%E3%81%A7%E9%80%A3%E7%B5%90%E3%81%97%E3%81%9F%E9%A0%98%E5%9F%9F%E3%81%AE%E5%8F%96%E5%BE%97/4fad02405d448c838cb1af94842c2bbb_huc336200167bfc01149489055c5443329_25774_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/connectedcomponents%E3%81%A7%E9%80%A3%E7%B5%90%E3%81%97%E3%81%9F%E9%A0%98%E5%9F%9F%E3%81%AE%E5%8F%96%E5%BE%97/","title":"connectedComponentsで連結した領域の取得"},{"content":"本記事はQrunchからの転載です。\n OpenCVのfindContoursで見つけた輪郭はdrawContoursで簡単に描画できます。 次のようにして使えます。\ndrawed = cv2.drawContours(img, contours=contours, contourIdx=-1, color=(255, 0, 0), thickness=10, lineType=8, hierarchy=hierarcies, maxLevel=1) 引数の意味はそれぞれ次のとおりです。必須なのはcolorまでです。\n   引数 意味     contours findContoursで見つかった輪郭   contourIdx 描画する輪郭のインデックスを指定する（-1だと全て描画）   color 描画する輪郭の色   thickness 描画する輪郭の太さ   lineType 4、8、cv2.LINE_AAのどれかを指定し、後のほうがきれいに描画される   hierarchy findContoursで見つかった輪郭の階層構造   maxLevel 描画する最大の階層を指定する    maxLevelを1にしたときと、2にしたときの違いを次に示します。\n   maxLevel=1 maxLevel=2          maxLevelが2のときには外側の輪郭の中まで輪郭が描画されていますね。\n","date":"2020-08-17T11:03:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E6%8A%BD%E5%87%BA%E3%81%97%E3%81%9F%E8%BC%AA%E9%83%AD%E3%81%AE%E6%8F%8F%E7%94%BB/d0411dcb235b3add1e1a55f1a9394d2d_hu4313cfe2dd0d98bca2a11b97cc7461c5_40382_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E6%8A%BD%E5%87%BA%E3%81%97%E3%81%9F%E8%BC%AA%E9%83%AD%E3%81%AE%E6%8F%8F%E7%94%BB/","title":"抽出した輪郭の描画"},{"content":"本記事はQrunchからの転載です。\n 画像から物体の輪郭を見つけたくなることが多々あります。 そんなときにもOpenCVを利用することができます。\nfindContoursで輪郭抽出 次の画像から輪郭の抽出をおこなうことを考えます。\n最初に次のように二値化しておきます。\n_, bi_img = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) これに対して次のようにfindContoursを適用します。\ncontours, hierarcies = cv2.findContours(bi_img, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)  第二引数は輪郭の取り出し方を指定しており、cv2.RETR_EXTERNALは一番外側の輪郭だけを取り出します。ここに指定できる方法の比較は後でおこないます。 第三引数は輪郭の近似方法をあらわします。例えば、cv2.CHAIN_APPROX_SIMPLEにすると、返ってくる点の数が大きく減ります。cv2.CHAIN_APPROX_TC89_L1にすると返ってくる点の数をうまい具合に減らしてくれますが、他に比べると計算量がかかります。 返り値の1つめが輪郭を格納したリストです。２つめが輪郭の階層構造をあらわしています。  細かく言うと、輪郭の方は、点のリストが1つの輪郭をあらわし、それらのリストが格納されています。 階層構造の方は、輪郭ごとに１つの階層構造をあらわす4つの要素をもつリストが存在します。各要素の0番目は次の輪郭のインデックス、1番目は前の輪郭のインデックス、2番目は子の輪郭のなかで1番目のインデックス、3番目は親の輪郭のインデックスをあらわします。親と子が何かといえば、親はみている輪郭を囲んでいる輪郭のことで、子は中にある輪郭のことです。    見つかった輪郭を次のように描画してみます。\ndrawed = cv2.drawContours(np.stack([img, img, img], axis=-1), contours, -1, (255, 0, 0), 10) plt.imshow(drawed) plt.show() 描画の結果は以下のとおりです。\n輪郭の取り出し方を変えてみる 先程は輪郭の取り出し方にcv2.RETR_EXTERNALを指定しました。これは一番外側の輪郭しか取れません。 次にちゃんと階層構造をもった結果を返すようにしてみます。これにはcv2.RETR_TREEを指定します。\ncontours, hierarcies = cv2.findContours(bi_img, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE) 他にもcv2.RETR_LISTやcv2.RETR_CCOMPなどがありますが、hierarciesの中の階層構造の情報の持ち方が変わってきます。\n","date":"2020-08-16T17:56:36+09:00","image":"https://opqrstuvcut.github.io/blog/posts/findcontours%E3%81%A7%E8%BC%AA%E9%83%AD%E3%81%AE%E6%A4%9C%E5%87%BA/36626599de1a642e7bfec7fa47def298_huebb2864a8a513da9c313f1fee4e2d83c_46193_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/findcontours%E3%81%A7%E8%BC%AA%E9%83%AD%E3%81%AE%E6%A4%9C%E5%87%BA/","title":"findContoursで輪郭の検出"},{"content":"本記事はQrunchからの転載です。\n カメラを固定しておいて、何らかの被写体を取り続けるということはよくある問題設定です。 ただし、被写体の位置が毎回少しズレるということも多々あります。 そんなときにテンプレートマッチングを使うことができます。\nテンプレートマッチングについて テンプレートマッチングではテンプレート画像と呼ばれるものを事前に用意しておきます。 そして、検出したいものが写っている画像の左上の領域から順にテンプレート画像とどれくらい似ているかを計算していきます。 このようにして、テンプレート画像とよく似た領域を検出するというのがテンプレートマッチングです。\nOpenCVでテンプレートマッチング 次の左の画像をテンプレート画像として、右から同じ物体を検出してみます。\nテンプレートマッチングは次のようにしておこなえます。\nres = cv2.matchTemplate(img, template, cv2.TM_CCORR_NORMED) cv2.TM_CCORR_NORMEDは類似度の計算の方法です。 選択肢は複数あり、手法によって精度と計算時間が変わります。 詳細はこちらをご確認ください。\n返り値には各位置での類似度が格納されています。\nTM_CCORR_NORMEDの場合には大きな値ほど、似ていますので明るい部分がもっともテンプレートとマッチしたことをあらわします。\nこの部分の画像を次のように切り抜いてみます。\n_, max_val, _, max_loc = cv2.minMaxLoc(res) height, width = template.shape plt.imshow(img[max_loc[1]: max_loc[1] + height, max_loc[0]: max_loc[0] + width]) plt.show() 結果は以下のとおりです。 バッチリできていることがわかります。\n","date":"2020-08-15T11:09:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%8B%E3%82%89%E7%89%A9%E4%BD%93%E3%82%92%E3%81%BF%E3%81%A4%E3%81%91%E3%82%8B/0359ef5bc2687ab5c1e63356c89cc3e0_hu26ccc0caaa27d75445cd03ca7c8ff782_109523_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E3%83%86%E3%83%B3%E3%83%97%E3%83%AC%E3%83%BC%E3%83%88%E3%83%9E%E3%83%83%E3%83%81%E3%83%B3%E3%82%B0%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%8B%E3%82%89%E7%89%A9%E4%BD%93%E3%82%92%E3%81%BF%E3%81%A4%E3%81%91%E3%82%8B/","title":"テンプレートマッチングで画像から物体をみつける"},{"content":"本記事はQrunchからの転載です。\n 行列の最大値、最小値はNumPyのmaxやmin、またそれらのインデックスはargmaxやargminを使えば取得できるのですが、OpenCVでは一発ですべて取得できます。\nmin_val, max_val, min_idx, max_idx = cv2.minMaxLoc(np.array([[1, 2, 3], [4, 5, 6]])) print(min_val, max_val, min_idx, max_idx) この出力は以下のとおりですが、それぞれ最小値、最大値、最小値の位置、最大値の位置をあらわします。位置は$(x,y)$をあらわしていますので、行列でいえば、（列、行）の順に格納されています。\n(1.0, 6.0, (0, 0), (2, 1)) ","date":"2020-08-11T11:04:00+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/minmaxloc%E3%81%A7%E6%9C%80%E5%A4%A7%E3%81%A8%E6%9C%80%E5%B0%8F%E3%81%AE%E4%BD%8D%E7%BD%AE%E3%82%92%E6%A5%BD%E3%81%AB%E5%8F%96%E5%BE%97/","title":"minMaxLocで最大と最小の位置を楽に取得"},{"content":"本記事はQrunchからの転載です。\n 画像処理の領域では画像から特徴量をあらわすヒストグラムを生成することがよくあります。 特徴量としてヒストグラムを生成するということは、比較をすることもよくあるということで、今回はヒストグラムの比較を扱います。\ncompHistによるヒストグラムの比較の仕方 次のようにしてヒストグラムの比較をおこないます。\ncv2.compareHist(hist_1, hist_2, method) hist_1とhist_2はヒストグラムをあらわすNumPy arrayです。 methodは比較方法をあらわし、以下のようなものがあります。\n   方法 概要     cv2.HISTCMP_CORREL ピアソンの相関係数   cv2.HISTCMP_CHISQR カイ二乗検定   cv2.HISTCMP_KL_DIV KLダイバージェンス   cv2.HISTCMP_INTERSECT 交差法   cv2.HISTCMP_BHATTACHARYYA バタチャリア距離    それぞれの違いは式を見ればわかるという話もありますが、ぱっと分かるように数値的な違いを見ていきます。\n比較方法の一覧 次のようなヒストグラムを対象にして各比較方法の違いをみてみます。 結果は次のとおりです。\n   比較方法 2と2 1と2 2と1 2と3 1と3     HISTCMP_CORREL 1.0 0.22 0.22 -0.22 -0.87   HISTCMP_CHISQR 0.0 10.13 9.11 11.78 18.0   HISTCMP_KL_DIV 0.0 245.6 228.07 234.31 447.40   HISTCMP_INTERSECT 18.0 8.0 8.0 4.0 0.0   HISTCMP_BHATTACHARYYA 0.0 0.73 0.73 0.82 1.0    手法によって、完全一致は大きい値になるのか、小さい値になるのか、また最大値と最小値はあるのかといったところも違うので、注意が必要です。\nなお、利用したコードは以下のとおりです。\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt hist_1 = np.array([4, 6, 8, 0, 0, 0], dtype=np.float32) hist_2 = np.array([0, 0, 9, 9, 0, 0], dtype=np.float32) hist_3 = np.array([0, 0, 0, 4, 6, 8], dtype=np.float32) comp_1_2_results = [] comp_2_1_results = [] comp_2_3_results = [] comp_1_3_results = [] comp_2_2_results = [] methods = [\u0026#34;HISTCMP_CORREL\u0026#34;, \u0026#34;HISTCMP_CHISQR\u0026#34;, \u0026#34;HISTCMP_KL_DIV\u0026#34;, \u0026#34;HISTCMP_INTERSECT\u0026#34;, \u0026#34;HISTCMP_BHATTACHARYYA\u0026#34;] for method_name in methods: method = getattr(cv2, method_name) comp_1_2_results.append(cv2.compareHist(hist_1, hist_2, method)) comp_2_1_results.append(cv2.compareHist(hist_2, hist_1, method)) comp_2_3_results.append(cv2.compareHist(hist_2, hist_3, method)) comp_1_3_results.append(cv2.compareHist(hist_1, hist_3, method)) comp_2_2_results.append(cv2.compareHist(hist_2, hist_2, method)) res_df = pd.DataFrame({\u0026#34;比較方法\u0026#34;: methods, \u0026#34;2と2\u0026#34;: comp_2_2_results, \u0026#34;1と2\u0026#34;: comp_1_2_results, \u0026#34;2と1\u0026#34;: comp_2_1_results, \u0026#34;2と3\u0026#34;: comp_2_3_results, \u0026#34;1と3\u0026#34;: comp_1_3_results, }) ","date":"2020-08-10T13:20:47+09:00","image":"https://opqrstuvcut.github.io/blog/posts/comphist%E3%81%A7%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AF%94%E8%BC%83%E3%82%92%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E3%81%AA%E3%82%84%E3%82%8A%E6%96%B9%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/ae3cebef75b7a49ca03c77500f3925a0_hu9104ce78f1f2bfac9f75d762ca1268ff_5783_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/comphist%E3%81%A7%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E6%AF%94%E8%BC%83%E3%82%92%E3%81%84%E3%82%8D%E3%81%84%E3%82%8D%E3%81%AA%E3%82%84%E3%82%8A%E6%96%B9%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/","title":"compHistでヒストグラム比較をいろいろなやり方でおこなう"},{"content":"本記事はQrunchからの転載です。\n 画像処理や集計、機械学習では何かとヒストグラムを計算するケースがありますね。\nこれに伴い、ヒストグラムを計算できるライブラリは色々あるかと思いますが、OpenCVでもヒストグラムを計算する機能をもっています。 NumPyでもヒストグラムの計算できるじゃない、と思いますが、実はOpenCVの方がNumPyのヒストグラムよりも断然速いです。今回はその辺りの比較もおこなっていきます。\nOpenCVのヒストグラム せっかくOpenCVを使うので、以下の画像の画素値のヒストグラムを計算してみます。\nOpenCVでのヒストグラムの計算は以下のようにおこなえます。\nhist = cv2.calcHist(images=[img], channels=[0], mask=None, histSize=[256], ranges=(0, 256))  imagesにはヒストグラムの計算のもととなる画像をリストの形式で渡します。 channelsには画像のチャネルのうち、どれを用いてヒストグラムを計算するかを指定します。いまはグレースケールで1チャネルしかないため、0を指定しています。カラー画像のときにはBGRの3チャネルなので、channelに対応する0~2のどれかを指定します。 maskには画像と同じサイズの1チャネルのマスクを与えることで、ヒストグラムを計算する領域を制限できます。 histSizeにはヒストグラムのbinの数を与えます。 rangesにはヒストグラムの下限と上限を指定します。厳密には(0,256)を与えるということは$[0, 256)$のような区間をあらわすことに注意してください。  結果を以下のように描画してみます。\nplt.bar(range(len(hist)), hist.ravel()) plt.ylabel(\u0026#34;freq\u0026#34;) plt.xlabel(\u0026#34;val\u0026#34;) plt.show() NumPyとの比較 cv2.calcHistによって得たヒストグラムと全く同じヒストグラムをNumPyを用いて得ることができます。 具体的には次のようにします。\nnumpy_hist, bin_edges = np.histogram(img.ravel(), bins=256, range=(0, 255)) さて、速度はどれくらい違うかという話になりますが、%%timeitによって測定した結果が以下のとおりです。\n   方法 timeitの結果     cv2.calcHist 2.95 ms ± 186 µs per loop   np.histogram 109 ms ± 7.22 ms per loop    36倍程度OpenCVのほうが速いことがわかります。 全然違うのでびっくりしますね。\n","date":"2020-08-10T11:03:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/opencv%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E3%81%AE%E8%A8%88%E7%AE%97%E3%81%AFnumpy%E3%82%88%E3%82%8A%E6%96%AD%E7%84%B6%E9%80%9F%E3%81%84/5703e41eaa88a7eb01fcb7ef796e3bb3_hu059ae91a6bbe2d88e31d1aaed36f2ac4_72196_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/opencv%E3%81%AE%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E3%81%AE%E8%A8%88%E7%AE%97%E3%81%AFnumpy%E3%82%88%E3%82%8A%E6%96%AD%E7%84%B6%E9%80%9F%E3%81%84/","title":"OpenCVのヒストグラムの計算はNumPyより断然速い"},{"content":"本記事はQrunchからの転載です。\n 次のような画像があったとします。\nここから猫だけ抽出したいときに、ツールを使えば少し手間はかかりますが、切り取れると思います。\n実はOpenCVのGrabcutsを使えば非常に簡単にそれが実現できます。 （ディープラーニング使えばできるよね？はおいておいて）\nGrabcutsを使ってみる 矩形を指定 最初に猫を囲うような矩形を指定する方法を試していきます。 OpenCVのGrabcutsは以下のように利用できます。\nbgd_model = np.zeros((1, 65), np.float64) fgd_model = np.zeros((1, 65), np.float64) rect = (0, 30, 300, 120) mask = np.zeros(img.shape[:2], np.uint8) cv2.grabCut(img, mask, rect, bgd_model, fgd_model, 10, cv2.GC_INIT_WITH_RECT) 各引数の意味は以下のとおりです。\n maskの詳細は一旦おいておきます。 rectは猫を囲う矩形をあらわし、$(x,y,w,h)$の形式のタプルです。 bgd_modelとfgd_modelは内部で利用する変数なのですが、わざわざ外から与える必要があります。 なぜかといえば、grabCut関数を適用したあとに、同じ画像に再度grabCutを適用したいケースがあるのですが、そういったときに同じbgd_modelとfgd_modelを使い回す必要があるためです。 そのため、外から変数を与えられるようになっています。 6つめの引数の10とあるのは、アルゴリズムの反復回数です。 最後のcv2.GC_INIT_WITH_RECTは指定した矩形をもとに前景である猫を抽出してくださいと指定しているflagです。  分割された領域の情報はmaskに格納されます。 maskに格納される値は以下のような意味になります。\n 0は確実に背景 1は確実に前景 2は多分背景 3は多分前景  以下のようにして抽出された前景を抽出します。\ndef plot_cut_image(img, mask): cut_img = img * np.where((mask==1) | (mask==3), 1, 0).astype(np.uint8)[:, :, np.newaxis] plt.imshow(cut_img[:, :, ::-1]) plt.show() 上手く猫だけを抽出できていますね。\nmaskを指定 次に下の画像から猫を抽出することを考えます。\nまずは、さきほどと同じようにやってみます。\nbgdModel = np.zeros((1,65),np.float64) fgdModel = np.zeros((1,65),np.float64) rect = (0, 30, 300, 120) mask = np.zeros(img.shape[:2],np.uint8) cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_RECT) 椅子と猫の色味が似ているためか上手くいきません。\nここでmaskの出番です。 前景として扱いたい部分をmaskに指定してあげることができます。 猫の顔の右下の部分を前景としたいので、その部分のmaskの値を1にします。\nまた、grabCutの最後の引数もcv2.GC_INIT_WITH_MASKというflagに変えることで、maskを使えるようにします。\nmask[100:130, 200:280] = 1 cv2.grabCut(img, mask, rect, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_MASK) いい感じです！ 追加で猫の顔の右上もmaskに前景として指定します。\nmask[50:100, 250:270] = 1 cv2.grabCut(img, mask, None, bgdModel, fgdModel, 10, cv2.GC_INIT_WITH_MASK) ほぼほぼ上手く猫が抽出できました！\n","date":"2020-08-09T11:00:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/dba26e4507ff86058f0c703b8298c1f9_hu8e5f3a9dc326abbf2e84a9c98158d4c1_37539_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/grabcuts%E3%81%A7%E8%83%8C%E6%99%AF%E3%81%A8%E7%8C%AB%E3%82%92%E5%88%86%E9%9B%A2%E3%81%97%E3%81%9F%E3%81%84/","title":"Grabcutsで背景と猫を分離したい"},{"content":"本記事はQrunchからの転載です。\n Watershedと呼ばれる方法を使うと、指定したマーカーの情報と画像のエッジから画像中の領域の分割をおこなってくれます。 マーカーとしては、この位置は領域1、この位置は領域2それ以外は背景だよといった感じの情報を与えます。\n実際にOpenCVでやってみましょう。\nOpenCVでWatershed 次の画像にWaterShedを適用してみます。\nいま、4つの物体が写っていますので、これを4つの領域と背景に分けることを考えます。 マーカーは以下のように指定します。\nmarker = np.zeros((504, 378), np.int32) marker[90:130, 100:130] = 1 marker[230:270, 125:180] = 2 marker[120:150, 250:280] = 3 marker[280:310, 290:320] = 4 markerに代入した1~4の値がそれぞれの物体上にくるようにしています。 マーカーの位置と画像を重ねると次のようになります。\nOpenCVのWatershedは次のようにして実行できます。\nres = cv2.watershed(img, marker) 返り値には領域を分割した結果をあらわす行列が格納されています。 行列のサイズは画像と同じになっていて、各要素の値はその座標がどの領域かを示した値が入っています。 描画してみると以下のようになります。\n3つはちゃんと領域が分割できています。 白いボトルは上手くいきませんでした。エッジがあまり取れていないのかもしれないです。\n","date":"2020-08-08T11:10:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/watershed%E3%81%A7%E9%A0%98%E5%9F%9F%E6%A4%9C%E5%87%BA/37b16430461df0454f7771d8d4bbfaef_hue580e7aeb2deef823b73a9f7d4d34136_8885_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/watershed%E3%81%A7%E9%A0%98%E5%9F%9F%E6%A4%9C%E5%87%BA/","title":"Watershedで領域検出"},{"content":"本記事はQrunchからの転載です。\n 画像に対する距離変換とは、グレースケールの画像において、ピクセルから最も近い0の値をもつピクセルまでの距離を求めたものです。\n早速OpenCVで試してみます。\nOpenCVで距離変換 次のようにして距離変換をおこなえます。\ndist = cv2.distanceTransform(img, distanceType=cv2.DIST_L2, maskSize=5 ) distanceTypeに距離の計算方法を指定します。DIST_L2はユークリッド距離です。 maskSizeには最も近い0の値をもつピクセルまでの距離の近似値を計算するときに使うmaskの大きさを指定します。maskSize=5の例でいえば、maskをあらわす$5\\times5$の行列の各要素にはmaskの中心からの距離が格納されています。このmaskを使うことで、正確に距離を計算するよりも速く距離（の近似値）が計算できます。\n結果は以下のとおりです。\n   入力画像 距離変換適用（明るいほど距離大）          背景が0の値をもつので、そこまでの距離が反映されています。窓の中心や、猫の顔の中心は背景から遠いので、大きな値をもっています。\n","date":"2020-08-07T11:00:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%AE%E8%B7%9D%E9%9B%A2%E5%A4%89%E6%8F%9B%E3%82%92%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/10fec2481837301639f81cf34c21e4b1_huc44386e84077c5479b72411ac06532c0_16926_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%AE%E8%B7%9D%E9%9B%A2%E5%A4%89%E6%8F%9B%E3%82%92%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86/","title":"画像の距離変換をおこなう"},{"content":"本記事はQrunchからの転載です。\n OpenCVのfloodFillを使うことで、選んだ点の周辺の似たような色のピクセルを塗りつぶすことができます。\n使い方 次のようにしてfloodFillを利用できます。\nmask = np.zeros((img.shape[0] + 2, img.shape[1] + 2), dtype=np.uint8) res = cv2.floodFill(img, mask=mask, seedPoint=(400, 700), newVal=(0, 0, 255), loDiff=30, upDiff=30) まずmaskですが、入力画像の$(x,y)$がmaskの$(x+1, y+1)$に対応し、maskの値が0でないところは塗りつぶされません。入力画像に比べて縦横が2ピクセルずつ大きいので、元の画像の周辺に1ピクセルずつpaddingができたようなイメージですね。 seedPointに指定した座標が塗りつぶしの処理の起点になります。 newValに塗りつぶす色を指定します。 seedPointに指定したピクセルの値からloDiffを引いた値とseedPointに指定したピクセルの値にupDiffを加えた値の間に入っているピクセルをseedPointの隣から順に塗りつぶしていきます。\n結果は以下のとおりです。\n   入力画像 floodFillの結果          ","date":"2020-08-06T11:08:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/floodfill%E3%81%A7%E9%A0%98%E5%9F%9F%E3%81%AB%E8%89%B2%E3%82%92%E5%A1%97%E3%82%8B/b7d4e85930d3ec37e0350ab3fe75e877_hu7a2287441c4c8a30607a7f2d938c96e2_46814_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/floodfill%E3%81%A7%E9%A0%98%E5%9F%9F%E3%81%AB%E8%89%B2%E3%82%92%E5%A1%97%E3%82%8B/","title":"floodFillで領域に色を塗る"},{"content":"本記事はQrunchからの転載です。\n Hough変換は直線を検出する方法として前回紹介したのですが、Hough変換を応用することで、円の検出も行えます。\nOpenCVで円の検出 次の画像から円を検出してみます。\n円の検出は以下のようにおこないます。\nhough_circle = cv2.HoughCircles(img, method=cv2.HOUGH_GRADIENT, dp=1, minDist=5, param1=100, param2=80) HoughLinesと異なり、画像はグレースケールの状態で渡せば、なかでエッジ検出をおこなってくれます。\nmethodには手法を指定しますが、HOUGH_GRADIENTしかないようです。\ndpには分解能を指定しています。1にすると画像の解像度と同じ分解能をもちます。\nminDistには円同士の最小の距離を指定します。これより近いと2つの円として認識されません。\nparam1はCanny法のしきい値の上限、param2は円上にあると判定されたエッジの点の数に対するしきい値です。\n結果は以下のとおりです。\n大まかには円が検出できていることがわかります。\n","date":"2020-08-05T11:05:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/hough%E5%A4%89%E6%8F%9B%E3%81%A7%E5%86%86%E3%82%92%E6%A4%9C%E5%87%BA/51aa777f4487689837d45257fa1eba91_hu4a013b8f274b2c6544f4ac056d5ab0b4_40001_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/hough%E5%A4%89%E6%8F%9B%E3%81%A7%E5%86%86%E3%82%92%E6%A4%9C%E5%87%BA/","title":"Hough変換で円を検出"},{"content":"本記事はQrunchからの転載です。\n Hough変換は画像から直線をみつける方法です。\n簡単な原理 入力として2値画像を考えます。 Hough変換では候補となる直線を用意し、直線上にいくつ0でないピクセルがあるかを数えます。 このピクセルの個数が指定したしきい値以上であった場合、その候補の直線は正しい直線として扱います。\nなお、OpenCVでは直線の候補は以下のように$(\\rho, \\theta)$による極座標系であらわされています。 $$ \\rho = x \\cos \\theta + y \\sin \\theta .$$ $\\rho$は原点からの直線の距離、$\\theta$は直線の角度をあらわします。\n$\\theta$が0でないとしたとき、上式をちょっと変形することで見慣れた形の方程式になるかと思います。 $$ y = \\frac{\\rho}{\\sin\\theta} - x \\frac{\\cos \\theta}{\\sin \\theta}. $$\nわざわざ極座標系であらわす理由はなにかというと、$y=ax+b$ような直線に対してy軸に平行な直線を考えるときに、傾きが$\\infty$の直線となり扱いづらくなることを防ぐためです。 極座標系ですと、無理なくy軸に平行な直線を扱うことができます。\nOpenCVで試してみる 次の画像に対してHough変換を適用します。\nHough変換にかける前に、Canny法でエッジを抽出しておきます。\ncanny = cv2.Canny(img, threshold1=50, threshold2=100, apertureSize=3, L2gradient=True) Canny法の結果に対して、次のようにHough変換を適用できます。\nhough_lines = cv2.HoughLines(canny, rho=5, theta=0.01, threshold=300) rhoとthetaはそれぞれの軸方向の直線の候補の分解能になります。小さいほどたくさんの直線が見つかるかと思います。thresholdに直線の候補を採用するかを決めるしきい値を指定します。 また、min_thetaとmax_thetaで見つかる直線のthetaの最小値、最大値を決めることもできます。\n検出された直線のパラメータ$(\\rho, \\theta)$は以下のようにして変換して、画像に直線として書き込んでいます。\nt = 3000 for params in hough_lines: rho, theta = params[0] a = np.cos(theta) b = np.sin(theta) x0 = a * rho y0 = b * rho x1 = int(x0 - t * b) # 媒介変数表示 y1 = int(y0 + t * a) x2 = int(x0 + t * b) y2 = int(y0 - t * a) 結果は以下のとおりです。\nカーテンや窓、猫の底の部分が直線として検出されています。余計な直線も結構検出されています。\n","date":"2020-08-04T19:00:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/hough%E3%83%8F%E3%83%95%E5%A4%89%E6%8F%9B%E3%81%A7%E7%9B%B4%E7%B7%9A%E3%82%92%E8%A6%8B%E3%81%A4%E3%81%91%E3%82%88%E3%81%86/f57bd9c6dc056b82e658a2f6ca7b6258_hu5ddbb1da31877a9e022410b6c73f6fa0_41649_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/hough%E3%83%8F%E3%83%95%E5%A4%89%E6%8F%9B%E3%81%A7%E7%9B%B4%E7%B7%9A%E3%82%92%E8%A6%8B%E3%81%A4%E3%81%91%E3%82%88%E3%81%86/","title":"Hough（ハフ）変換で直線を見つけよう"},{"content":"本記事はQrunchからの転載です。\n エッジ検出の方法として、Canny法というものがあります。 SobelフィルタやLaplacianフィルタもエッジ検出ができるわけですが、Canny法を使うとより正確に輪郭を検出することが可能です。\nCanny法の簡単な原理 勾配の計算 Canny法では画像を平滑化したあとに、Sobelフィルタによって勾配を計算します。 OpenCVでは勾配の大きさは以下の2つのうちのどちらかで計算がなされます。$G_x$と$G_y$はそれぞれ$x$方向、$y$方向の勾配です。\n 2ノルムの場合 $$ \\rm{grad}=\\sqrt{G_x^2 + G_y^2}. $$ 1ノルムの場合 $$ \\rm{grad}= |G_x| + |G_y|. $$  2ノルムのほうが正確ですが、計算量では1ノルムのほうが優れています。\n極大値を求める 次に、計算された勾配から、勾配の極大値を求めます。こうすることで、余計な箇所がエッジとして検出されるのを防ぎます。\nしきい値処理 最後に、しきい値処理でエッジとして扱うかどうかを決めます。 Canny法のしきい値は2つあり、1つはこの値より大きければエッジとすると決めるためのもの、もう1つはこの値よりも小さければエッジではないと決めるためのものです。 じゃあ2つのしきい値の間はどうなるの？という話ですが、隣接しているピクセルがエッジと判定されていれば、エッジと判定するようにし、そうでなければエッジではないと判定します。 単純なしきい値でのエッジの判定よりも、より柔軟ですね。\nただし、しきい値が非常に重要になることが容易に想像できます。\nOpenCVでCanny法をためす Canny法は以下のようにして実行できます。\ncanny = cv2.Canny(img, threshold1=10, threshold2=50, apertureSize=3, L2gradient=True) threshold1がしきい値の小さい方で、threshold2がしきい値の大きい方です。apertureSizeにSobelフィルタのサイズを指定しています。また勾配の大きさに2ノルムを使う場合にはL2gradientをTrueにします。\n結果を以下に示します。\n   元画像 canny（2ノルム） canny（1ノルム）           2ノルムのほうがきれいにエッジが取れている気がします。\n","date":"2020-08-03T20:17:14+09:00","image":"https://opqrstuvcut.github.io/blog/posts/canny%E6%B3%95%E3%81%A7%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA/5154e105157ba91bf3bf7306d1cfffe1_hud15ff8e0698fd4e8689d5fe3f1bbf41c_1046193_120x120_fill_q75_box_smart1.jpg","permalink":"https://opqrstuvcut.github.io/blog/posts/canny%E6%B3%95%E3%81%A7%E3%82%A8%E3%83%83%E3%82%B8%E6%A4%9C%E5%87%BA/","title":"Canny法でエッジ検出"},{"content":"本記事はQrunchからの転載です。\n 今日はヒストグラム平坦化を扱います。\nヒストグラム平坦化はコントラストが偏っているような画像を補正します。 結果として、コントラストがある程度平坦化された結果が得られます。\n処理の中身としては、実際には画像のピクセル値の累積分布関数で写像したうえで、最大値と最小値が広がるように調整してあげるというイメージです。\nOpenCVでヒストグラム平坦化 次の画像にヒストグラム平坦化を適用してみます。このままだと全くみえません。\nこの画像の画素値のヒストグラムは以下のとおりです。だいぶ偏ってますね。\nヒストグラム平坦化は次のようにしておこなえます。めちゃくちゃ簡単です。\nres = cv2.equalizeHist(img) ちゃんと見えるようになりましたね。\nこの画像の画素値のヒストグラムは以下のとおりです。\n","date":"2020-08-02T22:50:29+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E5%B9%B3%E5%9D%A6%E5%8C%96/e534d7e8e9969a0405e4a1b8f7eea112_hua70cd25a91f361232721d1da9fb274b8_53437_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E3%83%92%E3%82%B9%E3%83%88%E3%82%B0%E3%83%A9%E3%83%A0%E5%B9%B3%E5%9D%A6%E5%8C%96/","title":"ヒストグラム平坦化"},{"content":"本記事はQrunchからの転載です。\n Non-Local Means Denoisingのアイデア 今回はノイズ除去を扱うのですが、特にガウスノイズを考えます。 これは平均が0となるノイズですので、着目しているピクセルにある意味で似ているピクセルを画像中から探してきて、それらの平均を取れば、ノイズの影響が消えたピクセルが得られるはずです。 これがNon-Local Means Denoisingのアイデアになります。\n似ているピクセルをどう定義するか Non-Local Means Denoisingでは着目しているピクセルの値自体ではなく、着目しているピクセルの周辺の値同士の差分を取ることで、似ているかどうかを考えます。 この考えから定義されるピクセル$p$と$q$間の距離は以下のようになります。 $$ d^2(B(p, f), B(q,f)) = \\frac{1}{3(2f + 1)^2} \\sum_{c=1}^3 \\sum_{j \\in B(0, f)} (I_c(p+j) - I_c(q+j))^2. $$ ここで$B(p,f)$は着目しているピクセル$p$のサイズの周辺のピクセルで、サイズが$(2f + 1) \\times (2f + 1)$となっています。$I_c(p+j)$が周辺ピクセルの$c$番目のchannelの値をあらわします。\n平均値の取り方 \u0008先程定義した距離を使って以下のような重みを計算します。 $$ w(p,q) = e^{-\\max(d^2 - 2\\sigma^2, 0) / h^2}. $$ $\\sigma^2$はノイズの分散になります（OpenCVの関数で実行するときには特にこれを指定しないので、上手く処理されている？）。$h$は与えるパラメーターで、大きいほど$w$の値に差がつきづらくなります。 距離$d^2$が小さいと$w$が1に近い値を取り、$d^2$が大きいほど$w$は小さい値になります。 この$w$を重みとしたピクセル値の重み付き平均を取ることがNon-Local Means Denoisingでの処理になります。\nこの重み付き平均をとることで、似ているピクセルは強く考慮されますが、似ていないピクセルはほとんど影響を与えないため、似ているピクセルだけでの平均が取れるような計算処理になっています。\nなお、すべてのピクセル同士で距離$d^2$を計算すると、当然計算量が大変なことになります。 このため、実際には着目しているピクセルの周辺のどこまでを考慮するかを指定します。\nOpenCVでやってみる OpenCVでNon-Local Means Denoisingをやってみます。\n次の左の画像にノイズをのせて右の画像を生成しました。 これに対して次のようにして、Non-Local Means Denoisingを適用します。\ndenoised = cv2.fastNlMeansDenoisingColored(img, h=3, templateWindowSize=7, searchWindowSize=21) hはさきほどの重みで出てきた$h$と同じで、templateWindowSizeは$d^2$の計算で使われる$f$と同じで、searchWindowSizeは着目しているピクセルの周辺をどこまで考慮するかをあらわします。 ちなみに、fastNlMeansDenoisingという関数もありますが、カラー画像に対してはfastNlMeansDenoisingColoredが良いらしいです。\n結果が以下のとおりです。\n　ノイズをのせた画像　ノイズ除去後の画像 体の部分がぼやけていますが、ノイズはきれいにとれてますね。\n","date":"2020-08-01T10:04:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/non-local-means-denoising%E3%81%A7%E3%83%8E%E3%82%A4%E3%82%BA%E9%99%A4%E5%8E%BB/783215bfb0d1c75f10337b84d03a1a34_hubead929561f1b0e7093fb3a31d66a0bf_39514_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/non-local-means-denoising%E3%81%A7%E3%83%8E%E3%82%A4%E3%82%BA%E9%99%A4%E5%8E%BB/","title":"Non-Local Means Denoisingでノイズ除去"},{"content":"本記事はQrunchからの転載です。\n 画像に汚れがついたり、傷がついているケースの修復には、最近ではディープラーニングを使った手法が色々出ていますが、画像処理の範囲でもできることがあります。 今回はOpenCVで修復をおこなってみます。\nOpenCVでやってみる 次の画像にノイズをのせていきます。 次のようなコードで画像にノイズをのせていきます。\ncv2.rectangle(img, (100,100),(300,105),(255,255,255), -1) cv2.rectangle(img, (400, 450),(600,460),(255,255,255), -1) cv2.rectangle(img, (0, 750),(800, 760),(255,255,255), -1) plt.imshow(img[:, :, ::-1]) plt.show() mask = np.zeros(img.shape[:2], dtype=np.uint8) cv2.rectangle(mask, (100,100),(300, 105),(255), -1) cv2.rectangle(mask, (400, 450),(600,460),(255), -1) cv2.rectangle(mask, (0, 750),(800, 760),(255), -1) plt.imshow(mask) plt.gray() plt.show() 　ノイズがのった画像　ノイズ部分のmask画像 OpenCVのinpaint関数を使うと、このノイズがのった画像をある程度復元できます。 次のように利用します。\ninpainted = cv2.inpaint(img, mask, 3, cv2.INPAINT_NS) 第一引数に復元したい画像を指定し、第二引数に復元したい箇所をあらわしたマスク画像を指定します。第三引数が復元時に周辺のピクセルをいくつ利用するかを指定します。第四引数に復元のアルゴリズムを指定します。INPAINT_NS（Navier Stokes法）かINPAINT_TELEA（Alexandru Telea法）を指定できます。\n　Navier Stokes法　Alexandru Telea法 どちらも結構いい感じに復元できています。右図のほうが文字の部分などはきれいに復元できている気がします。\nちなみにAlexandru Telea法で第三引数を10まで大きくしてみると、以下のようになります。 ちょっと復元した箇所が滲んだような感じになってしまってます。大きくしすぎには注意ですね。\n","date":"2020-07-31T11:04:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/inpaint%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE%E4%BF%AE%E5%BE%A9%E3%82%92%E3%81%99%E3%82%8B/6cfb060c01c9e2a5a569b5c14ee05620_hu7a2287441c4c8a30607a7f2d938c96e2_46674_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/inpaint%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE%E4%BF%AE%E5%BE%A9%E3%82%92%E3%81%99%E3%82%8B/","title":"inpaintで画像の修復をする"},{"content":"本記事はQrunchからの転載です。\n 透過変換とは？ 透過変換はアフィン変換よりも柔軟な変換になっていまして、アフィン変換ではできない台形への変換が可能です。また台形から長方形への変換も可能です。 つまり、斜めに写っているものを上から見たような感じに変換ができるというわけです。\nOpenCVでやってみる 次の画像を長方形の画像に変換することを考えます。 やりたいこととしてはこの本が斜めに（台形に）写っているので、これを長方形にすることです。\nまず変換行列を作る必要があります。 これには次のようにgetPerspectiveTransformを使えば簡単にできます。\nsrc = np.array([[830, 675], [26, 2872], [2579, 2852], [2350, 455]], dtype=np.float32) dst = np.array([[0, 0], [0, 1150], [800, 1150], [800, 0]], dtype=np.float32) perspective_mat = cv2.getPerspectiveTransform(src, dst) これはsrcで指定した4つの座標がdstで指定した4つの座標に変換されるような変換行列を作ってくださいと関数に依頼しています。 srcで指定している4点は本の4隅の座標です。dstの1150と800という数値は実際の本の縦横比から適当に決めました。\nこの行列を使い、次のように変換をおこないます。\ntransformed = cv2.warpPerspective(img, perspective_mat, (800, 1150)) plt.imshow(transformed[:, :, ::-1]) plt.show() それっぽく長方形になりました。 ちょっと文字などが斜めになっていますが、本の表紙が浮いているせいかもしれません。\n","date":"2020-07-30T11:04:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E9%80%8F%E9%81%8E%E5%A4%89%E6%8F%9B%E3%81%A7%E6%96%9C%E3%82%81%E3%81%8B%E3%82%89%E6%92%AE%E3%81%A3%E3%81%9F%E7%94%BB%E5%83%8F%E3%82%92%E4%B8%8A%E3%81%8B%E3%82%89%E8%A6%8B%E4%B8%8B%E3%82%8D%E3%81%99/80ab2bb0c531b5c091411d41a9dd10a4_hucecd12b8b8b485b4272c30076c10b4f8_45441_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E9%80%8F%E9%81%8E%E5%A4%89%E6%8F%9B%E3%81%A7%E6%96%9C%E3%82%81%E3%81%8B%E3%82%89%E6%92%AE%E3%81%A3%E3%81%9F%E7%94%BB%E5%83%8F%E3%82%92%E4%B8%8A%E3%81%8B%E3%82%89%E8%A6%8B%E4%B8%8B%E3%82%8D%E3%81%99/","title":"透過変換で斜めから撮った画像を上から見下ろす"},{"content":"本記事はQrunchからの転載です。\n アフィン変換といえば、普通は2次元上の点や図形を拡大縮小したり、回転したり、平行移動したりといった変換をさします。 式の話をすると、ある2次元上の点$(x,y)$の$(x', y')$へのアフィン変換は次のようにして表現できます。 $$\\begin{pmatrix}x' \\\\ y' \\\\ 1 \\end{pmatrix} =\\begin{pmatrix} a \u0026amp; b \u0026amp; c\\\\ e \u0026amp; f \u0026amp; g \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\\\ 1 \\end{pmatrix}. $$ $a,b,e,f$の値によって拡大縮小、回転をおこなうようにできますし、$c,g$の値によって平行移動が可能です。\n今回はこのアフィン変換をOpenCVを使っておこないます。\nアフィン変換のやり方 OpenCVでは次のようにしてアフィン変換をおこないます。\ntransformed_img = cv2.warpAffine(img, affine_mat, (width, height)) affine_matとしているのが、アフィン変換で用いる行列です。 widthとheightは変換後の画像のサイズになります。\n以下では次の画像に対するアフィン変換の例を示します。 平行移動 平行移動をするときは次のようなアフィン変換になります。 $$\\begin{pmatrix}x' \\\\ y' \\\\ 1 \\end{pmatrix} =\\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; c\\\\ 0 \u0026amp; 1 \u0026amp; g \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\\\ 1 \\end{pmatrix}. $$ もう少し式を書きくだせば、 $$\\begin{eqnarray}x' \u0026amp;=\u0026amp; x + c, \\\\y'\u0026amp;=\u0026amp;y+g\\end{eqnarray}$$ となるので、平行移動だとわかりますね。\n$x$と$y$を1000ずつ動かすとすると、コードでは次のようになります。\naffine_mat = np.array([[1, 0, 1000], [0, 1, 1000]], dtype=np.float) transformed_img = cv2.warpAffine(img, affine_mat, (width, height)) 拡大・縮小 拡大・縮小をするときは次のようなアフィン変換になります。 $$\\begin{pmatrix}x' \\\\ y' \\\\ 1 \\end{pmatrix} =\\begin{pmatrix} a \u0026amp; 0 \u0026amp; 0\\\\ 0 \u0026amp; e \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\\\ 1 \\end{pmatrix}. $$ もう少し式を書きくだせば、 $$\\begin{eqnarray}x' \u0026amp;=\u0026amp; ax \\\\y'\u0026amp;=\u0026amp;ey\\end{eqnarray}$$ となるので、縮小だとわかりますね。\n大きさを半分にする場合は次のようなコードになります。\naffine_mat = np.array([[.5, 0, 0], [0, .5, 0]], dtype=np.float) transformed_img = cv2.warpAffine(img, affine_mat, (width, height)) 回転 $\\theta$だけ回転をするときは次のようなアフィン変換になります。 $$\\begin{pmatrix}x' \\\\ y' \\\\ 1 \\end{pmatrix} =\\begin{pmatrix}\\cos\\theta \u0026amp; -\\sin \\theta \u0026amp; 0\\\\ \\sin \\theta \u0026amp; \\cos \\theta \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} \\begin{pmatrix}x \\\\ y \\\\ 1 \\end{pmatrix}. $$ これでなぜ回転になるかわからない方は回転行列などでぐぐってください。 45度時計回りに回転させるときのコードは以下のようになります。\naffine_mat = np.array([[1 / 1.4142, -1 / 1.4142, 0], [1 / 1.4142, 1 / 1.4142, 0]], dtype=np.float) transformed_img = cv2.warpAffine(img, affine_mat, (width, height)) ","date":"2020-07-29T11:03:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%B8%E3%81%AE%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B/81a46863d26154ebd07353949e4d8171_hu476832c9551a683b8f4490b96b748859_34786_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%B8%E3%81%AE%E3%82%A2%E3%83%95%E3%82%A3%E3%83%B3%E5%A4%89%E6%8F%9B/","title":"画像へのアフィン変換"},{"content":"本記事はQrunchからの転載です。\n OpenCVでの動画の書き込み方 次のようにしてtest.mp4という名前の動画を作成します。\nfourcc = cv2.VideoWriter_fourcc(\u0026#34;m\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;4\u0026#34;, \u0026#34;v\u0026#34;) writer = cv2.VideoWriter(\u0026#34;test.mp4\u0026#34;, fourcc, 30, (1920, 1080)) print(writer.isOpened()) 第二引数のfourccは動画のコーデックをあらわしており、mp4のときにはcv2.VideoWriter_fourccの引数には\u0026quot;m\u0026quot;, \u0026ldquo;p\u0026rdquo;, \u0026ldquo;4\u0026rdquo;, \u0026ldquo;v\u0026quot;を指定します。他にもmpgで保存するときには\u0026quot;D\u0026rdquo;, \u0026ldquo;I\u0026rdquo;, \u0026ldquo;V\u0026rdquo;, \u0026ldquo;X\u0026quot;を指定したりできます。拡張子に対応してどういうコーデックが指定できるかは、ググっていただくのが良いかと思います。 また、第三引数にFPSを第四引数に動画の横と縦の大きさを指定しています。 isOpenedメソッドにより動画を書き込むための準備ができているかを確認できます。FalseのときにはPCがコーデックに対応していなかったりで上手くいっていません。\n実際に書き込みをおこなうときはwriteメソッドを使います。以下では3フレーム分書き込んでいます。\nwriter.write(frame) writer.write(frame) writer.write(frame) writer.release() # 書き込み後はreleaseする ","date":"2020-07-28T11:00:00+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/%E5%8B%95%E7%94%BB%E3%81%AE%E6%9B%B8%E3%81%8D%E8%BE%BC%E3%81%BF/","title":"動画の書き込み"},{"content":"本記事はQrunchからの転載です。\n 今日はOpenCVでの動画の読み書きを扱います。\n動画の読み込み 動画の読み込みは簡単です。\n最初に次のように保存されている動画を開きます。\nimport cv2 v = cv2.VideoCapture(\u0026#34;./ex.mp4\u0026#34;) カメラからフレームを取得する場合はデバイスのIDの指定すればよいです。 普通は次のように0を指定すればよいかと思います。\nv = cv2.VideoCapture(0) フレームの読み込みは以下のようにします。\nret, frame = v.read() フレームが読み込めれば、retにはTrueが入ってきて、フレームが読み込めない状態になるとFalseが入ります。 これを利用すれば、次のようにしてフレームを次々に読み込めます（保存されているファイルを開いている場合には、動画の終わりまでが読み込まれます）。\nwhile True: ret, frame = v.read() if not ret: break プロパティの取得 動画のフレームの大きさ、FPS、フレーム数は以下のようにして取得できます。\nprint(v.get(cv2.CAP_PROP_FRAME_WIDTH)) print(v.get(cv2.CAP_PROP_FRAME_HEIGHT)) print(v.get(cv2.CAP_PROP_FPS)) print(v.get(cv2.CAP_PROP_FRAME_COUNT)) 取得できるプロパティは以下に一覧があります。 http://opencv.jp/opencv-2svn/cpp/highgui_reading_and_writing_images_and_video.html\n","date":"2020-07-27T22:53:54+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/%E5%8B%95%E7%94%BB%E3%81%AE%E8%AA%AD%E3%81%BF%E3%81%93%E3%81%BF/","title":"動画の読みこみ"},{"content":"本記事はQrunchからの転載です。\n OpenCVのfilter2Dを使うのは良いのですが、分離可能フィルタのときにはsepFilter2Dを使うことで、高速化できます。 今回はこのsepFilter2Dを扱います。\n分離可能フィルタ 分離可能フィルタとは2つのベクトルの畳み込みであらわされるフィルタのことを指します。\n分離可能フィルタの具体例1 Sobelフィルタは分離可能フィルタです。 X方向のSobelフィルタは以下であらわされます。 これは次のような2つのベクトルの畳み込みとしてあらわされます（$\\ast$は畳み込みをあらわしています）。 $$ \\begin{pmatrix} 1 \\\\ 2 \\\\ 1 \\end{pmatrix} \\ast\t\\begin{pmatrix} -1 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix}. $$\n分離可能フィルタの具体例2 平滑化フィルタは分離可能フィルタです。 3×3のサイズの平滑化フィルタは次のような2つのベクトルの畳み込みとしてあらわされます。 $$ \\begin{pmatrix} \\frac{1}{3} \\\\ \\frac{1}{3} \\\\ \\frac{1}{3} \\end{pmatrix} \\ast\t\\begin{pmatrix} \\frac{1}{3} \u0026amp; \\frac{1}{3} \u0026amp; \\frac{1}{3} \\end{pmatrix}. $$\n分離可能フィルタで高速化できる理由 行列形式のサイズ$n$のカーネルを使う場合には1回の畳み込み演算に$n^2$のオーダーの計算量が必要です（実際、掛け算は$n^2$回、足し算は$n^2-1$回です）。これを画像のピクセルの数$S$だけおこなうとすると、$n^2S$のオーダーの計算量がかかります。\n次に分離した2つのベクトルであらわされたカーネルを2回適用するケースを考えます。このカーネルの1回の畳み込みには$n$のオーダーの計算量がかかります。これをすべてのピクセルに2回適用すると、計算量のオーダーは$2nS$です。\n以上から$n$が大きくなると、計算量に大きな違いがでることがわかります。\n実際にsepFilter2Dを試す sepFilter2Dは以下のようにして利用できます。\nsep_filter_res = cv2.sepFilter2D(img, ddepth=cv2.CV_16S, kernelY=col_kernel, kernelX=row_kernel) kernelXに行ベクトルのカーネルを指定し、kernelYに列ベクトルのカーネルを指定しています。\n実際にSobelフィルタを適用することを考えます。\ncol_kernel = np.array([1, 2, 1]).T row_kernel = np.array([-1, 0, 1]) sep_filter_res = cv2.sepFilter2D(img, ddepth=cv2.CV_16U, kernelY=col_kernel, kernelX=row_kernel) plt.imshow(sep_filter_res) plt.gray() plt.show() 結果自体は次のようにfilter2Dを使ったSobelフィルタと等しいです。\nkernel = np.array([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]) filter_res = cv2.filter2D(img, ddepth=cv2.CV_16S, kernel=kernel) print((sep_filter_res - filter_res).sum()) # output: 0 速度比較 上記のSobelフィルタを適用したときの速度を、Jupyter notebookの%%timeitを使って比較してみます。 ついでにcv2.Sobelも比較してみます。\n   関数 timeitの結果     sepFilter2D 240 µs ± 12.3 µs per loop   filter2D 523 µs ± 18.9 µs per loop   cv2.Sobel 231 µs ± 12 µs per loop    $n=3$でこれだけ差がつきました。$n$が大きいときにはsepFilter2Dを使うといいですね。 当たり前ですが、用意されているSobelフィルタはsepFilter2Dと同程度の速さです。\n","date":"2020-07-26T11:06:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/sepfilter2d%E3%81%A7%E5%88%86%E9%9B%A2%E5%8F%AF%E8%83%BD%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E9%AB%98%E9%80%9F%E5%8C%96/7f919cf47778b5338c27d63bca691006_hu8aafcebbe0a95b3ee0e4bcd249ac24b6_23002_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/sepfilter2d%E3%81%A7%E5%88%86%E9%9B%A2%E5%8F%AF%E8%83%BD%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%82%92%E4%BD%BF%E3%81%A3%E3%81%A6%E9%AB%98%E9%80%9F%E5%8C%96/","title":"sepFilter2Dで分離可能フィルタを使って高速化"},{"content":"本記事はQrunchからの転載です。\n OpenCVではいろいろなカーネルによる演算が用意されていますが、自分で定義したカーネルを使いたいこともあります。 そんなときにはfilter2Dが活躍します。\nfilter2Dの使い方 filter2Dのシンプルな利用例としては次のようになります。\nres = cv2.filter2D(img, ddepth=cv2.CV_8U, kernel=kernel) ddepthに返り値の型を指定します。ここでは符号なしの8ビット整数を指定しています。 kernelに自分で定義したカーネルを指定します。\nfilter2Dを使ってみる 次の画像にfilter2Dを使った平滑化を適用してみます。 ksize = 11 kernel = np.ones([ksize, ksize]) / (ksize ** 2) res = cv2.filter2D(img, ddepth=cv2.CV_16U, kernel=kernel) plt.imshow(res) plt.gray() plt.show() 一応、cv2.blurと等しいかを調べてみます。 次のようにすると等しい結果になったかが分かります。\nblur = cv2.blur(img, ksize=(ksize, ksize)) print((blur - res).sum()) # output: 0 ","date":"2020-07-25T12:12:06+09:00","image":"https://opqrstuvcut.github.io/blog/posts/filter2d%E3%81%A7%E4%BB%BB%E6%84%8F%E3%81%AE%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86/99878fdaca9a827d2ba5359aa4dcade7_hu54f3afb52e74754110cb87d28dd75f72_27333_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/filter2d%E3%81%A7%E4%BB%BB%E6%84%8F%E3%81%AE%E3%82%AB%E3%83%BC%E3%83%8D%E3%83%AB%E3%82%92%E6%89%B1%E3%81%86/","title":"filter2Dで任意のカーネルを扱う"},{"content":"本記事はQrunchからの転載です。\n 画像に対する膨張と収縮の組み合わせによって、openingとclosingという2つの操作が実現できます。 openingは周辺よりもピクセル値が大きい点を取り除くことができ、closingは周辺よりもピクセル値が小さい点を取り除くことができます。これによってノイズの除去や連結した領域を分割したり、逆に連結させたりできます。\nopening openingは収縮(erode)の後に膨張(dilate)をおこなうことで実現できます。 例えば次のような画像を考えます。\nnp.random.seed(0) A = (np.random.rand(15, 15) \u0026gt; 0.3) * 255 A = A.astype(np.uint8) これの画像に対して、次のようにopeningの操作をおこないます。\nkernel = np.ones([2, 2], np.uint8) erosion = cv2.erode(A, kernel, iterations=1) dilation = cv2.dilate(erosion, kernel, iterations=1) plt.imshow(dilation) plt.gray() plt.show() 周辺よりもピクセル値が大きい点を取り除けていることが分かるでしょうか。\nちなみに次のようにしてもopeningをおこなえます。結果は上記と全く同じになります。\nopening = cv2.morphologyEx(A, cv2.MORPH_OPEN, kernel) plt.imshow(opening) plt.gray() plt.show() closing closingは膨張(dilate)の後に収縮(erode)をおこなうことで実現できます。 例えば次のような画像を考えます。\nnp.random.seed(0) A = (np.random.rand(15, 15) \u0026gt; 0.7) * 255 A = A.astype(np.uint8) この画像に対して、次のようにopeningの操作をおこないます。\nkernel = np.ones([2, 2], np.uint8) dilation = cv2.dilate(A, kernel, iterations=1) erosion = cv2.erode(dilation, kernel, iterations=1) plt.imshow(erosion) plt.gray() plt.show() openingと同様に次のようにしてもclosingをおこなえます。結果は上記と全く同じになります。\nclosing = cv2.morphologyEx(A, cv2.MORPH_CLOSE, kernel) plt.imshow(closing) plt.gray() plt.show() ","date":"2020-07-21T22:31:32+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E8%86%A8%E5%BC%B5%E3%81%A8%E5%8F%8E%E7%B8%AE%E3%81%AE%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%81%AB%E3%82%88%E3%82%8Bopening%E3%81%A8closing/72c323555493bf03f85379bed4fdc20e_hu4ec129273aaaf1812f4bdcd1dd59883c_3678_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E8%86%A8%E5%BC%B5%E3%81%A8%E5%8F%8E%E7%B8%AE%E3%81%AE%E7%B5%84%E3%81%BF%E5%90%88%E3%82%8F%E3%81%9B%E3%81%AB%E3%82%88%E3%82%8Bopening%E3%81%A8closing/","title":"膨張と収縮の組み合わせによるopeningとclosing"},{"content":"本記事はQrunchからの転載です。\n erodeによる収縮 erodeは指定した局所領域内の最小値を取るような操作になります。\n具体的な例で説明していきます。 次のようなピクセル値をもった3×3の画像があったとします。\nerodeの処理で2×2の局所領域を指定すると、次のような手順で計算がおこなわれていきます。 オレンジ色の枠が注目している局所領域になります。 まず、次のように最初の局所領域に左上のピクセルしか含まれていないので、この局所領域の最小値は1として扱います。\n次に局所領域を右にスライドさせると、今度は1と2が局所領域に含まれますので、この局所領域の最小値は1となります。 次の局所領域では最小値は2です。 局所領域を下の段に下げていき、上記の操作を続けていくと以下のような画像を得られます。 OpenCVでerode OpenCVでのerodeは次のようにおこないます。\nkernel = np.ones([5, 5], np.uint8) erosion = cv2.erode(img, kernel, iterations=1) kernelが局所領域をあらわし、iterationsは収縮の操作を何度おこなうかをあらわします。\n先程の例に適用 先程の例の画像でerodeを試してみましょう。\nA = np.array([[1, 2, 4], [0, 2, 3], [1, 4, 2]], dtype=np.uint8) とし、以下を実行します。\nkernel = np.ones([2, 2], np.uint8) erosion = cv2.erode(A, kernel, iterations=1) erosionの値は以下のとおりです。さきほどの計算例と一致するのがわかります。\narray([[1, 1, 2], [0, 0, 2], [0, 0, 2]], dtype=uint8) kernelを変わり種にする kernelの値を1つだけ0にして、局所領域に含めないようにしてみます。具体的には以下のようにします。\nkernel = np.ones([2, 2], np.uint8) kernel[0, 0] = 0 erosion = cv2.erode(A, kernel, iterations=1) この結果は以下のとおりです。\narray([[1, 1, 2], [0, 0, 2], [0, 1, 2]], dtype=uint8) 局所領域をいじったため、$(3,2)$の要素が$1$になりました。\n画像へ適用 以下の画像に5×5の局所領域でのerodeを適用してみます。 　iterations=1　iterations=2 文字の部分をみるとわかりやすいですが、だいぶ小さくなっています。 目の部分はピクセル値が小さいので、ここは逆に膨張しています。\n","date":"2020-07-20T23:14:16+09:00","image":"https://opqrstuvcut.github.io/blog/posts/erode%E3%81%A7%E7%8C%AB%E3%82%92%E5%8F%8E%E7%B8%AE%E3%81%95%E3%81%9B%E3%82%8B/fa45ab8993211945b777cb332c493e6a_hud2e86eaec3a26b71b577c25f3c91797a_28308_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/erode%E3%81%A7%E7%8C%AB%E3%82%92%E5%8F%8E%E7%B8%AE%E3%81%95%E3%81%9B%E3%82%8B/","title":"erodeで猫を収縮させる"},{"content":"本記事はQrunchからの転載です。\n OpenCVで用意されているdilateを使うことで、画像の中の物体などを膨張させることができます。 ただ膨張させるだけだとあまり使いみちがあるのかよく分かりませんが、収縮などと組み合わせることで色々な用途があります。\ndilateについて dilateは指定された局所領域の中で最大値のピクセル値に置き換えていくような処理になります。 このため、例えば背景よりも物体のほうがピクセル値が大きければ、その物体の端の部分が膨らんでいくような処理がおこなわれます。\ndilateは次のようにして利用します。\nkernel = np.ones((5,5),np.uint8) dilation = cv2.dilate(img, kernel, iterations=1) ここでkernelは局所領域をあらわしており、5×5の局所領域がdilateに利用されています。 また、iterationsは何回同様の処理をおこなうかをあらわします。複数回実行することで、より膨張を促すことができます。\n実際に試した結果が以下のとおりです。\n　元画像 iterations=1　iterations=2 猫が太っていっているのがわかるでしょうか？文字のほうがわかりやすいかもしれませんが。 iterations=2のときのほうが1のときよりも膨張していることがわかります。\n","date":"2020-07-19T20:40:11+09:00","image":"https://opqrstuvcut.github.io/blog/posts/dilate%E3%81%A7%E7%8C%AB%E3%82%92%E8%86%A8%E5%BC%B5%E3%81%95%E3%81%9B%E3%82%8B/28690ce14686ae4e18da248ce434c3cc_hub4b0febc1f21980679e4fd191e7dadd2_27472_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/dilate%E3%81%A7%E7%8C%AB%E3%82%92%E8%86%A8%E5%BC%B5%E3%81%95%E3%81%9B%E3%82%8B/","title":"dilateで猫を膨張させる"},{"content":"本記事はQrunchからの転載です。\n 今回はLaplacianを扱います。\nそもそものLaplacian Laplacianの復習的な話ですが、2階偏微分可能な関数$f(x,y)$に対して以下をLaplacianといいます。 $$ \\Delta f = \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2}. $$\nこれを画像に適用することで、ピクセル値の極小値あるいは極大値となるピクセルを見つけることが可能になります。これはエッジ検出に利用可能だということがわかるかと思います。\nLaplacianのフィルタ Laplacianのフィルタの最も基本的なものは以下で定義されます。 これを使った畳み込み演算によってLaplacianができるという主張ですが、このフィルタの導出は以下のとおりです。\n$(x,y)$の位置にあるピクセルの1階の偏微分の近似は以下のようにあらわされます。\n$$ \\frac{\\partial f}{\\partial x} \\approx f(x + 1, y) - f(x,y ).$$ これを利用すると、2階の偏微分は\n$$\\begin{aligned} \\frac{\\partial^2 f}{\\partial x^2} \u0026amp;\\approx\u0026amp; f(x+1,y ) - f(x, y) - (f(x,y ) - f(x-1,y )) \\\\ \u0026amp;=\u0026amp; f(x+1, y) - 2f(x, y) + f(x-1,y ).\\end{aligned}$$ 同様に $$ \\begin{aligned} \\frac{\\partial^2 f}{\\partial y^2} \u0026amp;\\approx\u0026amp; f(x,y+1) - f(x, y) - (f(x,y ) - f(x,y-1 )) \\\\ \u0026amp;=\u0026amp; f(x, y+1) - 2f(x, y) + f(x,y-1 ). \\end{aligned}$$ よって、 $$ \\begin{aligned} \\Delta f \u0026amp;= \\frac{\\partial^2 f}{\\partial x^2} + \\frac{\\partial^2 f}{\\partial y^2} \\\\ \u0026amp;= f(x+1,y ) - 2f(x,y ) + f(x-1,y ) + f(x,y+1 ) - 2f(x,y ) + f(x,y -1)\\\\ \u0026amp;= f(x+1,y ) + f(x-1,y ) + f(x,y+1 ) + f(x,y -1) - 4f(x,y ) . \\end{aligned} $$ 以上から先程のようなフィルタになることが理解できたでしょうか？\nこのフィルタを使うと、ノイズにも強く反応することが予想されます。 そのため、場合によっては平滑化によりノイズを軽減する必要です。\n斜めも考慮したLaplacian また、斜め方向も考慮したlaplacianのフィルタが以下のようになります。 （これって導出あるんでしょうか？）\nOpenCVでLaplacianを試す OpenCVでは以下のように利用できます。\nlaplacian = cv2.Laplacian(img, ddepth=cv2.CV_16S, ksize=1) ddepthに出力されるlaplacianの型を指定します。CV_16Sは符号付きの16ビット整数です。 ksizeにフィルタ（カーネル）の大きさを指定します。最初に示したフィルタを使うためにはksize=1を指定します。2つ目に示したフィルタはksize=3になります。\nksizeを変えて2パターン実行した結果が以下のとおりです。 描画の都合上、laplacianに絶対値を取ったものを表示しています。\nksize=1のとき ksize=3のとき ","date":"2020-07-18T13:05:29+09:00","image":"https://opqrstuvcut.github.io/blog/posts/laplacian%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE2%E9%9A%8E%E5%BE%AE%E5%88%86/87a340899433b8d46b1c936e1a54fad5_huc804305ef286bf71ff0865d646e67a9c_23184_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/laplacian%E3%81%A7%E7%94%BB%E5%83%8F%E3%81%AE2%E9%9A%8E%E5%BE%AE%E5%88%86/","title":"Laplacianで画像の2階微分"},{"content":"本記事はQrunchからの転載です。\n よくある画像処理のオペレーターとして、画像の微分があります。 いくつかやり方はありますが、今日はSobel微分を取り上げます。\nSobelフィルタ Sobel微分はSobelフィルタを使った畳み込みをすることで実現できます。 例えば、3×3のSobelフィルタは以下のようなカーネルになります。\nx方向の微分用のSobelフィルタy方向の微分用のSobelフィルタこれらのフィルタは何をあらわしているんでしょうか？ 実はSobelフィルタは微分と平滑化をあわせもったフィルタになっています。 ここでいう微分のフィルタとはx方向の場合には以下を指します。 これは$(x,y)$座標のピクセルに注目しているときに、その左右にあるピクセルの差を取る演算を示しています。いわゆる中心差分と呼ばれる微分の計算方法になります。\n次に平滑化ですが、これは以下のフィルタです。 ガウス平滑化に似たように中心の重みが大きい平滑化になります。\nここまでで定義した微分のフィルタに対して平滑化のフィルタによる畳込みを計算すると、実はSobelフィルタと同じものがあらわれます。つまり、画像に対して微分のフィルタを適用した後に平滑化のフィルタを適用することとと、画像に対してSobelフィルタを適用することは等しいです。\n以上がSobelフィルタが何をしているかの話になります。\nSobelフィルタを適用 OpenCVでは以下のようにすることで、Sobelフィルタを適用できます。\nsoblex = cv2.Sobel(img, ddepth=cv2.CV_16S, dx=1, dy=0, ksize=3) 第二引数のddepthにSobelによる返り値を格納する型を指定します。CV_16Sは符号付きの16ビット整数です。 第三、第四引数のところは微分する次数を指定します。dx=1、dy=0とすると、x方向のSobelフィルタを使うことになりますし、dx=0、dy=1とするとy方向のSobelフィルタです。 最後のksizeはカーネルサイズになります。一応31まで指定が可能なようです。\n次の画像にSobelフィルタを適用してみます。 x方向のSobelフィルタの適用\nsoblex = cv2.Sobel(noise_img, ddepth=cv2.CV_16S, dx=1, dy=0, ksize=3, ) 正の勾配は白色、負の勾配は黒色で描画されています。\ny方向のSobelフィルタの適用\nsobley = cv2.Sobel(noise_img, ddepth=cv2.CV_16S, dx=0, dy=1, ksize=3, ) これも同様に正の勾配は白色、負の勾配は黒色で描画されています。\nなお、それぞれのSobelフィルタの適用結果を足し合わせると次のようになります。 ","date":"2020-07-16T14:06:05+09:00","image":"https://opqrstuvcut.github.io/blog/posts/sobel%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%A7%E5%BE%AE%E5%88%86/d0092a9ae18af82d66d3a2d25fd7b5b8_hu25a8a6e540551f3949649296f27dbcc8_30368_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/sobel%E3%83%95%E3%82%A3%E3%83%AB%E3%82%BF%E3%81%A7%E5%BE%AE%E5%88%86/","title":"Sobelフィルタで微分"},{"content":"本記事はQrunchからの転載です。\n 画像をpngなどからjpgに変換したいときに、ぱっと思いつくのはファイルを読み込んで、それをjpgの拡張子で書き込みした後に再度読み込みなおすことです。 1度動かすならばそれでも良いのですが、何度も繰り返しおこなう場合にはファイルの読み書きの時間が気になります。\nOpenCVではファイルへの読み書きをおこなうことなく、メモリ上でファイル形式を変更できる（jpgへの圧縮などができる）ような方法が提供されています。\n流れとしては、imencodeでメモリ上にファイル形式を変更したバイト列を作成し、それをimdecodeで画像に変換するという流れになります。imencodeがファイルへの書き込み、imdecodeがファイルの読み込みに対応する感じになります。\nimencode 画像を他のファイルを形式に変更するimencodeは次のようにして利用します。\nret, encoded = cv2.imencode(\u0026#34;.jpg\u0026#34;, img, (cv2.IMWRITE_JPEG_QUALITY, 10)) 1つめの引数がどの拡張子に変換するかをあらわす文字列で、ここではjpgを指定しています。\n3つめの引数に指定した拡張子に変換するときのパラメータを指定します。 例えばjpgの場合には画像の質を指定できますので、それをタプルの形式で与えており、ここではjpgの質を10で圧縮するようにしています。\nimencodeによって生成されたjpgになった画像の情報はencodedに格納されています。\nimdecode メモリ上の画像データを読み込むimdecodeは以下のようにします。\ndecoded = cv2.imdecode(encoded, flags=cv2.IMREAD_COLOR) 第一引数はimencodeの出力です。 flagsは何かしら指定しないといけないのですが、これはどう読み込むかをあらわすフラグです。 BGRの3channelで読み込む場合にはcv2.IMREAD_COLORを指定し、Gray scaleの1channelで読み込む場合にはcv2.IMREAD_GRAYSCALEを指定します。\n適用結果 jpgのqualityを10にしてimencodeした後にimdecodeした結果を元の画像と比較してみます。\n   元画像 imdecode後の画像          右側の画像はノイズがのっていることが分かるでしょうか？ちゃんとjpgの形式で圧縮されたようです。\n","date":"2020-07-16T11:00:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/imencode%E3%81%A8imdecode%E3%81%AB%E3%82%88%E3%82%8B%E3%83%A1%E3%83%A2%E3%83%AA%E4%B8%8A%E3%81%A7%E3%81%AE%E7%94%BB%E5%83%8F%E5%9C%A7%E7%B8%AE/ef33a82444b4c9bca9b7fb97cad0d467_hu84448d5f1de44d6618fcb6e3cdb2700d_42025_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/imencode%E3%81%A8imdecode%E3%81%AB%E3%82%88%E3%82%8B%E3%83%A1%E3%83%A2%E3%83%AA%E4%B8%8A%E3%81%A7%E3%81%AE%E7%94%BB%E5%83%8F%E5%9C%A7%E7%B8%AE/","title":"imencodeとimdecodeによるメモリ上での画像圧縮"},{"content":"本記事はQrunchからの転載です。\n 今日はなにかとサンプルコードで使われるガウス平滑化です。\nガウス平滑化とは 前々回取り上げた単純平滑化は局所領域の平均をとることで、平滑化をおこないました。これは局所領域内の各ピクセルの重み付けがすべて等しいともいえます。 ガウス平滑化では二次元のガウス分布を離散化した値を重みとして利用するような平滑化になります。 $$g(x,y) = \\frac{1}{2\\pi\\sqrt{\\sigma^2}}\\exp\\left(-\\frac{x^2 + y^2}{\\sigma^2}\\right).$$\n単純平滑化との違いは？ 具体的なカーネルの比較の例は以下のとおりです。\n　単純平滑化　ガウス平滑化 ガウス平滑化の場合には中心の重みが大きく、そこから遠ざかるほど、重みが小さくなっていきます。\n画像に与える影響の違いとしては、単純平滑化よりも中心の重みが大きいことで、平滑化後のボケが少ないことが挙げられます。\n単純平滑化とガウス平滑化の違いを実験 OpenCVでガウス平滑化を使う場合は以下のようにすればOKです。\nblur = cv2.GaussianBlur(img, ksize=(9, 9), sigmaX=2, sigmaY=2) ksizeはカーネルの大きさ（局所領域のサイズ）、sigmaXはガウス分布のx方向の分散、sigmaYはy方向の分散になります。分散は0を入れると、デフォルト値を計算し、それを利用してくれます。\n次のようなノイズを乗せた画像を用意しました。 それぞれの平滑化の適用結果が以下のとおりです。すべてカーネルサイズは9×9です。 単純平滑化　メディアンフィルタ　ガウス平滑化 単純平滑化とガウス平滑化を比べると、ガウス平滑化のほうが若干ノイズが多めの気がしますが、ボケが少ないです。 メディアンフィルタはノイズは取れますが、もとの情報が結構落ちてますね。\n","date":"2020-07-14T18:30:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AA%E3%81%AB%E3%81%8B%E3%81%A8%E3%81%82%E3%82%89%E3%82%8F%E3%82%8C%E3%82%8B%E3%82%AC%E3%82%A6%E3%82%B9%E5%B9%B3%E6%BB%91%E5%8C%96/bc05ecb93cbcd05f4c4724aca94cb99d_hu36eace3aceeacd3c7a9a41f34fa94d3f_35649_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E3%82%B5%E3%83%B3%E3%83%97%E3%83%AB%E3%82%B3%E3%83%BC%E3%83%89%E3%81%A7%E3%81%AA%E3%81%AB%E3%81%8B%E3%81%A8%E3%81%82%E3%82%89%E3%82%8F%E3%82%8C%E3%82%8B%E3%82%AC%E3%82%A6%E3%82%B9%E5%B9%B3%E6%BB%91%E5%8C%96/","title":"サンプルコードでなにかとあらわれるガウス平滑化"},{"content":"本記事はQrunchからの転載です。\n 単純平滑化の場合には、局所領域内での平均を取るため、周辺とは大きく異なるピクセル値をもつピクセルがあると、その影響が大きすぎて上手くいかない場合があります。 そのようなケースでは中央値を使うようにすると、上手くいくかもしれません。\nmedianBlur OpenCVではmedianBlurという関数で局所領域内の中央値を使うような平滑化をおこなえます。\n以下がmedianBlurを実際に実行したコードになります。\nimport cv2 import matplotlib.pyplot as plt image = cv2.imread(\u0026#39;noro-min.jpeg\u0026#39;) blur = cv2.medianBlur(img, ksize=5) blur = cv2.cvtColor(blur, cv2.COLOR_BGR2RGB) plt.imshow(blur[:, :, ::-1]) plt.show() 人工的に画像にノイズを乗せて、blurとmedianBlurを適用した結果を比べてみます。 ノイズを乗せた画像 blurを適用した画像（左）、medianBlurを適用した画像（右） 中央値を使うことで、ノイズを上手く取り除くことができています。 ただし、文字の部分などは結構ボケるようになりました。中央値を使うと、白い背景と近い部分のピクセルはすべて白に置き換えられてしまうからです。\n","date":"2020-07-13T11:00:00+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E5%A4%96%E3%82%8C%E5%80%A4%E3%81%AB%E5%BC%B7%E3%81%84medianblur/38fd4d2636a29433a5cd4a3204c1dca5_huf1fb85762e46c02545e05964bb4e6005_47893_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E5%A4%96%E3%82%8C%E5%80%A4%E3%81%AB%E5%BC%B7%E3%81%84medianblur/","title":"外れ値に強いMedianBlur"},{"content":"本記事はQrunchからの転載です。\n 画像処理で結構シビアなのが、照明環境です。 例えば次の画像のように、画像の中で明暗が異なると、大津の二値化ではうまくいきません。 入力画像 大津の二値化適用 とはいえ、アプリケーションによっては撮影者に常に気をつけてもらうことも難しかったりします。 そんなときにはAdaptiveThresholdが役に立ちます。\nAdaptiveThresholdとは？ OpenCVで使えるAdaptiveThresholdには2パターンあるのですが、まずは簡単な局所領域での平均を利用する方から説明します。\n局所領域での平均を用いたAdaptiveThreshold この方法では、ある座標$(x,y)$のピクセルの二値化をおこなうときには、$(x,y)$を中心としたある大きさの局所領域内の各ピクセルのグレースケール値の平均値を計算します。 この平均値から指定した定数を引いた値をしきい値$T(x,y)$とします。 もし$(x,y)$のグレースケール値が$T(x,y)$を超えれば255に置き換え（255以外にもこの値は指定できます）て、$T(x,y)$以下であれば、$0$にします。\nざっくり言えば、$(x,y)$の周辺領域の平均値を二値化のしきい値にするということになります。\nこうすると何が良いかといえば、周辺領域が暗ければ、しきい値は暗い方に設定されますし、周辺領域が明るければ、しきい値は明るい方に設定されます。つまり、局所領域内である程度明暗がわかれていれば、きちんと二値化ができるということです。すごいですね。\nこの方法は、次のようにcv2.adaptiveThresholdによって利用可能です。\ngray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) bi_img = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 11, 5) plt.imshow(bi_img) plt.gray() plt.show() ちゃんとそれっぽく二値化されてます！\nadaptiveThresholdの各引数は以下のとおりです。 局所領域は$(x,y)$を中心とした領域になるため、領域の大きさは奇数で指定しなければいけないことに注意してください。\n   引数 意味     1 入力画像   2 ここで説明した方法を使うことをあらわす値   3 threshold typeでこれは前々回説明したものと同じ   4 周辺領域の大きさで、11ということは11×11の領域で平均値を計算している   5 しきい値を決めるときに平均値から引かれる定数    局所領域でのガウス分布による重み付を用いたAdaptiveThreshold 先程の平均値は局所領域内は平等に扱うような方法でしたが、問題によっては、局所領域の中心$(x,y)$に近いほど重要視して、遠ざかるほど影響を小さくしたいなぁと思うときがあります。 そんなときにはガウス分布による重み付けを利用することができます。\nOpenCVで利用するときにはさきほどの第二引数をcv2.ADAPTIVE_THRESH_GAUSSIAN_Cに変えるだけでOKです。\ngray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) bi_img = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 5) plt.imshow(bi_img) plt.gray() plt.show() こちらも上手くいっています。\nおわりに 問題設定によっては平均の方だと上手くいかず、ガウス分布の重み付けのほうは上手くいったりしますので、そのあたりの使い分けは試行錯誤するしかないかなと思います。\n","date":"2020-07-11T09:22:28+09:00","image":"https://opqrstuvcut.github.io/blog/posts/adaptivethreshold%E3%81%A7%E7%85%A7%E6%98%8E%E7%92%B0%E5%A2%83%E3%81%8C%E5%BE%AE%E5%A6%99%E3%81%AA%E7%94%BB%E5%83%8F%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96/849c7e82970effeb19b44b3bf511192f_hu10ea85cf68759cab2e544853b5039614_48271_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/adaptivethreshold%E3%81%A7%E7%85%A7%E6%98%8E%E7%92%B0%E5%A2%83%E3%81%8C%E5%BE%AE%E5%A6%99%E3%81%AA%E7%94%BB%E5%83%8F%E3%82%92%E4%BA%8C%E5%80%A4%E5%8C%96/","title":"AdaptiveThresholdで照明環境が微妙な画像を二値化"},{"content":"本記事はQrunchからの転載です。\n 大津の2値化とは？ シンプルな二値化では、何かしらのしきい値を決めてあげる必要がありました。\n人間がグレースケール値のヒストグラムを見てしきい値を決めたり、試行錯誤するというのも良いですが、場合によってはしきい値を自動で決定したくなります。\nそのような方法として有名なのが大津の2値化です。 大津の2値化を使うことで、ある意味での最適なしきい値を決定してくれます。\n大津の2値化の中身は？ 大津の2値化では、グレースケールのヒストグラムを描いたときに、山が2つ存在するケースを想定しています。例えば次のようなヒストグラムです。 つまり、画像の白い部分と黒い部分の区別がある程度はっきりとつくようなケースを指しています。 白いところ、黒いところ、それらの間くらいの色の3種類が多数を占めているような、ヒストグラム上で山が3つできるような状況は想定されていません（アプリケーションによっては、それでも上手くいくかもしれませんが）。\nさて、ヒストグラムが2つの山をもつようなグレースケールの画像が与えられたとして、大津の2値化はどのようにしきい値を決めているのでしょうか？\n大津の2値化では、しきい値以下のグレースケール値としきい値より大きい値のグレースケール値の2つのグループにわけ、それぞれの分散をそれぞれ計算した後、それらの重み付きの和を考えます。しきい値はこの分散の重み付き和が最小になるように決められます。\n式であらわせば、グレースケール値のしきい値$t$、しきい値$t$以下のグループの分散$\\sigma-2_1(t)$、しきい値より大きいグループの分散$\\sigma^2_2(t)$、しきい値以下の値の個数$q_1(t)$、しきい値より大きい値の個数$q_2(t)$を用いて以下のようになります。\n$$ \\sigma^2(t)=p_1(t) \\sigma^2_1(t) + p_2(t) \\sigma^2_2(t).$$\n大津の2値化では$\\sigma^2(t)$を最小化するようなしきい値$t$を見つけます。\n直感的には分散が最小になるようなしきい値を見つけるのは良い方法のように思えます。 なぜかといえば、谷の部分からしきい値を動かしていき、どちらかの山の一部が他方のグループに取り込まれると、取り込まれた分が与える分散の増加分が非常に大きいと予想できるからです。\n大津の2値化の適用結果 大津の2値化を実際に適用してみます。 次のようなグレースケールの画像が与えられたとします。 この画像のグレースケール値のヒストグラムは以下のとおりです（先程のヒストグラムと同じものです）。 大津の2値化を適用する際にはThreshoold typeのところにcv2.THRESH_OTSUを追加します。\nret, bin_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) 上記のようなコードを実行すると、大津の2値化によって2値化された画像が得られます。\nimg = cv2.imread(\u0026#34;ex_img.jpg\u0026#34;) gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) ret, bin_img = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU) plt.imshow(bin_img) plt.gray() plt.show() しきい値が自動で適切に設定され、キレイに二値化できてますね。\nガウシアンフィルタとの組み合わせ ここでは詳しくは述べませんが、ノイズが多い画像では、ガウシアンフィルタで平滑化することでノイズが軽減され、ヒストグラムの山がよりシャープになりえます。\nそうすると、大津の2値化後の結果がより人間の感覚にあったものとなったりします。\n","date":"2020-07-10T13:08:19+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E5%A4%A7%E6%B4%A5%E3%81%AE%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%A7%E6%A5%BD%E3%82%92%E3%81%99%E3%82%8B/28686c735e0c8f6271a0dd5b95693c1a_hu6de79d44a3518fa57d2054848cc8b45a_24574_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E5%A4%A7%E6%B4%A5%E3%81%AE%E4%BA%8C%E5%80%A4%E5%8C%96%E3%81%A7%E6%A5%BD%E3%82%92%E3%81%99%E3%82%8B/","title":"大津の二値化で楽をする"},{"content":"本記事はQrunchからの転載です。\n 最近自然言語処理をよくやっていて、BERTを使うことも多いです。 BERTの性能は高く素晴らしいのですが、実際使う上では、私のような計算リソース弱者には辛いところがあります。\n例えば、BERTは非常にパラメータ数が多いことで有名ですが、パラメータが多いと、fine-tuningでの学習や推論の時間がかかることや大きめのメモリが積んであるGPUがないと学習ができない、といった部分がネックになりえます。\nBERTのパラメータ数を減らす試みとしてはTinyBERTやDistilBERTによる蒸留を使った手法がありますが、今回紹介するPoor Man’s BERT: Smaller and Faster Transformer ModelsではBERTのTransformerの数を単純に減らすことでパラメータ数を減らしています。\n実際にTinyBERTやDistilBERTと同じことをするのは難しいですが、今回のように層を減らして学習するのは容易にできますので、とても実用性があるのではないかと思います。\n比較実験 論文では12層のTransformerをもつBERTモデルから色々な方法でTransformerを減らし、性能比較をおこなっています。24層をもつ、いわゆるBERT-Largeは、貧乏人にはメモリが足らずにfine-tuningも難しいのです。\n次の図がTransformer層の減らし方の一覧です。 各方法の詳細は以下のとおりです。\nTop-Layer Dropping 先行研究によると、BERTの後ろの層は目的関数に特化したような重みになっているようです。つまり、BERTで汎用的に使えるように学習されている部分は前の層ということになります。 このため、後ろの層に関しては減らしても性能がそんなに悪化しないんじゃないかという仮定のもと、BERTの最後から4つあるいは6つのTransformerを削除します。\nEven Alternate Dropping、Odd Alternate Dropping 先行研究によると、BERTの各層では冗長性があります。つまり、隣り合った層の出力は似ているということです。 このため、1個おきにTransformerを削除します。\nContribution based Dropping Alternate Droppingと少し似ていますが、入力と出力があまり変わらないような層を削除するような方法です。 各Transformer層のなかで[CLS]の入力と出力のcosine類似度が大きい傾向にある層をあらかじめ見つけておき、それを削除します。\nSymmetric Dropping もしかすると、12層のTransformerのうち、真ん中のあたりはあまり重要じゃないかもしれません。 ということで、前と後ろは残して真ん中付近のTransformerを削除します。\nBottom-Layer Dropping BERTの最初のほうの層が文脈の理解に重要といわれており、最初のほうを消す理論的な理由はないですが、年のために最初のほうのTransformerを削除したモデルも試します。\n実験 手法間の性能比較 先程示した方法とDistilBERTをGLUEタスクのスコアで比較した結果が以下になります。BERTだけではなくXLNetでも実験してくれています。 これから以下のことが分かります。\n 各方法のスコアは12層あるBertには劣る。 4層減らす分にはBottom-Layer Dropping以外の方法ではそれほど性能に差がでないが、6層減らす場合にはTop-Layer Dropping（最後の6層を消す）が性能劣化が小さい。 Top-Layer Droppingの6層を消した場合はDistilBERTと似たような性能になっている。学習の手間はDistilBERTのほうが圧倒的に大きいので、性能が同程度、計算時間も同程度ならば本手法を使うメリットが大きいです。 XLNetの場合には最後の4層を消したモデルでも12層あるXLNetとほぼ同じ性能が出せる（＝性能劣化が少ない）。  タスクごとの性能変化の検証 次にタスクごとの性能の変化を見ていきます。前の実験から後ろの層を消していくTop-Layer Droppingが良いとわかっているため、Top-Layer Droppingに限って実験がされています。 問題によっては6層消してもほとんど変化がなかったりします。\n余談ですが、私が自分で試したある問題では6層消して8ポイント分、4層消して4ポイント分の性能劣化、2層消して2ポイント分の性能劣化になりました。\nタスクごとの性能劣化がおこる層数の検証 タスクごとに後ろを何層削ると1%、2%、3%の性能劣化がおこるのかを示した表です。 ビックリしますが、XLNetは結構層を消しても性能劣化が起こりづらいですね。\nパラメータ数や計算時間比較 学習時間・推論時間は削った層の割合だけおおよそ減ることが予想されますが、実際に計算時間がどれくらい変わったかを示したのが以下の表です。 6層削ったモデルでは学習時間・推論時間の両方でだいたい半分くらいになってますね。\nBERTとXLNetの層数での比較 BERTとXLNetのTransformerの数を変えると、どう性能が変化するかを示したのが以下の図です。 なんとXLNetは7層にするあたりまではほどんど性能の変化がありません。BERTは層を減らすと順調に性能が悪化します。\n上記の話には実験的な根拠があり、それを示したのが以下の図です。 これはBERTとXLNetの事前学習モデルとfine-tunedモデル間で同じ層同士の出力のcosine類似度を計算した結果になります。つまり、小さい値になっているほど、fine-tuningで出力が大きく変わるような学習がおこなわれたことになります。 BERTの場合には後ろの層ほど大きな変化があることがわかります。またfine-tuningしても前の方の層はほとんど変わっていませんね。 一方でXLNetの場合には前の層の変化がないのはBERTと一緒ですが、後ろの層に関してもあまり変化がありません（もちろん12層目だけは大きく変わります）。つまり、問題を解くときにあまり8層以降は重要じゃないのではと考えられます。\n感想 私のような貧乏人には大変ありがたい論文でした。 計算リソースがあまりない方は使ってみましょう！\n","date":"2020-06-21T15:22:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/ade970e39b6211acf56131ea9aadba79_hua9848c14f6fc76924d36dd77c390b808_485748_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E8%B2%A7%E4%B9%8F%E4%BA%BA%E3%81%AA%E3%81%AE%E3%81%A7poor-mans-bert%E3%82%92%E8%AA%AD%E3%82%93%E3%81%A7%E8%A7%A3%E8%AA%AC/","title":"貧乏人なのでPoor Man’s BERTを読んで解説"},{"content":"本記事はQrunchからの転載です。\n AWSのLambda（Python）からPostgresを利用するためのライブラリの使い方のメモです。何もトラブルなく使えましたが、一応。 ライブラリのレポジトリはこちらです。\n ライブラリのclone  git clone https://github.com/jkehler/awslambda-psycopg2.git  適切な名前にリネーム LambdaでPython3.6を利用する場合にはcloneしてきたレポジトリにあるpsycopg2-3.6をpsycopg2にリネームします。あるいはPython3.7を利用する方はpsycopg2-3.7をpsycopg2にリネームします。\n  適切な位置への配置\npsycopg2をLambdaにデプロイするコードと同じディレクトリに配置します。 例： lambda/hoge.pyというPythonスクリプトをデプロイする場合にはlambdaディレクトリ以下にpsycopg2を配置する。\n  Lambdaにデプロイする！\n  ","date":"2020-05-04T13:35:16+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/aws%E3%81%AElambda%E3%81%8B%E3%82%89postgres%E3%82%92%E5%88%A9%E7%94%A8/","title":"AWSのLambdaからPostgresを利用"},{"content":"本記事はQrunchからの転載です。\n 関数が上に凸であることの必要十分条件はヘッセ行列が半負定値であることです。ネット上だと日本語でまとまっている文献があんまりないかもと思ったので、今回はこの証明をまとめます。 なお、関数が下に凸のときにはヘッセ行列は半正定値となります。上に凸の定義を使っているところを下に凸の定義に置き換え、正定値を負定値に置き換えれば、同じ議論が可能です。 また出てくる関数$f$は暗黙的に定義域で2階微分可能としています。\n定義 関数が上に凸の定義 関数$f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$が上に凸とは任意の元$\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)} \\in \\mathbb{R}^{n}$と任意の$t \\in [0,1]$に対して以下が成り立つことを指します。 $$ f(t\\mathbf{x}^{(2)} + (1 -t)\\mathbf{x}^{(1)}) \\geq tf(\\mathbf{x}^{(2)}) + (1 -t) f(\\mathbf{x}^{(1)}).$$\nヘッセ行列の定義 関数$f:\\mathbb{R}^{n} \\rightarrow \\mathbb{R}$のヘッセ行列$H$を以下のように定義します。 $$H_f = \\nabla^2 f = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2} \u0026amp;\\frac{\\partial^2 f}{\\partial x_1\\partial x_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial^2 f}{\\partial x_1\\partial x_n} \\cr \\frac{\\partial^2 f}{\\partial x_2\\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_2^2} \u0026amp; \\dots \u0026amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\cr \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\cr \\frac{\\partial^2 f}{\\partial x_n\\partial x_1} \u0026amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} \u0026amp; \\dots \u0026amp; \\frac{\\partial^2 f}{ \\partial x_n^2} \\end{pmatrix}.$$\n行列の半負定値性の定義 ある対称行列$A \\in \\mathbb{R}^{n \\times n}$に対して任意のベクトル$\\mathbf{x} \\in \\mathbb{R}^n$が次を満たすとき、$A$を半負定値といいます。 $$ \\mathbf{x}^T A \\mathbf{x} \\leq 0.$$\n証明 本題の証明に使うため、以下を先に証明しておきます。\n凸関数の一次条件 証明$(\\Rightarrow)$ 上に凸であることの定義から、 $$ \\begin{aligned} f(t\\mathbf{x}^{(2)} + (1 -t)\\mathbf{x}^{(1)}) \\geq \u0026amp; tf(\\mathbf{x}^{(2)}) + (1 -t) f(\\mathbf{x}^{(1)}) \\cr f(\\mathbf{x}^{(1)} + t(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)})) \\geq \u0026amp; f(\\mathbf{x}^{(1)}) + t(f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)})) \\cr \\frac{f(\\mathbf{x}^{(1)} + t(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)})) - f(\\mathbf{x}^{(1)}) }{t} \\geq \u0026amp; f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)}) \\end{aligned} $$ となります。 ここで、$g(t) = f(\\mathbf{x}^{(1)} + t(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)}))$とおくと、以下が成り立ちます。 $$ \\begin{aligned} \\frac{g(t) - g(0)}{t} \\geq \u0026amp; f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)}) \\end{aligned}. $$ さらに$t \\rightarrow 0$とすれば、以下のようになります。 $$ \\begin{aligned} g'(0) \\geq \u0026amp; f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)}) \\end{aligned}. $$ $g'(0)$がなんであるかというと、これは単純に計算すればよく、 $$g'(0) = \\left.\\frac{{\\rm d}g}{{\\rm d}t}\\right|_{t=0} = \\nabla f(\\mathbf{x}^{(1)})^ T(\\mathbf{x}^{(2)} - \\mathbf{x}^{(1)})$$ となります。 以上から、 $$ f(\\mathbf{x}^{(2)}) - f(\\mathbf{x}^{(1)})\\leq \\nabla f(\\mathbf{x}^{(1)})^T(\\mathbf{x}^{(2)}- \\mathbf{x}^{(1)})$$ が示されました。\n$(\\Leftarrow)$ 適当な$0\\leq t \\leq 1$を用いて$\\mathbf{z}=t\\mathbf{x}^{(2)} + (1-t)\\mathbf{x}^{(1)} $とおきます。仮定から以下が成り立ちます。 $$ \\begin{aligned} f(\\mathbf{x}^{(2)}) \\leq \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(2)}- \\mathbf{z}), \\cr f(\\mathbf{x}^{(1)}) \\leq \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(1)}- \\mathbf{z}). \\end{aligned} $$ 1つめの式に$t$を掛け、2つめの式に$1-t$を掛けて足し合わせることで以下のようになります。 $$ \\begin{aligned} tf(\\mathbf{x}^{(2)}) + (1 -t )f(\\mathbf{x}^{(1)}) \\leq \u0026amp; tf(\\mathbf{z}) + (1-t)f(\\mathbf{z}) + t\\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(2)}- \\mathbf{z}) + (1-t) \\nabla f(\\mathbf{z})^T(\\mathbf{x}^{(1)}- \\mathbf{z}) \\cr = \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(t(\\mathbf{x}^{(2)}-\\mathbf{z}) + (1-t) (\\mathbf{x}^{(1)}- \\mathbf{z})) \\cr = \u0026amp; f(\\mathbf{z}) + \\nabla f(\\mathbf{z})^T(t\\mathbf{x}^{(2)} + (1-t) \\mathbf{x}^{(1)}-\\mathbf{z}) \\cr = \u0026amp; f(\\mathbf{z}) \\cr = \u0026amp; f(t\\mathbf{x}^{(2)} + (1-t)\\mathbf{x}^{(1)} ) . \\cr \\end{aligned} $$ 以上から逆も証明できました。\nこの証明の参考： http://mathgotchas.blogspot.com/2011/10/proof-for-first-order-condition-of.html\nテイラーの定理 本題の証明に用いるため、テイラーの定理の特別な場合を紹介しておきます。 テイラーの定理の証明はここではしません。また実際は成り立つための条件がありますが、ここでは以下のように利用できるとします。\n関数が上に凸であることの必要十分条件がヘッセ行列が半負定値であることの証明 本題の証明です。\n証明$(\\Rightarrow)$ 適当な元$\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^{n}$を考えます。テイラーの定理より以下が成り立ちます。 $$ \\begin{aligned} f(\\mathbf{x} + \\mathbf{y}) = \u0026amp;f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y} + \\frac{1}{2} \\mathbf{y}^T H_f(\\mathbf{x}) \\mathbf{y} + o(||\\mathbf{y}||^2_2).\\tag{1} \\end{aligned}$$ また、$f$が上に凸であるという仮定から、一次条件より $$ f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y}\\tag{2}$$ が成り立ちます。式(1)と式(2)から、 $$ \\frac{1}{2} \\mathbf{y}^T H_f(\\mathbf{x})\\mathbf{y} + o(||\\mathbf{y}||^2_2) \\leq 0. $$ $\\mathbf{y} = \\alpha \\mathbf{z} , ||\\mathbf{z}||_2 = 1 $とすると、\n$$ \\begin{aligned} \\frac{1}{2} \\alpha^2 \\mathbf{z}^T H_f(\\mathbf{x})\\mathbf{z} + o(\\alpha^2) \\leq \u0026amp; 0 \\cr \\frac{1}{2} \\mathbf{z}^T H_f(\\mathbf{x})\\mathbf{z} + \\frac{o(\\alpha^2)}{\\alpha^2} \\leq \u0026amp; 0 . \\end{aligned} $$ このとき、$\\alpha \\rightarrow 0$とすると、$o(x^n)$の定義から以下が成り立ちます。 $$ \\frac{1}{2} \\mathbf{z}^T H_f(\\mathbf{x}) \\mathbf{z} \\leq 0. $$ $\\mathbf{y}$は任意のベクトルでしたので、$H_f(\\mathbf{x})$は半負定値となります。\u001c よって$f$が上に凸であるとき、ヘッセ行列$H_f(\\mathbf{x})$は任意の$\\mathbf{x}$で半負定値であることが示されました。\n$(\\Leftarrow)$ テイラーの定理より、ある$0 \u0026lt; t \u0026lt; 1$を用いて以下が成り立ちます。 $$ \\begin{aligned} f(\\mathbf{x} + \\mathbf{y}) = \u0026amp;f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y} + \\frac{1}{2} \\mathbf{y}^T H_f(\\mathbf{x} + t\\mathbf{y} ) \\mathbf{y}. \\end{aligned}$$ ヘッセ行列$H_f(\\mathbf{x} + t\\mathbf{y} ) $が半負定値であるため、明らかに $$ f(\\mathbf{x} + \\mathbf{y}) \\leq f(\\mathbf{x}) + \\nabla f(\\mathbf{x})^T\\mathbf{y} $$ が成り立ちます。これは先程示した一次条件であらわれる式と同じです。 以上から、ヘッセ行列$H_f(\\mathbf{x})$が任意の$\\mathbf{x}$で半負定値であるとき、$f$が上に凸であることが示されました。\n","date":"2020-03-11T00:08:01+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/%E9%96%A2%E6%95%B0%E3%81%8C%E4%B8%8A%E3%81%AB%E5%87%B8%E3%81%A7%E3%81%82%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E5%BF%85%E8%A6%81%E5%8D%81%E5%88%86%E6%9D%A1%E4%BB%B6%E3%81%AF%E3%83%98%E3%83%83%E3%82%BB%E8%A1%8C%E5%88%97%E3%81%8C%E5%8D%8A%E8%B2%A0%E5%AE%9A%E5%80%A4%E3%81%AE%E8%A8%BC%E6%98%8E/","title":"関数が上に凸であることの必要十分条件はヘッセ行列が半負定値の証明"},{"content":"本記事はQrunchからの転載です。\n みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。\nKL divergence KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。 具体的には次のように定義されます。 $$ KL(p||q) = \\int p(\\mathbf{x}) \\log \\left(\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\right) {\\rm d\\mathbf{x}}. $$ $p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。 なお、$KL(p||q) \\geq 0$が成り立つことに注意してください。\nKL divergenceの最小化問題 KL(p||q)のケース 仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。\n前述したKL divergenceの定義をみてみると、$p(\\mathbf{x})$が0でない値をもつ領域では$q(\\mathbf{x})$も$p(\\mathbf{x})$に近い値かあるいは$p(\\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような$q$は$p$全体をカバーするように広がる分布になると考えられます。\nKL(q||p)のケース 次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は $$ KL(q||p) = \\int q(\\mathbf{x}) \\log \\left(\\frac{q(\\mathbf{x})}{p(\\mathbf{x})}\\right) {\\rm d\\mathbf{x}}$$ ですね。 $KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\\mathbf{x})$が0に近いような領域で$q(\\mathbf{x})$が小さくなるようにすればよいです。$p(\\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような$q$は$p$の値が大きいところに集中するような分布になると考えられます。\n実験 上記の話が成り立つのかを実験してみます。\n実験準備 $p(\\mathbf{x})$は次のようにします。\n$$p(\\mathbf{x}|\\mathbf{u},\\Sigma)=\\frac{1}{{2\\pi}|\\Sigma|^{1/2}}\\exp\\biggl[-\\frac{(\\mathbf{x}-\\mathbf{u})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\mathbf{u})}{2}\\biggr].$$ また$\\mathbf{u}$と$\\Sigma$はそれぞれ $$\\mathbf{u} = \\begin{pmatrix} 0.3 \\\\ -0.2 \\end{pmatrix}, \\Sigma =\\begin{pmatrix} 0.9\u0026amp;-0.7 \\\\ -0.7 \u0026amp; 0.9 \\end{pmatrix}$$ とました。 $p$を確率密度毎に色わけして表示してみると、以下のとおりです。 また$q(\\mathbf{x})$は次のようにします。 $$q(\\mathbf{x}|\\mathbf{s},\\alpha)=\\frac{1}{{2\\pi}\\alpha}\\exp\\biggl[-\\frac{(\\mathbf{x}-\\mathbf{s})^{\\top}(\\mathbf{x}-\\mathbf{s})}{2\\alpha}\\biggr].$$\n$q$のうち、$\\mathbf{s}$と$\\alpha$が最適化するべきパラメータです。 $q$は同心円状に確率密度をもつ分布になりますので、パラメータをどうやっても$p$と一致することはできません。\n実験結果 $KL(p||q)$を最小化したケースをまず示します。 白い線が$p$の等高線です。色分けされて表示されているのが、$q$の確率密度になります。 先程の話のとおり、$q$は$p$に対して広がった分布になっていることがわかります。\n次に$KL(q||p)$を最小化したケースです。 こちらも先程の話のとおり、$q$は$p$の値が大きい箇所に集中した分布になっています。\nまとめ 今回は$q$を$p$に近づける話に限定しましたが、KL divergenceに与える分布を入れ替えると結果が変わるケースが多そうだなと想像できたんじゃないかと思います。 頭の片隅に留めておくと役立つかもしれません。\n実験に使ったスクリプト #! /usr/bin/env python import argparse import os import logging import matplotlib.pyplot as plt import numpy as np from scipy.stats import multivariate_normal import torch from torch.distributions import MultivariateNormal import torch.optim as optim def parse_argument(): parser = argparse.ArgumentParser(\u0026#34;\u0026#34;, add_help=True) parser.add_argument(\u0026#34;-o\u0026#34;, \u0026#34;--output_dir\u0026#34;, type=str) args = parser.parse_args() return args def make_data(border=5): xy = np.mgrid[-border:border:0.005, -border:border:0.005] grids = xy.shape[1] x = xy[0] y = xy[1] xy = xy.reshape(2, -1).T p_pdf = multivariate_normal.pdf(xy, np.array([0, 0]), np.array([[.9, -.7], [-.7, .9]])) return xy, x, y, p_pdf, grids def kl_div(p, q): finite_index = ~((q == 0.) | (torch.isinf(p))) q = q[finite_index] logq = torch.log(q) return torch.sum(q * (logq - p[finite_index])) def optimize_q(xy, p_pdf, invert=False): p_pdf = torch.tensor(p_pdf, requires_grad=False, dtype=torch.float32) mean = torch.tensor([0.3, -0.2], requires_grad=True) cov_coeff = torch.tensor(1., requires_grad=True) xy = torch.tensor(xy, requires_grad=False, dtype=torch.float32) optimizer = optim.SGD([mean, cov_coeff], lr=.000005) if not invert: p_pdf = torch.log(p_pdf) for i in range(25): optimizer.zero_grad() cov = torch.eye(2, 2) * cov_coeff norm_torch = MultivariateNormal(mean, cov) q_pdf = norm_torch.log_prob(xy) if invert: loss = kl_div(q_pdf, p_pdf) else: q_pdf = torch.exp(q_pdf) loss = kl_div(p_pdf, q_pdf) loss.backward() optimizer.step() logging.info( f\u0026#34;[{i + 1}iter] loss:{loss}, mean:{mean}, cov_alpha:{cov_coeff}\u0026#34;) return mean.detach().numpy(), cov_coeff.detach().numpy() def plot_dist(x, y, p_pdf, border, output_path, contour=None): plt.pcolormesh(x, y, p_pdf, cmap=\u0026#34;nipy_spectral\u0026#34;) plt.colorbar() if contour is not None: plt.contour(x, y, contour, colors=\u0026#34;white\u0026#34;, levels=5) plt.savefig(output_path) plt.close() if __name__ == \u0026#34;__main__\u0026#34;: logger = logging.basicConfig(level=logging.INFO) args = parse_argument() output_dir = args.output_dir border = 5 xy, x, y, p_pdf, grids = make_data(border) data_dist_path = os.path.join(output_dir, \u0026#34;data_dist.png\u0026#34;) plot_dist(x, y, p_pdf.reshape(grids, grids), border, data_dist_path) for invert in [True, False]: mean, cov_coeff = optimize_q(xy, p_pdf, invert=invert) output_path = os.path.join( output_dir, f\u0026#34;gauss_m{mean}_c{cov_coeff}.png\u0026#34;) q_pdf = multivariate_normal.pdf(xy, mean, np.eye(2, 2) * cov_coeff) plot_dist(x, y, q_pdf.reshape(grids, grids), border, output_path, contour=p_pdf.reshape(grids, grids)) logging.info(f\u0026#34;{output_path}is saved.\u0026#34;) ","date":"2020-03-02T18:01:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/983be7a190c4aaf3488cd6c3d4471158_hucd343461d408ae8b77fea0b56735bfcd_50111_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/kl-divergence%E3%81%AB%E4%B8%8E%E3%81%88%E3%82%8B%E5%88%86%E5%B8%83%E3%82%92%E5%85%A5%E3%82%8C%E6%9B%BF%E3%81%88%E3%82%8B%E3%81%93%E3%81%A8%E3%81%AE%E6%84%8F%E5%91%B3%E3%82%92%E3%81%BE%E3%81%98%E3%82%81%E3%81%AB%E8%80%83%E3%81%88%E3%81%9F%E3%81%93%E3%81%A8%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99/","title":"KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？"},{"content":"本記事はQrunchからの転載です。\n 最近Microsoftから発表されたImageBERTについて紹介します。\nImageBERTはBERTの入力に自然言語だけではなく、画像も受け付けるようにしたマルチモーダルなモデルです。 また論文ではモデルのアーキテクチャだけではなく、学習方法にも新たな提案がされています。\n実験ではImage-to-Sentenceでの検索とSentence-to-Imageの検索タスクでSOTAが示されています。\n論文：ImageBERT: Cross-modal Pre-training with Large-scale Weak-supervised Image-Text Data\nアーキテクチャ ImageBERTのアーキテクチャは以下のとおりです。 テキストの入力と画像の入力で分けて説明します。 なお、論文中では画像のcaptioningのデータセットを用いています。\nテキストの入力 テキストは通常のBERTのようにsubwordに分割して、それらのembeddingを入力します。 BERTでは2つの文を与えるときに、1つ目の文か2つ目の文かを識別する情報をsubwordのembeddingに加えますが、ImageBERTでも同じように画像か文かを識別する情報を加えます。図でいうところのSegment Embeddingになります。\nまた、文の位置情報もBERTやTransformerでは与える必要があり、ImageBERTでも位置情報を加えます。しかし、ここではtokenの順番を昇順に与えるというシンプルなやり方のようです。これは図中のSequence Position Embeddingになります。\n画像の入力 画像はそのままモデルに入力するのではなく、FasterRCNNで物体検出をして、検出された箇所の特徴量をそれぞれ入力する形になります（画像の特徴量はsubwordのembeddingと同じ次元に射影します）。\nテキストの場合と同じようにSegment EmbeddingとSequence Position Embeddingも与えるのですが、Sequence Position Embeddingはテキストの場合とは与え方が異なります。テキストの場合にはsubwordに順序がありましたが、画像中の物体には順序がありませんので、すべて同じSequence Position Embeddingを与えます。\nまた、これら以外にPosition Embeddingというものも与えます。Position Emebeddingは以下で与えられるベクトルをsubwordのembeddingと同じ次元に射影したものです。 $$ c = \\begin{pmatrix} \\frac{x_{tl}}{W}, \\frac{y_{tl}}{H}, \\frac{x_{br}}{W}, \\frac{y_{br}}{H}, \\frac{(x_{br} - x_{tl}) (y_{br} - y_{tl}) }{WH} \\end{pmatrix}.$$ ここで、$x_{tl}, y_{tl}, x_{br}, y_{br}$はそれぞれ物体の左上の$x$と$y$、右下の$x$と$y$座標になります。$W$と$H$は入力画像の横と縦の大きさです。 つまり、$c$は物体の位置と面積の割合の情報になります。\n事前学習のタスク ImageBERTでは事前学習に次の4つタスクを解きます。\n Masked Language Modeling (MLM) これは通常のBERTと同じように、入力されるsubwordをランダムにマスクし、マスクされた単語を予測するようなタスクです。 Masked Object Classification (MOC) これはMLMの画像版のタスクです。検出された物体をランダムにマスクし、マスクされた物体のラベルを予測するようなタスクです。正解ラベルはFaster-RCNNで求まったラベルとしています。 Masked Region Feature Regression (MRFR) MOCはラベルを予測するようなタスクですが、MRFRはマスクされた物体の箇所の特徴量を予測するタスクです。 Image-Text Matching (ITM) 入力テキストと画像が対応しているかを予測するタスクです。ランダムに画像を選ぶことで、対応していないテキストと画像のペアを作っています。  マルチステージの事前学習 ImageBERTでは事前学習をデータセット単位で別々におこないます。実験結果で書かれていますが、別々にすることで性能が大きく変わります。 以下の図のように最初にLarge-Scale Weak-supervised Image-Text Data（これは次に説明します） で事前学習をし、その次にConceptual CaptionsとSBU Captionsのデータセットで事前学習をします。最後にfinetuningをおこないます。\nLarge-Scale Weak-supervised Image-Text 大量の画像とテキストのペアをweb上からクローリングして、事前学習に使っています。 論文中では画像とテキストのペアが10M個あるこのデータセットをLAIT (Large-scale weAk-supervised Image-Text) と読んでいます。\nLAITでは、webページ上の画像とHTMLのALTあるいはTITLEタグのテキストをcaptionとして対応付けています。単純にこれらを取得してくると、当然ノイジーなデータが多く含まれることになります。例えば、言語が英語ではない、画像のサイズが小さすぎる、現実の画像ではないなどが該当します。このようなペアをルールベースあるいは機械学習のモデルを用いてフィルタリングしています。\n一連の流れは以下のようになります。\n実験 準備 事前学習したImageBERTはMSCOCOとFlickr30kを用いてfinetuningしています。\nfinetuningするさいはITMのように、入力されたテキストと画像がペアであるかを正しく予測できるように学習していきます。事前学習でのITMに用いた出力のtokenを射影してfinetuningします。\n結果 性能比較 以下の表は他手法との性能の比較です。 Image Retrievalは与えられたテキストと対応づく画像を検索するタスク、Sentence Retrievalは与えられた画像から対応づくテキストを検索するタスクです。それぞれRecallで評価されています。\n他手法と比べて、ImageBERTは性能が良いことがわかります。全体傾向として、Sentence Retrievalは性能が高いですね。\nマルチステージの効果 以下の表がマルチステージの学習の効果をあらわしています。 上4つがそれぞれのデータセットのみで学習した場合、一番下がLAITで事前学習したあとにConceptual CaptionsとSBU Captionsで学習した場合の結果になります。\n明らかにマルチステージで学習することに優位性がありますね。この結果はImageBERTに限らず参考になりそうです。\nablation study ablation studyです。 それぞれ次を意味しています。\n 画像全体の特徴量もImageBERTに与えるケースで性能が変わるかを示しています。どちらが良い性能化は一概にいえない結果になっています。 MRFRの有無で性能が変わるかを示しています。すべてのケースでMRFRを解いたほうが良い性能になっています。 Faster-RCNNで検出された物体を最大いくつ入力するかをあらわしています。最大36個与えるときより最大100個としたときのほうが高い性能になっています。 finetuningのロスとしてどれがいいかを示しています。Binaryは正しいテキストと画像のペアか否かの2分類をBinary Cross Entropyを使って解いたケースをあらわします。2分類問題として解くのが一番良い性能になっています。理由はちょっとわかりません。  まとめ マルチモーダルのモデルであるImageBERTを紹介しました。\n事前学習したモデルが公開されていれば色々試したいですが、自分で1から学習する気にはなかなかなりませんね。\nImageBERTの事前学習モデルが公開されれば、以前公開した記事と同じ要領で画像からcaptionを生成できるんじゃないかなと思ってます。\n","date":"2020-02-24T19:46:50+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/3d410aa8e5a8ffef34906b41784d2cc8_hu83eaf0560f54c0fc88795e970098e27b_819557_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E7%94%BB%E5%83%8F%E3%81%A8%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E3%81%A7%E3%81%AE%E3%83%9E%E3%83%AB%E3%83%81%E3%83%A2%E3%83%BC%E3%83%80%E3%83%AB%E3%81%AAimagebert/","title":"画像と自然言語でのマルチモーダルなImageBERT"},{"content":"本記事はQrunchからの転載です。\n Pandasのgroupbyについては雰囲気でやっていたところがありますので、ちょっと真面目に使い方を調べてみました。使っているPandasのバージョンは1.0.1です。\n以下では次のようなDataFrameを使用します。\ndf = pd.DataFrame({\u0026#34;名字\u0026#34;: [\u0026#34;田中\u0026#34;, \u0026#34;山田\u0026#34;, \u0026#34;上田\u0026#34;, \u0026#34;田中\u0026#34;, \u0026#34;田中\u0026#34;], \u0026#34;年齢\u0026#34;: [10, 20, 30, 40, 50], \u0026#34;出身\u0026#34;: [\u0026#34;北海道\u0026#34;, \u0026#34;東京\u0026#34;, None, \u0026#34;沖縄\u0026#34;, \u0026#34;北海道\u0026#34;]})     名字 年齢 出身     0 田中 10 北海道   1 山田 20 東京   2 上田 30    3 田中 40 沖縄   4 田中 50 北海道    Pandasのgroupby PandasのgroupbyはSQLにおけるgroupbyと似たような働きになります。つまるところ、主に集計に使われます。\n例えば名字という列をキーとしてgroupbyするときには次のようにします。\ndf.groupby(\u0026#34;名字\u0026#34;) ただしこれだけでは全く意味がありません。 以下ではgroupbyをしたあとにどう利用することができるかを示します。\nグループ毎にDataFrameを取り出す forを使う forを使ってグループ毎にDataFrameとしてデータを取り出せます。\nfor name, grouped_df in df.groupby(\u0026#34;名字\u0026#34;): print(f\u0026#34;名字：{name}\u0026#34;) print(grouped_df) 名字：上田\n    名字 年齢 出身     2 上田 30     名字：山田\n    名字 年齢 出身     1 山田 20 東京    名字：田中\n    名字 年齢 出身     0 田中 10 北海道   3 田中 40 沖縄   4 田中 50 北海道    get_groupを使う get_groupを使えば1つのグループを指定することもできます。\ndf.groupby(\u0026#34;名字\u0026#34;).get_group(\u0026#34;田中\u0026#34;)     名字 年齢 出身     0 田中 10 北海道   3 田中 40 沖縄   4 田中 50 北海道    groupbyを用いた集計 describe グループごとに統計量を色々計算できます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].describe()    名字 count mean std min 25% 50% 75% max     上田 1 30 nan 30 30 30 30 30   山田 1 20 nan 20 20 20 20 20   田中 3 33.3333 20.8167 10 25 40 45 50    グループの個数のカウント グループごとに行数が数えられます。\ndf.groupby(\u0026#34;名字\u0026#34;).size()    名字 0     上田 1   山田 1   田中 3    次のcountも似たような感じで行数を数えます。ただし、欠損値はカウントされませんので、次のcountの結果とsizeの結果は異なります。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;出身\u0026#34;].count()    名字 出身     上田 0   山田 1   田中 3    演算 和 グループごとに和を計算します。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].sum()    名字 年齢     上田 30   山田 20   田中 100    平均 グループごとに平均を計算します。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].mean()    名字 年齢     上田 30   山田 20   田中 33.3333    特定のデータ取得 グループの先頭を取得します。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].first()    名字 年齢     上田 30   山田 20   田中 10    次のnthでもグループの先頭を取得できます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].nth(0)    名字 年齢     上田 30   山田 20   田中 10    nthの引数を1にするとグループの2番目のデータが取得できます。 dfでは名字が\u0026quot;田中\u0026quot;のケースのみが複数行存在しますので、\u0026ldquo;田中\u0026quot;の2番目だけが取得されます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].nth(1)    名字 年齢     田中 40    関数を渡して計算 次のように任意の関数をaggregateに渡すことで、好きなように集計できます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate(np.sum)    名字 年齢     上田 30   山田 20   田中 100    aggregateに渡す関数はもちろんlambda式でもOKです。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate(lambda vals: sum(vals))    名字 年齢     上田 30   山田 20   田中 100    ちなみにこんな感じで複数個の関数を渡すこともできます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate([np.mean, np.sum, lambda vals: sum(vals)])    名字 mean sum \u0026lt;lambda_0\u0026gt;     上田 30 30 30   山田 20 20 20   田中 33.3333 100 100    集計結果に名前をつけることもできます。以下ではmax_ageという名前の集計結果が得られます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].aggregate(max_age=pd.NamedAgg(column=\u0026#34;年齢\u0026#34;, aggfunc=np.max))    名字 max_age     上田 30   山田 20   田中 50    transformでグループの集計結果と各行の値から計算 transformを使えばグループの集計結果と行の値を組み合わせた値を計算できます。\n例1 以下のようにして、行の値とグループの平均との差を計算できます。lambda式のxがグループの各行であり、x.mean()でグループの平均を計算しています。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].transform(lambda x: (x - x.mean())     年齢     0 -23.3333   1 0   2 0   3 6.66667   4 16.6667    例2 グループの平均よりも大きいかどうかをあらわすbooleanが得られます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].transform(lambda x: (x \u0026gt; x.mean())     年齢     0 0   1 0   2 0   3 1   4 1    ブロードキャストの例 transformに渡す関数がスカラ値を返せば、同じグループのなかで同じ値をもつことになります。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].transform(lambda x: x.mean())     年齢     0 33.3333   1 20   2 30   3 33.3333   4 33.3333    前後の値を使う計算 rolling 移動平均の計算などで使うrollingはグループ単位でおこなえます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].rolling(window=2).mean()     年齢     (\u0026lsquo;上田\u0026rsquo;, 2) nan   (\u0026lsquo;山田\u0026rsquo;, 1) nan   (\u0026lsquo;田中\u0026rsquo;, 0) nan   (\u0026lsquo;田中\u0026rsquo;, 3) 25   (\u0026lsquo;田中\u0026rsquo;, 4) 45    cumsum 累積和もグループ単位でおこなえます。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].cumsum()     年齢     0 10   1 20   2 30   3 50   4 100    累積和は以下のようにしても計算できます。\ndf.groupby(\u0026#34;名字\u0026#34;).expanding().sum()     年齢     (\u0026lsquo;上田\u0026rsquo;, 2) 30   (\u0026lsquo;山田\u0026rsquo;, 1) 20   (\u0026lsquo;田中\u0026rsquo;, 0) 10   (\u0026lsquo;田中\u0026rsquo;, 3) 50   (\u0026lsquo;田中\u0026rsquo;, 4) 100    applyでグループ毎に好き勝手に処理する applyを使えば、グループ単位のDataFrameを好きに処理したあとにconcatできます。 以下の例を参照ください。\ndef f(df): return pd.DataFrame({\u0026#34;name\u0026#34;: df[\u0026#34;名字\u0026#34;], \u0026#34;age\u0026#34;: df[\u0026#34;年齢\u0026#34;] - df[\u0026#34;年齢\u0026#34;].mean()}) df.groupby(\u0026#34;名字\u0026#34;).apply(lambda x: f(x))     name age     0 田中 -23.3333   1 山田 0   2 上田 0   3 田中 6.66667   4 田中 16.6667    グループをfilteringする filterを使えば、条件を満たすグループのみを残すような処理が可能です。\n例1 グループ内の個数が1より大きいグループだけを残す。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].filter(lambda x: len(x) \u0026gt; 1)     年齢     0 10   3 40   4 50    例2 グループの値の最大値が40未満であるグループだけを残す。\ndf.groupby(\u0026#34;名字\u0026#34;)[\u0026#34;年齢\u0026#34;].filter(lambda x: x.max() \u0026lt; 40)     年齢     1 20   2 30    例3 グループに北海道出身の人が含まれるグループだけを残す。\ndf.groupby(\u0026#34;名字\u0026#34;).filter(lambda x: \u0026#34;北海道\u0026#34; in x[\u0026#34;出身\u0026#34;].tolist()     名字 年齢 出身     0 田中 10 北海道   3 田中 40 沖縄   4 田中 50 北海道    ","date":"2020-02-14T12:04:01+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/pandas%E3%81%AEgroupby%E3%81%AE%E4%BD%BF%E3%81%84%E6%96%B9%E3%82%92%E3%81%BE%E3%81%A8%E3%82%81%E3%82%8B/","title":"Pandasのgroupbyの使い方をまとめる"},{"content":"本記事はQrunchからの転載です。\n Pandas1.0からは次のようにしてDataFrameをMarkdownの表として出力できます。\nprint(df.to_markdown()) 以下のように表示されます。\n| | 名字 | 年齢 | 出身 | |---:|:-------|-------:|:-------| | 0 | 田中 | 10 | 北海道 | | 1 | 山田 | 20 | 東京 | | 2 | 上田 | 30 | | | 3 | 田中 | 40 | 沖縄 | | 4 | 田中 | 50 | 北海道 | QrunchやQiitaに大体そのままコピーできます。 ちゃんと以下のように表示されます。\n    名字 年齢 出身     0 田中 10 北海道   1 山田 20 東京   2 上田 30    3 田中 40 沖縄   4 田中 50 北海道    上手く表として表示されないときは、左上の空白のセルに全角スペース入れたり頑張りましょう。\n","date":"2020-02-13T01:55:35+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/pandas%E3%81%AEdataframe%E3%82%92%E6%9C%80%E9%AB%98%E3%81%AB%E7%B0%A1%E5%8D%98%E3%81%ABmarkdown%E3%81%AE%E8%A1%A8%E3%81%A8%E3%81%97%E3%81%A6%E5%87%BA%E5%8A%9B/","title":"PandasのDataFrameを最高に簡単にMarkdownの表として出力"},{"content":"本記事はQrunchからの転載です。\n モデルの予測結果を説明する方法としてLIMEがあります。 LIMEはディープラーニングに限らず、任意のモデルに対して予測結果を適用することができます。 また手法としては結構有名かと思います。\n今回はそんなLIMEの理論について説明します。\n論文：“Why Should I Trust You?” Explaining the Predictions of Any Classifie\nLIMEの戦略 任意のモデル$f$に入力$x \\in \\mathbb{R}^d$が与えられたときの予測結果$f(x)$への特徴量の寄与を求めることを考えます。\nLIMEでは$x$近傍（近傍については後述）に対しては$f$と同じような予測をすることができる、かつ解釈が容易なモデル$g$を求めます。 例えば$g$が線形モデルの場合には、$g$の各係数を見ることで特徴量の寄与を得ることが可能です。あるいは$g$が決定木であれば、人間でもある程度容易にモデルの解釈が可能です。ですから、このようなモデル$g$を$f$の代わりに使って、予測結果の解釈をしようというモチベーションです。 ただし、LIMEでは$g$には特徴量の値が$0$か$1$となるベクトル$x'$が入力として与えられるものとします。これは何らかのルールで$x$の要素と$x'$の要素が対応づいているとします。ここも詳細をあとで述べます。 以上のように、解釈が難しいモデル$f$を解釈が容易なモデル$g$に落とし込むことがLIMEのやりたいことになります。\n実際にどうやって$g$を求めるのかといえば、次式のようになります。 $${\\rm argmin_{g \\in G}} \\ L(f, g, \\pi_x) + \\Omega(g).$$\nここで、\n $L$は損失関数です。$x$近傍で$g$の予測値が$f$の予測値に近いと、小さくなるように$L$を定義します。 $\\pi_x$は損失関数で使われる重みで、$x$の近傍点が$x$から遠いほど小さい値を取るようにします。詳細は後述する線形モデルの項を参照。 $\\Omega$はモデルの複雑さとなります。決定木を使う場合には木の深さであったり、線形モデルの場合には非ゼロの重みの数になります。モデルを解釈するためには、モデルはシンプルな方が良いため、$\\Omega$を加えることで$g$をなるべく人間にやさしいモデルにしてあげます。  まだ色々と詳細を述べていないため、わからないところは多々あると思いますが、上式はなるべくシンプルなモデルで$x$の近傍で$f$と近似する$g$を見つけるといったことを意味します。 この局所的に近似された$g$が得られれば、$x$近傍での特徴量が$g$へ与える寄与がわかる、つまり$f$へ与える寄与が近似的にはわかります。\n次に画像の場合のケースについて、詳細に踏み込みます。\n画像に対する線形モデルでのLIME superpixel 画像にLIMEを適用する場合、まず次のように入力画像をsuperpixelに分割し、領域ごとに寄与を求めていきます。\n 引用元：https://towardsdatascience.com/understanding-how-lime-explains-predictions-d404e5d1829c\n 実際には上記のようにある程度細かく領域を分けますが、以下では例として扱いやすいように次のような画像を考えて、粗く領域を分けていきます（左がオリジナルのくまモンで、右がsuperpixelに分割されたくまモンです）。 各領域を$g$に与える入力$x'$の各要素に対応させます。例えば1番の領域が$x'$の1番目の要素、2番が2番目の要素のようにします。その上で、$x'$の各要素が1のときには対応する領域のピクセルが$x$と同じピクセル値、0のときにはその領域がグレーで埋められた画像と対応していると考えます。 具体的には $$x' = [0, 0, 1, 1, 0,0,0,0]$$ としたとき、3番目と4番目だけが1ですので、この$x'$に対応した画像は次のようになります。 近傍のサンプリング LIMEでは $x$の近傍のサンプリングをおこないます。 画像の場合に近傍とはどうなるんでしょうか？直感的には謎じゃないでしょうか。\nLIMEの場合には分割された領域のうち、適当な個数（個数もランダムに決めますが、個数の下限は決めておきます）をそのままにし、それ以外をグレーに置き換える処理をします。 $x'$の話でいえば、適当な個数の要素については1とし、それ以外は0とする処理に等しいです。\nこのようにして得られた画像を$x$の近傍として扱います。またこのようにして近傍を得ることを、近傍のサンプリングとします。 先程示した$x'$に対応した画像も$x$の近傍になります。\n線形モデルのケース $g$が線形モデルの場合には$g(z')$は次のようになります。 線形モデルの係数（寄与）を求めるため、次のように損失関数$L$を定義します。 $$ L(f, g, \\pi_x) = \\sum_{z,z'∈Z}\\pi_x(z) (f(z) − g(z'))^2.$$ ここで$\\pi_x$は以下のとおりです。 $$ \\pi_x = \\exp(−D(x, z)^2/\\sigma^2).$$ $z$は$x$近傍の画像をあらわし、$z'$は先程まで説明していた（$z$に対応する）$x'$と同じものです。$Z$はサンプリングされた$z$と$z'$のペアになります。\n上式の意味合いとしては、近傍画像$z$の学習済みモデルでの予測値$f(z)$と解釈が容易なモデル$g(z')$が近い値になるように$g$を学習していきます。\nまた、$\\pi_x$の存在のため、$z$が$x$に近ければ（=近傍が入力画像に近い）二乗誤差$(f(z) − g(z'))^2$が$L$に与える影響は大きいですが、一方で$z$が$x$と大きく異なれば（=近傍が入力画像と大きく異なる）、$(f(z) − g(z'))^2$が$L$に与える影響が小さくなります。 より$x$に近い$z$に関しては$g(z')$が良く$f(x)$に近似されるべきですので、このように重み付けされているのは分かる話かと思います。\nなお論文中ではLasso回帰として${\\rm argmin_{g \\in G}} \\ L(f, g, \\pi_x) + \\Omega(g)$を解いています。\nLIMEの実験結果 実験結果は他の方のブログなどで散々書かれていますので、そちらを参考ください（力尽きました）。\n","date":"2020-02-12T00:23:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/1341b73a17da593ffc43cebc86969604_hu43488a00ab78527cedf1030543e53948_126524_120x120_fill_q75_box_smart1.jpg","permalink":"https://opqrstuvcut.github.io/blog/posts/%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E4%BA%88%E6%B8%AC%E7%B5%90%E6%9E%9C%E3%82%92%E8%AA%AC%E6%98%8E%E3%81%99%E3%82%8Blime%E3%81%AE%E7%90%86%E8%AB%96/","title":"モデルの予測結果を説明するLIMEの理論"},{"content":"本記事はQrunchからの転載です。\n Uberが公開している機械学習モデルの予測と特徴量の関係性を可視化するツールであるManifoldを紹介します。\nManifoldを試す Manifoldでできることを見ていきます。\nインストール レポジトリをgit cloneしてから、githubのページにあるように以下のようにしてインストールできました。\n# under the root directory, install all dependencies yarn # demo app is in examples/manifold directory cd examples/manifold # instal demo app dependencies yarn # start the app npm run start 準備 まずユーザーは次の3つのデータを用意します。\n 入力データの特徴量を記述したcsv 入力データに対するラベル 入力データに対するモデルの予測値（分類問題の場合には各クラスに属する確率になります）  モデルはなんでも良く、必要なのは予測値であることに注意してください。\n今回はkaggleのタイタニックのデータから適当にテストデータを作ってみました。\u0008 テストデータとlightgbmのモデルを用いて、次のような感じでManifoldに必要なデータを作ってます。\nwith open(\u0026#34;./titanic_res/features.csv\u0026#34;, \u0026#34;w\u0026#34;) as f: columns = \u0026#34;,\u0026#34;.join(list(X_test.columns)) # X_testがテストデータの特徴量 f.write(f\u0026#34;{columns}\\n\u0026#34;) for i, features in X_test.iterrows():　f_string = \u0026#34;,\u0026#34;.join([str(x) for x in features]) f.write(f\u0026#34;{f_string}\\n\u0026#34;) with open(\u0026#34;./titanic_res/pred.csv\u0026#34;, \u0026#34;w\u0026#34;) as f: pred = bst.predict(X_test) # bstがlightgbmのモデル f.write(\u0026#34;survived,death\\n\u0026#34;) for prob in pred: f.write(f\u0026#34;{prob},{1-prob}\\n\u0026#34;) with open(\u0026#34;./titanic_res/truth.csv\u0026#34;, \u0026#34;w\u0026#34;) as f: f.write(\u0026#34;truth\\n\u0026#34;) for truth in y_test: # y_testがテストデータのラベル label = \u0026#34;survived\u0026#34; if truth == 1 else \u0026#34;death\u0026#34; f.write(f\u0026#34;{label}\\n\u0026#34;) Manifoldでの可視化 アップロード npm run startを実行すると、ブラウザ上でアプリが立ち上がります。 立ち上げ直後はファイルのアップロードを促されるので、準備したファイルをドラッグアンドドロップしてアップロードします。 性能の分布 アップロードすると、次のような画面になります。 Manifoldでは予測値の当たり具合によって、自動で各データがsegmentに分けられています。 各segmentはlog lossの値をK-meansに適用することでできたクラスタになっています。\nグラフの横軸がlog lossとなっており、segmentにわけてデータの個数が描画されています。 segment0に含まれるデータは性能が良く、segment3に含まれるデータは性能が悪いという見方になります。\nまた各segmentはGroup0かGroup1に振り分けられます。Group0が性能が悪く、Group1が性能が良いです。\n各segmentがどのGroupに入るのかはユーザー側で変えることが可能です。\n特徴量の分布 ManifoldではGroup毎の特徴量の分布の違いを見ることができます。 一番上の行がGroupによる性別の分布の違いをあらわしたもので、そこにマウスをもってくると次のようになります。 性別が男性である場合にはGroup1に入っている（性能が良い）データが46個、Group0に入っている（性能が悪い）データが7個となっています。 また、マウスを右側にもっていくと次のようになります。 これによると、性別が女性である場合にはGroup1に入っている（性能が良い）データは29個、Group0に入っている（性能が悪い）データは8個となっています。 よって、女性のほうが予測が上手くいっていないことになります。 タイタニック号に乗っていた女性は男性よりも優先されて救命ボートに乗っていました。男性である場合にはほぼボートに乗れず、そのような人たちは生き残らないため、予測が容易という解釈になるのかなと思います。\n一方で女性のなかでもボートに乗れるかどうかは他の要素によって左右されるため、予測が男性に比べると難しいという解釈かと思います。\n今は一番上が性別、2行目が年齢となっています。この並びの順番ですが、KL-divergenceによって求められた「2つのGroupでの特徴量の分布」が違う順になっています。 下にいくほどGroup間での違いがない特徴量であることを示します。\nどう使うべきか？ Manifoldで出来ることは、予測が上手くいっていないデータの傾向を可視化することです。 どうしてモデルの予測が上手くいっていないのかを説明すること、またモデルを改善することに役立てることができるのではないかと思います。\n","date":"2020-01-28T22:52:36+09:00","image":"https://opqrstuvcut.github.io/blog/posts/uber%E8%A3%BD%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%84%E3%83%BC%E3%83%ABmanifold/2f11d99ea2c3cf569c040d9555f5ab2c_hu0d0289d789c0c959994f816a625d3cae_309272_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/uber%E8%A3%BD%E3%81%AE%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E3%83%87%E3%83%90%E3%83%83%E3%82%B0%E3%83%84%E3%83%BC%E3%83%ABmanifold/","title":"Uber製の機械学習モデルのデバッグツールManifold"},{"content":"本記事はQrunchからの転載です。\n 吹き出しのライブラリ Flutterで吹き出しを出すためのライブラリとしてBubbleがあります。こちらを使うと吹き出しを簡単に表示できます。 もう一つSpeechBubbleというライブラリもありますが、Bubbleのほうが色々オプションが設定できます。\nBubble Bubbleを使うと以下のような吹き出しが簡単に表示できます。\n  最もシンプルな吹き出しの作り方は以下のようになります。\nBubble( nip: BubbleNip.leftTop, child: Text(\u0026#39;Hi, developer!\u0026#39;), ) Bubbleのオプション Bubbleでは次がオプションとして選べます。\n 吹き出しの色 吹き出しの形状 吹き出しからちょこんと出ているところの位置 影 マージン、パディング  欲しい機能は一通り揃っていてとても便利です。詳細はBubbleのgithubのページをご覧ください。\nBubbleの不満 素晴らしいライブラリなのですが、ちょっとだけ不満があります。 吹き出しからちょこんと出ているやつ（なんというか知らないんですが）の位置が現状は左上、左下、右上、右下しか選べません。\nなので、forkして左中央に位置を指定できるようにしてみました。 https://github.com/opqrstuvcut/bubble\nこちらを使うと次のように吹き出しの左中央からちょこんとあれが出せます。 コードは以下の通り。\nBubble( nip: BubbleNip.leftCenter, child: Text(\u0026#39;ちょこんとでるのが左中央だよ\u0026#39;), ) ","date":"2020-01-28T00:29:30+09:00","image":"https://opqrstuvcut.github.io/blog/posts/flutter%E3%81%A7%E5%90%B9%E3%81%8D%E5%87%BA%E3%81%97%E3%82%92%E4%BD%9C%E3%82%8B/56ff1ce17d741bc7f6aeb54a9c567e76_hub4d832f83afaad986e804b9ab594a001_70782_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/flutter%E3%81%A7%E5%90%B9%E3%81%8D%E5%87%BA%E3%81%97%E3%82%92%E4%BD%9C%E3%82%8B/","title":"Flutterで吹き出しを作る"},{"content":"本記事はQrunchからの転載です。\n Matplotlibの凡例を外側に出したい人用に色々な例を書いておきます。\n次のような凡例の位置をいじらずに表示した状態からいじっていきます。\ndata = np.random.rand(10, 3) labels = [\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;] plt.plot(range(10), data, marker=\u0026#34;o\u0026#34;, linewidth=3) plt.legend(labels) plt.title(\u0026#34;title\u0026#34;) plt.ylabel(\u0026#34;y label\u0026#34;) plt.xlabel(\u0026#34;x label\u0026#34;) plt.show() 右上に表示 凡例の枠の上部をグラフの枠の上部にあわせて、右上に表示するときは以下のようにします。\nplt.legend(labels, loc=\u0026#39;upper left\u0026#39;, bbox_to_anchor=(1, 1)) 右中央に表示 凡例の上下の位置をグラフと揃えて、右に表示するときは以下のようにします。\nplt.legend(labels, loc=\u0026#39;center left\u0026#39;, bbox_to_anchor=(1., .5)) 上に表示 凡例の左右の位置をグラフと揃えて、上に表示するときは以下のようにします。 ncol=3とすることで横一列に3つ分のグラフの凡例を表示できます。\nplt.legend(labels, loc=\u0026#39;lower center\u0026#39;, bbox_to_anchor=(.5, 1.1), ncol=3) 下に表示 凡例の左右の位置をグラフと揃えて、下に表示するときは以下のようにします。\nplt.legend(labels, loc=\u0026#39;upper center\u0026#39;, bbox_to_anchor=(.5, -.15), ncol=3) 理屈 plt.legendの引数のlocに指定した凡例の箇所がbbox_to_anchorで指定した座標になるように位置が調整されます。ここで、座標はグラフの枠の左下が(0,0)で右上が(1,1)となります。 例1loc=\u0026lsquo;upper left\u0026rsquo;、bbox_to_anchor=(1, 1)であるときには、凡例の枠の左上（locがupper leftなので）が(1,1)になるように凡例が配置されます。\n例2loc=\u0026lsquo;lower center\u0026rsquo;、bbox_to_anchor=(0.5, 1.1)であるときには、凡例の枠の中央下（locがlower centerなので）が(0.5,1.1)になるように凡例が配置されます。\n","date":"2020-01-20T21:09:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/matplotlib%E3%81%AE%E5%87%A1%E4%BE%8B%E3%82%92%E5%A4%96%E5%81%B4%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%97%E3%81%9F%E3%81%84%E4%BA%BA%E3%81%B8/bc011ef843d7d97092a83521e65e6a15_hu444fc9dc9cc4f90ff4e3d49d636ee18f_32079_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/matplotlib%E3%81%AE%E5%87%A1%E4%BE%8B%E3%82%92%E5%A4%96%E5%81%B4%E3%81%AB%E8%A1%A8%E7%A4%BA%E3%81%97%E3%81%9F%E3%81%84%E4%BA%BA%E3%81%B8/","title":"Matplotlibの凡例を外側に表示したい人へ"},{"content":"本記事はQrunchからの転載です。\n Pythonのnamedtuple使ってますか？ 案外使っていない方が多いので、ご紹介しておきます。\nnamedtupleとは？ 通常のタプルはインデックス指定でのみ要素を参照します。一方で、NamedTupleはタプルの各要素を名前によって参照できます。\n例えばpというnamedtupleの要素にnameというものがあれば、次のようにして参照できます。\nname = p.name 他の部分はほとんど通常のタプルと同じと思って問題ありません。\nnamedtupleを使うメリット 要素に名前がつけられるようになっただけですが、私が思うメリットは以下の通りです。\n タプルのようなインデックスの指定では参照する要素を誤る可能性が出てきますが、名前で指定することで誤りを防ぐことができます。 タプルの各要素の意味がはっきりするのでコードの可読性がよくなります。 タプルを生成する箇所が複数あった場合に、要素の順番を誤ったり要素数を誤ったりすることがなくなります。  他にもいいところがあるかもしれませんね。\nnamedtupleの使い方 その1 使い方はそれほど難しくありません。以下のようにしてnamedtupleを定義できます。\nfrom collections import namedtuple Person = namedtuple(\u0026#34;Person\u0026#34;, [\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;sex\u0026#34;]) 上記により、Personのタプルが宣言できました。Personはnamedtupleの第二引数に指定されたnameとageとsexを要素にもつタプルです。ちなみに以下のようにリストではなく、スペース区切りの文字列で与えても同じ意味となります。\nPerson = namedtuple(\u0026#34;Person\u0026#34;, \u0026#34;name age sex\u0026#34;) 宣言したPersonというタプルを生成するには以下のようにします。\np = Person(\u0026#34;太郎\u0026#34;, 10, \u0026#34;男\u0026#34;) このpの要素の参照は以下のようにしてできます。\nprint(p.name, p.age, p.sex) # output: 太郎 10 男 簡単です！\nその2 （おそらく）Python3.6からは次のようにもnamedtupleが利用できます。\nfrom typing import NamedTuple class Person(NamedTuple): name: str age: int sex: str p = Person(\u0026#34;太郎\u0026#34;, 10, \u0026#34;男\u0026#34;) print(p.name, p.age, p.sex) # output: 太郎 10 男 個人的にはこの書き方のほうがぱっと見たときにわかりやすいような気がして好きです。 あと多分補完もこちらのほうが効きやすいのではと思っています。\nまとめ コードの可読性があがり、間違いも減るので、namedtupleの利用をオススメします！\n","date":"2020-01-06T21:57:05+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/python%E3%81%AEnamedtuple%E3%82%92%E4%BD%BF%E3%81%8A%E3%81%86/","title":"Pythonのnamedtupleを使おう"},{"content":"本記事はQrunchからの転載です。\n BERTのパラメータの数を減らしたモデルであるALBERTについての概要を書いていきます。\n参考論文：ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n問題意識 2018年に提案されたBERTは自然言語界隈では非常に上手くいった手法です。先程論文の引用数を見たら、もう3000を超えていまして、この数字を見てもよくわかります。\nBERTは高い性能で色々な問題に適用することができる汎用性の高いモデルですが、パラメータ数が多いという特徴があります。なんでパラメータ数が多いかといえば、全結合層が沢山使われるからです。これは内部的にはそれなりに大きな行列を沢山持っているような状態です。\nパラメータ数が多いことで以下のような問題が起こります。\n メモリにモデルが乗らない 計算量が多い（論文中で特に言われているのが、分散処理での通信のコストです。通信は遅いのであまりやりたくありません。）  また、パラメータ数を増やしていっても順調に性能が高まるわけではなく、逆に大きく性能を落とすことがあります。以下の表がそれを示しています。\nBERT-xlargeというのがBERT-largeよりも隠れ層のパラメータ数を多くしたものですが、RACEを解いたときのAccuracyが大きく下がっているのがわかります（過学習のように思われますが、過学習だと明確にわかるようなサインが出ていないと論文には書かれています）。\n提案手法 語彙の埋め込みの行列分解 英版のBERTでは30000の語彙が存在します。BERTではこの語彙の埋め込みベクトルの次元が隠れ層の次元と同じですので、BERT-largeの場合には30000×1024のサイズの行列をもつことになります。\nこれに対してALBERTでは行列を分解して、語彙の埋め込みベクトルのサイズと隠れ層のサイズを別にしてしまいます。具体的には、語彙の数を$V$、語彙の埋め込みベクトルの次元を$E$、隠れ層の次元を$H$としたとき、語彙の埋め込みベクトルの行列のサイズは$V \\times E$となり、それに$E \\times H$のサイズの行列を掛けて$H$次元の空間に射影するようにします。そうすることで、もともとパラメータ数が$O(V \\times H)$だったのが、$O(V \\times E + E \\times H)$となり、$E \\ll H$のときには大きくパラメータ数が削減されることになります。\nこのようにしてしまって問題ないかと疑問が出てきますね。\n語彙のベクトル自体は文脈に依存しないベクトルで、その後の隠れ層を経て文脈を考慮したベクトルへと変わっていきます。この文脈に依存しないベクトルが持つ情報は大きくなく、次元を隠れ層ほど大きくする必要がないため、上記のようにしても問題がないということのようです。\n層間のパラメータの共有 BERTではEncoderを何度も重ねる構造になっています。ALBERTでは各層の重みを共通にすることで、パラメータ数を大きく削減しています。\nNSPからSOPへの変更 BERTではMASKされたトークンを予測することと、与えられた2つの文が連続しているかどうかを予測するタスクであるnext-sentence prediction(NSP)を同時に解けるように学習していきます。\nNSPの学習のため、実際に連続した文を与えるケースとランダムに選ばれた2つの文を与えるケースを用意します。NSPの意図はBERTに文の一貫性の理解を促すためです。しかしながら、ランダムに選ばれた2つの文だと、そもそも文のトピックが異なるために、あまり文脈を理解できなくともNSPが解けてしまいます。NSPは問題が簡単すぎるということです。\nこれを修正するため、ALBERTではsentence-order prediction(SOP)を提案しています。\nSOPは2つの連続した文の順番がそのままの順番か、逆になっているかを予測する問題です。これを解けるようにすることで、文の一貫性をモデルが理解できるようになるだろうという狙いです。トピックによって判断することができず、NSPよりも難しい問題設定になっていますね。\n実験結果 実験で使われているALBERTのモデルは以下のとおりです。 ALBERTは隠れ層の次元が大きくてもBERTに比べて大きくパラメータ数が抑えられていますね。\nBERTとの比較 BERTとの比較実験です。 ALBERTではパラメータ数が減るだけではなく、性能も大きく向上しています。少しじゃなく結構良くなっている感じですね。 訓練時間の速度比が最後の列です。すべてBERTのxlargeに比べての速度比です。同じ隠れ層の大きさのBERTに比べれば速いですが、ALBERTのxlargeがBERTのlargeより速くなるというほどのスピードアップではないことに気をつけてください。\n他の手法と比較 XLNetやRoBERTaとの比較です。 大体のタスクにおいて、ALBERTの性能が高いことがわかります。\n感想 ALBERTはどれくらいのメモリや訓練時間が必要なのかが気になって読んでみました。 BERTに比べるとパラメータ数と訓練時間が減っていますが、まだまだ自分で学習をさせられるものではないなぁという印象です。\n","date":"2019-12-28T23:36:43+09:00","image":"https://opqrstuvcut.github.io/blog/posts/bert%E3%82%92%E8%BB%BD%E9%87%8F%E5%8C%96%E3%81%97%E3%81%9Falbert%E3%81%AE%E6%A6%82%E8%A6%81/2b1477117e8654c1b558183f0277acdf_hu9dc25053793792fcc48628a740947cda_243061_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/bert%E3%82%92%E8%BB%BD%E9%87%8F%E5%8C%96%E3%81%97%E3%81%9Falbert%E3%81%AE%E6%A6%82%E8%A6%81/","title":"BERTを軽量化したALBERTの概要"},{"content":"本記事はQrunchからの転載です。\n ディープラーニングのモデルに対する特徴量の寄与を求める方法の1つである、DeepLiftについて今回は説明します。\n参考文献：Learning Important Features Through Propagating Activation Differences\n従来法の問題点 DeepLiftを提案している論文では、以下の2つが従来手法の問題点として挙げられています。\nsaturation problem saturation problemは勾配が0であるような区間では寄与が0になってしまう問題です。 従来手法には勾配を利用する手法が多いですが、そのような手法ではsaturation problemが発生してしまいます。 以下の図をご覧ください。 図中の関数は$y = 1 - {\\rm ReLU(1 - x)}$で、この関数を1つのネットワークとして考えてみます。 この関数では$x \u0026lt; 1$では勾配が$1$となり、$x\u0026gt;1$では勾配が$0$になります。 入力が$x=0$の場合に比べれば、$x=2$の場合は出力値が1だけ大きくなるため、寄与は$x=0$の場合よりも大きくなって欲しいです。しかしながら、寄与=勾配$\\times$入力とする寄与の計算方法の場合、 $x = 0 $では残念ながら寄与が等しく0になってしまいます。 このようにReLUによって勾配が0になってしまうことは、Integrated Gradientsの提案論文のなかでも同様に問題として挙げられています。\ndiscontinuous gradients 2つ目に挙げられている問題がdiscontinuous gradientsです。これも下図をご覧ください。 左から、ネットワークをあらわしている関数$y={\\rm ReLU(x - 10)}$、その勾配、寄与=勾配$\\times $入力です。 このような関数に対しては計算される寄与値が$x=10$で不連続となり、$x=10$までは寄与が全く無いのに、$x=10$を超えると突然寄与の値が$10$を超えるようになります。 入力値のちょっとした差で寄与が大きく変わるのは良くないですね。\nDeepLift 前述した2つの問題を解決するDeepLiftのアイディアと適用結果について述べていきます。DeepLift以外にも、Integrated Gradientsがこれら2つの問題を解決していますが、求まった寄与が直感的ではない場合があります。このことは適用結果で示します。\nなお、DeepLiftで利用されているアイディアの1つとして、RevealCancel Ruleというものがありますが、書くのが大変になりそうなので省略します。\nDeepLiftのアイディア DeepLiftはIntegrated GradientsやSHAPと同様に、基準となる点を決めておき、そこから入力$x$がどれだけ異なるか、また基準点と$x$のネットワークの出力がどれだけ異なるかをもとにして寄与値を計算していきます。 この基準となる点を$x_1^0, \\cdots, x_n^0$としておきます。\nディープラーニングで使われる計算は線形変換と非線形変換の2つに分けられ、DeepLiftではこれによって次のように寄与の計算方法が変わってきます。\nLinear Rule まず線形変換の方からです。線形変換には全結合層、畳み込み層が該当します。\n入力（あるいはある隠れ層の出力）$x_1,\\cdots, x_n$から次の層のあるニューロン$y$が、重み$w_i$とバイバス$b$を用いて次のようにあらわされるとします。 $$y = \\sum_{i=1}^N w_i x_i + b$$ 基準点$x_1^0, \\cdots, x_n^0$でも同様に $$y^0 = \\sum_{i=1}^N w_i x_i^0 + b$$ となります。\nこのとき、基準点$x_1^0, \\cdots, x_n^0$に対して、$x_1,\\cdots, x_n$における$y$の変化量は $$ \\Delta y =\\sum_{i=1}^N w_i \\Delta x_i $$ となります。ここで$\\Delta y = y - y^0, \\Delta x_i = x_i - x_i^0$です。\nDeepLiftではこの変化量に着目し、各入力$x_i$に対する$y$への寄与度$C_{\\Delta x_i \\Delta y} $を計算していきます。具体的には次のようになります。 $$ C_{\\Delta x_i \\Delta y} = w_i \\Delta x_i .$$\nつまり、入力$x_i$が基準点に比べてどれだけ$y$の変化に影響を及ぼしたかによって寄与が決まります。\nRescale Rule 次に活性化関数で用いられる非線形変換を扱っていきます。 非線形変換のときも線形変換の場合と同様にして考え、基準点に対するニューロンの出力からどれだけ変化を及ぼしたかによって、寄与を決定します。 ただしReLUやtanhなどは1変数$x$を入力としますから、線形変換の場合とは異なり、 $$C_{\\Delta x \\Delta y} = \\Delta y $$です。\nsaturation problemとdiscontinuous gradientsの解決 Linear RuleとRescale Ruleの2つを定義しましたが、このルールに則って寄与を計算することで、前述した2つの問題を解決することができます（どちらもRescale Rule絡みになりますが）。\nsaturation problem以下の図のように、DeepLiftでは勾配が0になる状況でも寄与は0になりません。 discontinuous gradients以下の3列目がDeepLiftでの寄与をあらわしたグラフです。DeepLiftでは寄与が不連続になりません。 非常に単純なアイディアですが、問題にあがっていた2つを解決することができました。\n連鎖律 ここまでで扱ってきた内容は、入力を線形変換したときの寄与、あるいは入力を非線形変換したときの寄与の計算になります。 それでは、入力に線形変換と非線形変換を順番に適用するときには、入力の最終的な出力に対する寄与はどのようにして求めると良いでしょうか。またディープラーニングのように層が複数あるようなケースではどうやって計算すれば良いでしょうか。 DeepLiftでは次のmultiplierとそれに対する連鎖律を導入することで、この計算を可能にしています。\nまず、multiplier $m_{\\Delta x \\Delta y}$の定義は以下のようになります。 $$ m_{\\Delta x \\Delta y} = \\frac {C_{\\Delta x \\Delta y}}{\\Delta x}.$$ これは$\\partial y/ \\partial x$と似たような形式になっています。特にRescale ruleのときには$C_{\\Delta x \\Delta y}=\\Delta y$ですから、意味合いは近いものがあります。\n次に連鎖律の定義です。 ネットワークへの入力を$x_1,\\cdots,x_n$、隠れ層のニューロンを$y_1,\\cdots, y_\\ell$、出力層のある1つのニューロンを$z$とします。このとき、multiplierに対して次のように連鎖律を定義します。 $$ m_{\\Delta x_i \\Delta z} = \\sum_{j=1}^\\ell m_{\\Delta x_i \\Delta y_j} m_{\\Delta y_j \\Delta z}.$$\nこれは丁度ディープラーニングでの計算で使われる連鎖律と同じものです。つまり、 $$ \\frac{\\partial z}{\\partial x_i} = \\sum_{j=1}^\\ell \\frac{\\partial z}{\\partial y_j} \\frac{\\partial y_j}{\\partial x_i} $$ と同じ形式です。 ただし、multiplierの連鎖律は導かれるものではなく、定義であることに注意が必要です。\nmultiplierの連鎖律を使うことで、backpropagationのようにして任意の層に対する任意の層へのmultiplierが求まります。こうして求まったmultiplierに対して基準点からの差をかけ合わせれば寄与が求まります。さきほどの連鎖律の話に出てきた変数の定義をそのまま使うと、 $$ C_{\\Delta x_i \\Delta z} = m_{\\Delta x_i \\Delta z} \\Delta x_i $$ が$x_i$が$z$への寄与になります。\nDeepLiftの適用結果 MNISTに適用した結果を示します。 1つの行が1つの手法をあらわしています（DeepLiftはRevealCancelとありますが、これは今回説明を省いたアイディアです）。1列目がオリジナルの画像で、2列目がCNNによって計算された「8」である確率への寄与でをあらわします。明るい部分が正の寄与で、暗いところが負の寄与になります。ちなみに基準点となる入力は全ピクセル値を0とした真っ黒な画像です。3列目は「3」である確率への寄与です。また4列目はオリジナルの画像から「3」である確率への寄与が高いピクセルを抜き出しているものです。 上2つの手法はピクセル間での寄与の差があまり明確ではありません。また4列目をみてみると、勾配と入力の積を寄与とした方法やIntegrated Gradientsよりも、「3」と判定するために必要なピクセルへはっきりと高い寄与を割り当てることができています。\nDeepLiftとIntegrated Gradients DeepLiftとIntegrated Gradientsは論文の中でお互いの問題点を指摘しあっています。\n DeepLiftの提案論文の主張： Integrated Gradientsは直感的でない寄与の割当がおこる。\n  Integrated Gradientsの提案論文の主張： DeepLiftはmultiplierの連鎖律の部分が数学的に問題がある。\n SHAPでも上記2つの手法を利用した計算が可能です。どちらが良いのかは悩ましいですが、結果が直感的になりやすいのはDeepLift、数学的に理論がしっかりしているのがIntegrated Gradientsという感じでしょうか（あとは実装しやすいのはIntegrated Gradientsとか計算量が少ないのはDeepLiftなどの観点もありますね）。\n","date":"2019-12-19T02:03:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/64a618e3bf36cb2953ac208966e42b90_hu1d3b8c411e821887d60eecf621421a88_489801_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E3%83%A2%E3%83%87%E3%83%AB%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bdeeplift/","title":"ディープラーニングのモデルの特徴量の寄与を求めるDeepLift"},{"content":"本記事はQrunchからの転載です。\n FlutterでS3へファイルをアップロードするための公式のライブラリはありませんが、有志によるライブラリamazon_s3_cognitoがあります。 今回はこちらの紹介+forkしてちょっと修正したのでよければ使ってねという話になります。\n事前準備 AWS cognitoでIDプールを作っておく必要があります。 cognitoのページを開くと以下のような表示がされるので、「IDプールの管理」を押します。 新しいIDプールの作成を押し、以下のような感じで設定をします。 次のページでRoleのポリシーの設定ができますので、「詳細を表示」 -\u0026gt; 「ポリシードキュメントを表示」 からポリシーを編集します。Uauthと書いてある方だけ編集すればOKです。 ポリシーは以下のようにすれば大丈夫ですが、バケット名は自分で適当なものに変更してください。\n{ \u0026quot;Version\u0026quot;: \u0026quot;2012-10-17\u0026quot;, \u0026quot;Statement\u0026quot;: [ { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor0\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: [ \u0026quot;mobileanalytics:PutEvents\u0026quot;, \u0026quot;cognito-sync:*\u0026quot; ], \u0026quot;Resource\u0026quot;: \u0026quot;*\u0026quot; }, { \u0026quot;Sid\u0026quot;: \u0026quot;VisualEditor1\u0026quot;, \u0026quot;Effect\u0026quot;: \u0026quot;Allow\u0026quot;, \u0026quot;Action\u0026quot;: \u0026quot;s3:*Object\u0026quot;, \u0026quot;Resource\u0026quot;: \u0026quot;arn:aws:s3:::(バケット名)*\u0026quot; } ] } おそらくこれでAWS側の設定は大丈夫かと思います。\nFlutter側からファイルを送信する amazon_s3_cognitoをpubspec.yamlに追加して、flutter pub getしたら使う準備はできました。 次のようなコードでファイルをS3に送ることができます。\nimport 'package:amazon_s3_cognito/amazon_s3_cognito.dart'; import 'package:amazon_s3_cognito/aws_region.dart'; String uploadedImageUrl = await AmazonS3Cognito.upload( imagePath, BUCKET_NAME, IDENTITY_POOL_ID, IMAGE_NAME, AwsRegion.AP_NORTHEAST_1, AwsRegion.AP_NORTHEAST_1)  imagePathはスマートフォン内の送りたいファイルのパスを指定します。 BUCKET_NAMEはS3のバケット名を指定します。 IDENTITY_POOL_IDはさきほど設定したAWS cognitoから次のような詳細ページにいくことで、取得できます。以下のIDプールのIDと書かれている行のダブルクォーテーションの部分をコピペすればOKです。  IMAGE_NAMEはS3のバケット以下のファイルの保存先のパスを指定します。 AwsRegion.AP_NORTHEAST_1はregionを指定しています。2つ目はsub region？の設定らしいですが、なければ同じもので特に問題ありません。  返り値はS3上の保存先のファイルパスになります。失敗したときは\u0026quot;Failed\u0026quot;だったり空のパスが渡ってきます。\nあれ、iOSでは失敗する… Androidではここまでの設定等でうまくいったのですが、iOSでは常にうまく送信できませんし、空のパスが返り値として受け取られます。 実はこれはアクセス権の問題でうまく動きませんでした。iOS版の実装をみると、publicなバケットにしかファイルを送れないようになっていました。 というわけで、amazon_s3_cognitoのレポジトリをforkして、privateなバケットにもファイルを送れるように修正しましたので、よければ使ってみてください。 https://github.com/opqrstuvcut/amazon_s3_cognito\npubspec.yamlには以下のようにかけばOKです。\n amazon_s3_cognito: git: url: https://github.com/opqrstuvcut/amazon_s3_cognito ","date":"2019-12-08T19:04:32+09:00","image":"https://opqrstuvcut.github.io/blog/posts/flutter%E3%81%A7s3%E3%81%AB%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B/5156acb6c29b977915456e21c1d96fb8_hua0e9ef26a9d3ab210e1922bc431eb065_258199_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/flutter%E3%81%A7s3%E3%81%AB%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB%E3%82%92%E3%82%A2%E3%83%83%E3%83%97%E3%83%AD%E3%83%BC%E3%83%89%E3%81%99%E3%82%8B/","title":"FlutterでS3にファイルをアップロードする"},{"content":"本記事はQrunchからの転載です。\n 機械学習のモデルの出力に対する入力された特徴量の寄与を求める手法の1つに、Integrated Gradientsというものがあります。 Integrated Gradientsはディープラーニング向けの手法ですが、他のディープラーニング向けの手法では満たしていない公理（性質）をいくつも満たしているという点で優れています。 今回はそんなIntegrated Gradientsを解説します。\n参考論文：Axiomatic Attribution for Deep Networks\n先にbaselineのお話 本題に入る前に、大事な考え方であるbaselineを説明しておきます。\n人間が何か起こったことに対して原因を考えるとき、何かの基準となる事がその人の中にはあり、それに比べ、「ここが良くない」とか「ここが良かったから結果としてこういう結果になったんだな」、と考えるんじゃないでしょうか。 Integrated Gradientsの場合もその考え方を用います。 先程の例の基準がbaselineと呼ばれ、画像のタスクでは例えば真っ黒の画像が使われたり、自然言語のタスクではすべてを0にしたembeddingが使われたりします（これは手法によって異なります）。つまり、真っ黒の何も写っていない画像に比べて猫の写った画像はこういう風に異なるから、これは猫の画像と判断したんだな、というように考えていくことになります。\n2つの公理 特徴量の寄与を求める既存手法の中でも勾配を用いた手法というのは多いです。しかしながら、論文中では勾配を用いた既存手法には問題があると指摘しています。 例えばGuided back-propagationは次のSensitivity(a)を満たしていませんし、DeepLiftはImplementation Invarianceを満たしていません。\nSensitivity(a) Sensitivity(a)の定義は以下のとおりです（ちなみにaと書いてあるのはbもあるということです。詳しく知りたい方は論文を参照ください）。\n Sensitivity(a): 入力値に対する出力がbaselineの出力と異なったとき、baselineと異なる値をもつ入力の特徴量の寄与は非ゼロである。\n 次のような例を考えると、勾配を用いる手法におけるSensitivity(a)の必要性がわかります。 $f(x) = 1 - {\\rm Relu}(1-x)$というネットワークを考えます。baselineが$x=0$、入力値が$x=2$とします。$f(0)=0$、$f(2)=1$となりますのでbaselineとは出力値が変わっています。しかしながら、$x=2$では勾配が$0$になりますので、例えば「勾配×入力値」で寄与を求める場合、寄与も$0$になります。 baselineに比べて出力値が変わったのに、寄与が$0$というのはおかしい結果だというのは納得いく話かなと思います。 このため、Sensitivity(a)は寄与を求める手法として満たすべきものだと著者は主張しています。\nImplementation Invariance Implementation Invarianceの定義は以下のとおりです。\n Implementation Invariance: 実装方法が異なっていても、同じ入力に対しては求まる寄与値は等しい。\n 具体例を次に示します。\nImplementation Invarianceの例例えば勾配${\\partial f}/{\\partial x}$を計算する手法の場合、この計算は隠れ層の出力$h$を使って、 $$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial h}\\frac{\\partial h}{\\partial x}$$ とあらわせます。 勾配を求める際に${\\partial f}/{\\partial x}$を直接計算しても、連鎖律を使って右辺の計算を用いても結果は一緒になります。 このケースはImplementation Invarianceを満たします。\nImplementation Invarianceではない例DeepLiftの場合は離散化した勾配を用いて寄与を計算します。 連続値を扱っている限りは連鎖律が成り立ちますが、離散化すると連鎖律が成り立たなくなります。 つまり、 $$ \\frac{f(x_1) - f(x_0)}{x_1 - x_0} \\neq \\frac{f(x_1) - f(x_0)}{h(x_1) - h(x_0)} \\frac{h(x_1) - h(x_0)}{x_1 -x_0}$$ となります。 このように計算方法（実装方法）によって結果が変わる場合はImplementation Invarianceを満たしません。\nImplementation Invarianceを満たさないことの問題点 Implementation Invarianceを満たさないことの問題点って何なんでしょうか。論文中で指摘されているのは次のようなケースです。 あるモデルのパラメータ数が多いなどが理由で、自由度が非常に高いモデルがあるとします。このモデルを学習した結果として、同じ入力に対して出力が同じになるが、パラメータの値が異なる組み合わせであるような学習済みモデル1とモデル2の2つが得られたとします。 このような状況で2つのモデルに対する離散化された勾配は、モデル1の隠れ層を$h_1$、モデル2の隠れ層を$h_2$としたとき、 $$ \\frac{f(x_1) - f(x_0)}{h_1(x_1) - h_1(x_0)} \\frac{h_1(x_1) - h_1(x_0)}{x_1 -x_0} \\neq \\frac{f(x_1) - f(x_0)}{h_2(x_1) - h_2(x_0)} \\frac{h_2(x_1) - h_2(x_0)}{x_1 -x_0}$$ となりえます。なぜかといえば、Implementation Invarianceではないので、どちらも$ {f(x_1) - f(x_0)}/{x_1 - x_0}$とは異なる何らかの値になるためです。 入力と出力が同じであるのに、モデルによって寄与が異なるというのは確かに違和感がありますね。たしかにImplementation Invarianceも満たすべきであるといえそうです。\nIntegrated Gradients Integrated Gradientsは前述した2つの公理を満たす手法になります。\nアルゴリズム 手法は単純で、また実装も簡単です。 Integrated Gradientsではbaseline $x'$から入力$x$までの勾配を積分し、入力とbaselineとの差と積を取るだけです。式であらわすと以下のようになります。 $$ {\\rm Integrated\\ Gradients} = (x - x') \\int_{0}^{1} \\nabla F(x' + \\alpha(x - x')){\\rm d} \\alpha .$$\n上記のように勾配の積分を寄与とすることで、baselineから入力$x$までの勾配をすべて考慮することができ、その結果としてSensitivity(a)を満たすことになります。またIntegrated Gradientsは勾配と積分から成りますので、Implementation Invarianceも満たされます。\nコンピュータ上では上記の積分をそのまま実行することはできませんので、実際には数値積分をして、近似値を求めることになります。数値積分を厳密にやろうとするほど、計算量が多く掛かることに注意してください。\nIntegrated Gradientsの適用結果 画像の分類問題の例 GoogleNetを使って画像の分類問題を学習させ、それにIntegrated Gradientsを適用した結果が以下のとおりです。\n一番左が入力画像で、その隣にラベルとスコアが書いてあり、3列目がIntegrated Gradients×入力画像、4列目が勾配×入力画像です。 これを見ると、単に勾配を用いる場合に比べて、物体を認識するのに必要そうな箇所が寄与していると判定されていることがわかります。例えば、2行目のfireboatは勾配の場合よりも、Integrated Gradientsのほうがより水しぶきの細かい部分に着目していると判定できています。\nテキストの分類問題の例 次にテキストの分類問題の例です。 質問の答えがどういった種類の回答かを予測する問題です。例えば答えが数値なのか、日付なのかなどを予測します。\n下が予測モデルにIntegrated Gradientsを適用した結果です。 赤いほど予測への寄与が大きい単語となっています。 結果を見てみると、疑問詞やyearなどに着目していることがわかります。それらしい結果になっていますね。\nおわりに Integrated GradientsではDeepLiftのココがダメと言及している一方で、DeepLiftはIntegrated Gradientsの性能が低いと指摘しています。使う側は難しいですね。\n","date":"2019-12-08T16:17:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/33ce0e44d5ce1595ba0980aaa9a27c83_hua0d4d4b24f4da3a102e4452ac738b95f_358238_120x120_fill_q75_box_smart1.jpg","permalink":"https://opqrstuvcut.github.io/blog/posts/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%A9%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E5%90%91%E3%81%91%E3%81%AE%E7%89%B9%E5%BE%B4%E9%87%8F%E3%81%AE%E5%AF%84%E4%B8%8E%E3%82%92%E6%B1%82%E3%82%81%E3%82%8Bintegrated-gradients%E3%81%AE%E8%A7%A3%E8%AA%AC/","title":"ディープラーニング向けの特徴量の寄与を求めるIntegrated Gradientsの解説"},{"content":"本記事はQrunchからの転載です。\n CNNの表現能力の高さはすばらしいものがありますが、何でもうまくいくわけではありません。例えば、画像中の位置情報を考慮しないと解けないような問題は、通常のCNNではうまく対応できません（具体的な例はこの後説明します）。\nこのような問題に対応した手法としてCoordConvというものがあります。CoordConvは座標情報をCNNのなかに組み込む手法で、これを使うことで解けるようになるケースや性能が大きく改善されるようなケースがあります。また「効くか分からないけど、とりあえず組み込む」ということをしても、デメリットはそれほどありません。\n今回はこのCoordConvの紹介です。\n論文：https://arxiv.org/pdf/1807.03247.pdf Keras実装：https://github.com/titu1994/keras-coordconv\nPyTorch実装：https://github.com/mkocabas/CoordConv-pytorch\nちなみに、Keras実装は使ったことがありますが、いい感じに仕事してくれました。\n通常のCNNだと解けない問題 解けない問題の紹介 以下の図は論文で示されている、通常のCNNではうまく解けない、あるいは性能が悪い問題設定です。\n   Supervised Coordinate Classification は2次元座標xとyを入力として2次元のグレイスケールの画像を出力する問題です。入力の(x,y)の座標に対応するピクセルだけが1、それ以外のところは0になるように出力します。出力されるピクセルの数の分類問題となります。 Supervised Renderingも画像を出力しますが、入力(x,y)を中心とした9×9の四角に含まれるピクセルは1、それ以外は0になるように出力します。 Unsupervised Density LearningはGANによって赤か青の四角と丸が書かれた画像を出力する問題となります。 上記の画像にはないのですが、Supervised Coordinate Classification の入力と出力を逆にした問題も論文では試されています。つまり、1ピクセルだけ1でそれ以外は0であるようなone hot encodingを入力として、1の値をもつピクセルの座標(x,y)を出力するような問題です。  Supervised Coordinate Classificationを通常のCNNで学習させた結果 Supervised Coordinate Classificationを通常のCNNで学習させたときの結果を示します。\n訓練データとテストデータの分け方で2種類の実験をおこなっています。\n1つは取りうる座標全体からランダムに訓練データとテストデータに分けたケースです。もう一つは座標全体のうち、右下の部分をテストデータにし、それ以外を訓練データとするケースです。これをあらわしたのが、それぞれ以下の図のUniform splitとQuadrant splitになります。\n  上記の2つのパターンでそれぞれ訓練データでCNNを訓練し、accuracyを計測した結果が以下の図になります。\n  1つの点が1つの学習されたモデルでの訓練データとテストデータのaccuracyに対応しています（多分それぞれのモデルはハイパーパラメータが異なるのですが、はっきりと読み取れませんでした）。\nこのグラフから、Uniform splitのときには訓練データのaccuracyは1.0になることがあっても、テストデータは高々0.86程度にしかならないことがわかります。また、Quadrant splitのときにはさらにひどい状況で、テストデータはまったく正解しません（ほとんど0ですね）。\n問題設定を見ると、一見簡単な問題のように思えますが、実際には驚くほど解きにくい問題であることがわかります。\nUnsupervised Density Learningを通常のCNNで学習させた結果 次にGANのケースも見てみます。\n学習データでは青の図形と赤の図形はそれぞれ平面上に一様に分布します。下図の上段右がそれを示しており、赤の点と青の点がそれぞれの色の図形の中心位置をプロットしたものです。GANで生成する画像もこのように、図形が一様に色々なところに描かれて欲しいところです。\nしかしながら、CNNを使ったGANのモデルが生成した画像では赤の図形と青の図形の位置の分布には偏りがあります（モード崩壊）。下図の下段右がこれを示しています。 CoordConv 前述の問題はなぜ解きにくいのでしょうか。\n理由としては、CNNでは畳み込みの計算をおこなうだけであり、この畳み込みの計算では画像中のどこを畳み込んでいるのかは考慮できておらず、座標を考慮する必要がある問題がうまく解けないということが挙げられます。\n座標を考慮できていないから解けないならば、畳み込むときに座標情報を付与すればよいのでは、というのがCoordConvの発想です。\n具体的には以下の右の層がCoordConvになります。 通常のCNNとの違いは、画像の各ピクセルのx軸の座標をあらわしたチャネル（i coordinate）とy軸の座標をあらわしたチャネル（j coordinate）を追加するということだけです。ただし、それぞれのチャネルの値は[-1,1]に正規化されています。\n例えば、5×5の画像の場合では、x軸の座標をあらわしたチャネル（i coordinate）は以下のような行列になります。\n$$ {\\rm (i \\ coordinate)} = \\begin{bmatrix} -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ -1 \u0026amp; -0.5 \u0026amp; 0 \u0026amp; 0.5 \u0026amp; 1 \\\\ \\end{bmatrix} $$\nまた、y軸の座標をあらわしたチャネル（j coordinate）は以下のような行列になります。\n$${\\rm (j \\ coordinate)} = \\begin{bmatrix} -1 \u0026amp; -1 \u0026amp; -1 \u0026amp; -1 \u0026amp; -1 \\\\ -0.5 \u0026amp; -0.5 \u0026amp; -0.5 \u0026amp; -0.5 \u0026amp; -0.5 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0.5 \u0026amp; 0.5 \u0026amp; 0.5 \u0026amp; 0.5 \u0026amp; 0.5 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 1 \\\\ \\end{bmatrix} $$\nまた、画像の中心からの距離をあらわしたチャネルを追加することでも性能が向上するようで、論文ではこちらも利用されています。\nCoordConvによってすべてのCNNを代替すべきなのか、一部にしてもどこを置き換えるべきなのかは議論の余地があるかもしれませんが、例えばSupervised Coordinate Classificationの問題では、次の緑の部分にCoordConvが使われています。 ちなみにCoordConvの性能面に関して、論文中では以下の2点に関して言及されています。\n 追加されたチャネル分の畳み込みが増えるだけですので、それほど計算量は大きくなりません。 CoordConvによる性能の悪影響があり得るんじゃないかと思えますが、そのような場合には重みが0に近づくように学習されるはずなので、予測結果に悪影響を与えないはず。  CoordConvの効果 CoordConvの効果について実験結果を述べていきます。\nSupervised Coordinate Classificationの結果 Supervised Coordinate Classificationの結果が以下のようになります。 Convolutionの行が通常のCNNの場合、CoordConvの行がCoordConvを使った場合の結果です。\nCoordConvではデータの分割方法によらず、accuracyが1になり、非常にうまく問題が解けるようになります。また、収束性も非常によくなり、通常のCNNでは4000秒かかってテストデータのaccuracyが0.8を超えていますが、CoordConvでは20秒でaccuracyが1になっています。\nUnsupervised Density Learningの結果 GANの結果が以下のようになります。 3行目のCoordConvを導入したGANでは赤と青の図形の位置の偏りが緩和されていることがわかります。 ただしc列が2つの図形の中心座標の差を示しているのですが、これを見ると、残念ながらまだ分布に偏りがあるといえそうです。\n強化学習の結果 論文では実際の問題にも適用して有効性を確認しています。 強化学習で使われているCNNをCoordConvに置き換えてatariのゲームを学習させています。結果は以下の通りです。 縦軸がゲームのスコアだと思いますが、9つのゲームのなかで、6つは性能が向上し、2つは変わらず、1つは悪くなった（理屈の上では悪影響がでないはずですが…？）という結果になりました。パックマンなど一部のゲームは非常に性能が良くなっていますね。\n終わりに CoordConvは問題によっては非常に有用です。座標を考慮したほうがが良いと思ったら、とりあえず利用するといいと思います。 既存のCNNをCoordConvに置き換えるのも簡単です！\n","date":"2019-11-30T21:57:17+09:00","image":"https://opqrstuvcut.github.io/blog/posts/cnn%E3%81%A7%E7%94%BB%E5%83%8F%E4%B8%AD%E3%81%AE%E3%83%94%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AE%E5%BA%A7%E6%A8%99%E6%83%85%E5%A0%B1%E3%82%92%E8%80%83%E6%85%AE%E3%81%A7%E3%81%8D%E3%82%8Bcoordconv/87ac6257d733ab494c7d120ec4e79a99_huad0610d545df126cbc0dbaec61a8f428_203414_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/cnn%E3%81%A7%E7%94%BB%E5%83%8F%E4%B8%AD%E3%81%AE%E3%83%94%E3%82%AF%E3%82%BB%E3%83%AB%E3%81%AE%E5%BA%A7%E6%A8%99%E6%83%85%E5%A0%B1%E3%82%92%E8%80%83%E6%85%AE%E3%81%A7%E3%81%8D%E3%82%8Bcoordconv/","title":"CNNで画像中のピクセルの座標情報を考慮できるCoordConv"},{"content":"本記事はQrunchからの転載です。\n 逆行列を使った計算というのは機械学習ではそれなりに出てきます。 例えば、最小二乗法では $$ x = (X^T X) ^{-1} Xb$$ の形の式を計算する必要がありますし、正規分布の分散を扱うときにも逆行列が出てきます。 こういうときにnp.linalg.invを使って逆行列を求めて、その後にベクトルとの積を求めるは簡単にできますから、特に何も考えずにそういうふうにしたくなります。\nでもそれって本当に逆行列の計算が必要ですか？多くの問題では逆行列の値そのものよりも、$x=A^{-1}b$のような逆行列とベクトルとの積が必要になります。そのような場合、実は計算はもっと速くできますよ、というのが今日のお話です。\nただし今回は式を深く追うことはしませんので、細かい計算量などが気になる方は別途どこかの講義資料などの参照をお願いします。\n逆行列を求めるための計算量 逆行列を求めるための方法として多くの人が思いつくのが、おそらく線形代数の教科書に載っている掃き出し法でしょう。掃き出し法は逆行列を求めたい行列$A$に対して操作をおこない、単位行列にしていくやり方ですね。 行列$A$のサイズを$n \\times n$としたとき、掃き出し法に必要な乗除算は$n^3$回、引き算は$n(n-1)^2$回です。 また別途、行列$b$との積を計算する場合には乗算が$n^2$回、足し算が$n(n-1)$回かかることに注意してください。\n実際にはnp.linalg.invはこの方法ではなく、後述する方法を利用して（半ば無理やり？）逆行列を求めますが、そうしても計算量は上記と同じ程度になります。\n連立一次方程式を解く方法 $x=A^{-1}b$の計算は、$Ax=b$の形をした連立一次方程式とみなすことができます（$x=A^{-1}b$の両辺に左から$A$を掛けるとわかりますね）。よって、連立一次方程式が解ければ、逆行列を求める必要はないということです。\n以下ではnp.linalg.solveでもおこなわれている、LU分解と前進後退代入を使った連立一次方程式の解き方について述べます。\nLU分解 行列$A$に対してLU分解をおこなうことを考えます。LU分解というのは下三角行列$L$と上三角行列$U$の積に行列$A$を分解することを指します。つまり、$$A = LU$$が成り立つような$L$と$U$を求めます。\nLU分解の計算量は乗除算が$(n-1)(n^2+n+3)/3$回で引き算が$n(n-1)(2n-1)/6$回です。ここまでは先程出てきた逆行列を求めるための計算量よりも大分少ない計算量です。\nもちろんLU分解だけでは連立一次方程式は解けず、次の前進後退代入をおこなう必要があります。\n前進後退代入 LU分解が済んでいるとすると、$Ax=b$は$LUx=b$とあらわせます。$y=Ux$とおいてあげると、 $$Ax=LUx= Ly=b$$ となりますので、$Ly=b$の連立一次方程式が出てきます。これを$y$について解くと次に $$Ux = y$$ の連立一次方程式があらわれます。最後にこれを$x$について解くことで、ようやく欲しかった$x$が求まります。\n$Ly=b$と$Ux=y$という連立一次方程式を解くなんて計算が重そうだ！と思うかもしれません。 しかしながら、$L$は下三角行列、$U$は上三角行列であるということを考慮するとそれほど計算量は多くなりません。実際、\n $Ly=b$を求める計算（前進代入）：乗算$n(n-1)/2$回、加減算$n(n-1)/2$回 $Ux=y$を求める計算（後退代入）：乗除算$n(n+1)/2$回、加減算$n(n-1)/2$回 上2つの計算量の和：乗除算$n^2$回、加減算$n(n-1)$回  となります。なんとこれは前述した$A^{-1}$を$b$に掛けるときの計算量と等しいです！ 一見大変そうな計算をしているのに、実は行列とベクトルの積と同じ計算量だなんて驚きです。\nLU分解と前進後退代入から逆行列を求める方法 np.linalg.invでは連立一次方程式の計算を利用して逆行列を求めるといいました。これは単位行列$E$を右辺とした連立一次方程式を解くことを指しています。つまり以下の方程式です（右辺と解$X$が行列になりますが、単純に列の分だけ解くべき方程式が増えたと思えばOKです）。 $$A X = E.$$ この方程式を解くと、$X = A^{-1}$となるのがわかりますね。\nこの方法の前進後退代入の計算量は乗除算$n(2n^2+1)/3$回、加減算$n(n-1)(4n-5)/6$回となります（この計算量の計算は結構大変…）。 LU分解の計算量との合計は乗除算が$n^3 + n- 1$回、加減算が$n(n-1)^2$回となります。掃き出し法と比べて乗除算が$n-1$回増えますが、$n$が大きくなれば無視できる程度の差です。\n計算量のまとめ 計算量についてまとめると、以下のようになります。\n   方法 乗除算 加減算     掃き出し法による逆行列の計算 $n^3$ $n(n-1)^2$   行列とベクトルの積 $n^2$ $n(n-1)$   LU分解 $(n-1)(n^2+n+3)/3$ $n(n-1)(2n-1)/6$   前進後退代入 $n^2$ $n(n-1)$   LU分解+前進後退代入による逆行列の計算 $n^3+n-1$ $n(n-1)^2$    LU分解と前進後退代入によって$Ax=b$を解いた場合の計算量では$n^3$に$1/3$がかかっていますから、「逆行列を求める+ベクトルとの積を計算する」の場合に比べて$1/3$程度計算量が減ることがわかります。\nNumPyで実験 実際にNumPyで計算時間を比較してみましょう。 以下のようにして行列とベクトルを作ります。\nimport numpy as np A = np.random.rand(1000, 1000) b = np.random.rand(1000) 次に、計算にかかった時間をそれぞれ測ります。\n 逆行列を求める+ベクトルとの積を計算  %%timeit inv_x = np.dot(np.linalg.inv(A), b) 結果：80.8 ms ± 4.29 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n 連立一次方程式を解く  %%timeit solve_x = np.linalg.solve(A, b) 結果：27.7 ms ± 1.21 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nおおよそ3倍くらいの差がつきましたね！\nまとめ 連立一次方程式の形に落とし込める場合には、逆行列を求めずに、連立一次方程式として解いてあげましょう！実は、計算量が減ることで数値誤差が増えづらくなり、精度面も連立一次方程式のほうが有利と言われています。色々な面で逆行列を計算するメリットはないのです。\n参考書籍：伊理 正夫、藤野 和建．数値計算の常識.\n","date":"2019-11-15T01:35:01+09:00","permalink":"https://opqrstuvcut.github.io/blog/posts/%E5%AE%89%E6%98%93%E3%81%AB%E9%80%86%E8%A1%8C%E5%88%97%E3%82%92%E6%95%B0%E5%80%A4%E8%A8%88%E7%AE%97%E3%81%99%E3%82%8B%E3%81%AE%E3%81%AF%E3%82%84%E3%82%81%E3%82%88%E3%81%86/","title":"安易に逆行列を数値計算するのはやめよう"},{"content":"本記事はQrunchからの転載です。\n AWSのS3を使うようなシステムを開発するときに、S3と連携する部分だけAWSにつなぐより、ローカルにS3が欲しいなぁってふと思いました。でもそんな都合が良い話があるわけないよなぁ、なんて思ったら実はありました！その名もMinIO。 今回はMinIOの使い方を簡単にご紹介します。とても簡単です。\nMinIOのページはこちら。https://min.io\n導入 自分はDockerを利用しましたので、Docker経由での使い方になります。 Dockerは嫌だという場合には公式のページをご確認下さい。https://docs.min.io/\n Dockerをインストール。 Dockerを入れていない人はこの機会にぜひ入れましょう！今使っていなくとも、きっといつの日か別の機会にも使うんじゃないかと思います。インストールにはこの辺が参考になりそうです。http://docs.docker.jp/engine/installation/docker-ce.html# ターミナル等で次を実行して、MinIOのサーバを立ち上げる。  docker run -p 9000:9000 \\ --name minio_test \\ -e \u0026quot;MINIO_ACCESS_KEY=access_key_dayo\u0026quot; \\ -e \u0026quot;MINIO_SECRET_KEY=secret_key_dayo\u0026quot; \\ minio/minio server /data MINIO_ACCESS_KEYがAWSのアクセスキーで、MINIO_SECRET_KEYはシークレットキーに対応します。都合がよいように決めましょう。\n上のコマンドの初回実行時にはdocker imageのdownloadなどが走るのでちょっと時間がかかります。\n（Dockerを知らない人向け）アクセスするときにポートが9000は嫌だという人は、9000:9000の左側の数字を変えましょう。例えば8888:9000とかです。\n実行がうまくいくと次のようなメッセージが表示されるかと思います。これでS3のようなものができました！すごく簡単\nhttp://127.0.0.1:9000 からMinIOのサーバにアクセスできるはずです。\nEndpoint: http://172.17.0.2:9000 http://127.0.0.1:9000 Browser Access: http://172.17.0.2:9000 http://127.0.0.1:9000 Object API (Amazon S3 compatible): Go: https://docs.min.io/docs/golang-client-quickstart-guide Java: https://docs.min.io/docs/java-client-quickstart-guide Python: https://docs.min.io/docs/python-client-quickstart-guide JavaScript: https://docs.min.io/docs/javascript-client-quickstart-guide .NET: https://docs.min.io/docs/dotnet-client-quickstart-guide 使ってみる ブラウザで利用 アクセス ブラウザで http://127.0.0.1:9000 にアクセスすると次のような画面が表示されます。\nAccess KeyとSecret Keyはdocker runコマンドのときに指定したMINIO_ACCESS_KEYとMINIO_SECRET_KEYの値を入れましょう。これでログインできます。\nログインすると以下のような画面になります。 バケット生成 ここでAWSのS3のバケット相当のものが作れます。\n右下の+マークを押して、Create bucketを選択後、バケット名を入力すればOKです。この手順で、例えばtestという名前のバケットを作ると以下のようになります。 左側に生成したバケットが表示されていますね。\nファイルを配置 画面の上のほうにファイルをドラッグするとこのバケット内にファイルが置けます。 boto3で利用 boto3を使ってS3の操作をおこないます。Python3を例にあげます。\nS3と接続するためのオブジェクト生成 以下のclientの引数のaws_access_key_idとaws_secret_access_keyには、docker runしたときに設定したMINIO_ACCESS_KEYとMINIO_SECRET_KEYをそれぞれ指定しましょう。\nimport boto3 s3_client = boto3.client(\u0026quot;s3\u0026quot;, endpoint_url=\u0026quot;http://127.0.0.1:9000\u0026quot;, aws_access_key_id=\u0026quot;access_key_dayo\u0026quot;, aws_secret_access_key=\u0026quot;secret_key_dayo\u0026quot;) ファイル検索 ファイルの検索を試します。以下のように、作ったBucketを指定してやります。\ncontents = s3_client.list_objects(Bucket=\u0026quot;test\u0026quot;, Prefix=\u0026quot;\u0026quot;).get(\u0026quot;Contents\u0026quot;) print(contents) 上記を実行すると、次のように表示されました。配置したファイルがちゃんと見つけられますね！\n[{'Key': 'テキストたよ.txt', 'LastModified': datetime.datetime(2019, 11, 9, 3, 7, 27, 360000, tzinfo=tzutc()), 'ETag': '\u0026quot;59681d790d132065f97faf0f52e9aa41-1\u0026quot;', 'Size': 27, 'StorageClass': 'STANDARD', 'Owner': {'DisplayName': '', 'ID': '02d6176db174dc93cb1b899f7c6078f08654445fe8cf1b6ce98d8855f66bdbf4'}}] ファイル取得 ファイルの取得も試しましょう。以下を実行します。\ntext = s3_client.get_object(Bucket=\u0026quot;test\u0026quot;, Key=\u0026quot;テキストたよ.txt\u0026quot;)[\u0026quot;Body\u0026quot;].read().decode('utf-8') print(text) 結果：\nテキストの中身だよ 正しくテキストを取得できています！\nまとめ MinIOはとても簡単に使えて、便利だと思いました。よくできていて感動しますね。 手元の環境にS3が欲しいなぁって思った方は使いましょう！\n","date":"2019-11-09T12:48:01+09:00","image":"https://opqrstuvcut.github.io/blog/posts/minio%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%81%ABs3%E3%81%BF%E3%81%9F%E3%81%84%E3%81%AA%E3%82%82%E3%81%AE%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%A6%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B/2d89cc4c8b3b3d34194b32b843ff40bf_hud6cb81a775978a88f5a41e08842d2fb5_16521_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/minio%E3%81%A7%E3%83%AD%E3%83%BC%E3%82%AB%E3%83%AB%E3%81%ABs3%E3%81%BF%E3%81%9F%E3%81%84%E3%81%AA%E3%82%82%E3%81%AE%E3%82%92%E4%BD%9C%E3%81%A3%E3%81%A6%E9%96%8B%E7%99%BA%E3%81%99%E3%82%8B/","title":"MinIOでローカルにS3みたいなものを作って開発する"},{"content":"本記事はQrunchからの転載です。\n 概要 自然言語界隈では非常によく話題になるBERTですが、BERTを使った文生成を実装してみたので今回はその話をします。BERTの事前学習モデルが文生成のタスクで使えたら、比較的少なめの学習データでもそれっぽく文生成できたりしないかなぁと思ってやってみました。\n実験ではポケモンの説明文を学習させて、生成させてみました。ちなみに自分はポケモンはルビー・サファイアで止まってますので、あんまりポケモンは分からないです。（他に面白そうな題材が見つからず…。遊戯王の通常モンスターの説明文でやりたかったんですが、データ数が700弱と少なすぎて断念。）\n参考にした論文：BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model\n使用した事前学習モデル：BERT日本語Pretrainedモデル\n実装したソースコード：https://github.com/opqrstuvcut/BertMouth\nBERTでの文生成 学習 学習は以下のようなネットワークを使っておこないます。 ネットワークへの入力となる各トークンはサブワードになります。\n例えば今回のように京都大学の事前学習モデルを利用する場合には、「何日だってなにも食べなくても元気 ！背中のタネ にたくさん栄養があるから元気だ！」という文はJuman++で形態素解析された後、サブワードに分割され、\n何/日/だって/なに/##も/食べ/なくて/も/元気/！/背中/の/タ/##ネ/に/たくさん/栄養/が/ある/から/元/##気/##だ/！ となります。\n上記のネットワークを使って、ランダムにマスクした部分のサブワードの確率が予測できるように、以下の手順を繰り返して学習をすすめていきます。\n ある文がN個のトークンから構成されているときに、ランダムに1つのトークンを[MASK]に置き換える（上の図の例だと2番目のトークンがこれに該当します）。 1つのトークンを[MASK]に置き換えたトークン列をBERTに与える。 BERTの出力のうち、[MASK]に対応するトークンの出力O[MASK]に対して全結合層とsoftmaxを適用する（softmaxの結果が全サブワードの出現確率になります）。 求められた[MASK]に対応する出現確率のうち、正解となるサブワードの確率が高くなるように、クロスエントロピーを用いて最適化する。  予測 予測は次のようにギブスサンプリングを使います。\n 長さNのトークン列を初期化する。 以下を適当な回数繰り返す。  次を全トークンに対しておこなう。  i番目(i=1,\u0026hellip;,N)のトークンを[MASK]で置き換え、学習したネットワークに入力する。 出現確率が最大のサブワードで[MASK]のトークンを置換する。      トークン列の初期化には全サブワードから一様分布に従ってサンプリングしていますが、人間が適当な文を入れてあげてもいいですし、色々やりようはあるかと思います。\n実験 データ 学習には https://wiki.ポケモン.com/wiki/ポケモン一覧 のポケモンの説明文から、漢字が使われている文のみを利用しています。訓練データに使われたのは4730文で、例えば以下のような文が含まれます。\n 生まれたときから 背中に 不思議な タネが 植えてあって 体と ともに 育つという。 トレーナーとの 絆が パワーの 源。 ジェット機を しのぐ 飛行能力を 誇る。  こんな感じのポケモンの説明文を自動で生成できたら面白いなぁと思ったので、このデータでやってみました。うまく行けば架空のポケモンが作れますね！\n結果 学習したモデルで予測した結果を示します。ちなみに予測するときにサブワードの数をあらかじめ指定しますが、以下の例ではサブワードの数は20です。\n生成文1: 弱い獲物を一度捕まえると止まらない。毎日１８時間鳴くチビノーズ。弱い獲物をいたぶっているのか、猟奇的な感じがします。\n生成文2: この姿に変化して連れ去ることでお腹を自在に操るピィができるのだ。お腹を自由に操る…？化して連れ去るあたりは悪いポケモン感が出ていていいですね。\n生成文3: ボールのように引っ張るため１匹。だが１匹ゆらゆら数は少ない。ちょっと解釈が難しいです。孤高の存在？\n生成文4: 化石から復活した科学者を科学力で壊し散らす生命力を持つポケモン。科学力で科学者に勝利するインテリポケモン。\n生成文5: ただ絶対に捕まえないので傷ついた相手には容赦しない。なぜだか。これは解釈が難しいですが、恐ろしいポケモン感がでてますね。「なぜだか。」がいいアクセントです。\nまとめ それっぽい文はできたけども、意味があまり通らない文が多いかなという印象です。とりあえず学習データが少ないので、文が多い他のデータで実験します。気力のある方はぜひ自分でデータを用意して、学習してみて結果を教えて欲しいです！\nおまけ 今回自分が使った京都大学の事前学習モデルを利用して学習する場合は、以下の手順で学習データを用意できます。\n  文を集めてきて、次のようなフォーマットのテキストファイルに保存する。\n文1 文2 ︙ 文N   juman++、pyknp、mojimojiをインストールする。pyknpとmojimojiはpipでOKです。\n  レポジトリにあるpreprocess.pyを次のように実行して、形態素解析と前処理をおこなう。\n python ./preprocess.py \\ --input_file 1で作ったテキストファイルのパス \\ --output_file 出力先のテキストファイルのパス \\ --model xxx/jumanpp-2.0.0-rc2/model/jumandic.jppmdl（jumanのモデルのパスが通っている場合は不要）   出力されたファイルを訓練データと検証データに適当に分割する。\n  ","date":"2019-11-07T11:42:23+09:00","image":"https://opqrstuvcut.github.io/blog/posts/bert%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%81%AE%E8%AA%AC%E6%98%8E%E6%96%87%E7%94%9F%E6%88%90/729b98aa8f9032f789244aa4e870b844_hua413e8993b42675a600325789a072244_97816_120x120_fill_box_smart1_3.png","permalink":"https://opqrstuvcut.github.io/blog/posts/bert%E3%81%A7%E3%81%8A%E3%81%93%E3%81%AA%E3%81%86%E3%83%9D%E3%82%B1%E3%83%A2%E3%83%B3%E3%81%AE%E8%AA%AC%E6%98%8E%E6%96%87%E7%94%9F%E6%88%90/","title":"BERTでおこなうポケモンの説明文生成"}]