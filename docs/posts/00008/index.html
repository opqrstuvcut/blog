<!DOCTYPE html>
<html lang="ja" class="direction-ltr">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<title>KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ - MatLoverによるMatlab以外のブログ</title>

<meta name="keywords" content="Python, KLdivergence, PyTorch, 機械学習, 正規分布" />
<meta name="description" content="本記事はQrunchからの転載です。
 みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。
KL divergence KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。 具体的には次のように定義されます。 $$ KL(p||q) = \int p(\mathbf{x}) \log \left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) {\rm d\mathbf{x}}. $$ $p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。 なお、$KL(p||q) \geq 0$が成り立つことに注意してください。
KL divergenceの最小化問題 KL(p||q)のケース 仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。
前述したKL divergenceの定義をみてみると、$p(\mathbf{x})$が0でない値をもつ領域では$q(\mathbf{x})$も$p(\mathbf{x})$に近い値かあるいは$p(\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような$q$は$p$全体をカバーするように広がる分布になると考えられます。
KL(q||p)のケース 次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は $$ KL(q||p) = \int q(\mathbf{x}) \log \left(\frac{q(\mathbf{x})}{p(\mathbf{x})}\right) {\rm d\mathbf{x}}$$ ですね。 $KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\mathbf{x})$が0に近いような領域で$q(\mathbf{x})$が小さくなるようにすればよいです。$p(\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような$q$は$p$の値が大きいところに集中するような分布になると考えられます。
実験 上記の話が成り立つのかを実験してみます。
実験準備 $p(\mathbf{x})$は次のようにします。
$$p(\mathbf{x}|\mathbf{u},\Sigma)=\frac{1}{{2\pi}|\Sigma|^{1/2}}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{u})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{u})}{2}\biggr].$$ また$\mathbf{u}$と$\Sigma$はそれぞれ $$\mathbf{u} = \begin{pmatrix} 0.3 \\ -0.2 \end{pmatrix}, \Sigma =\begin{pmatrix} 0.9&amp;-0.7 \\ -0.7 &amp; 0.9 \end{pmatrix}$$ とました。 $p$を確率密度毎に色わけして表示してみると、以下のとおりです。 また$q(\mathbf{x})$は次のようにします。 $$q(\mathbf{x}|\mathbf{s},\alpha)=\frac{1}{{2\pi}\alpha}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{s})^{\top}(\mathbf{x}-\mathbf{s})}{2\alpha}\biggr].">
<meta name="author" content="opqrstuvcut">
<link rel="canonical" href="https://opqrstuvcut.github.io/blog/posts/00008/" />
<link href="https://opqrstuvcut.github.io/blog/assets/css/stylesheet.min.7bd5899d65d8065bce667feacdde944a1911b79b7be54321635bc25d254c1b92.css" integrity="sha256-e9WJnWXYBlvOZn/qzd6UShkRt5t75UMhY1vCXSVMG5I=" rel="preload stylesheet"
    as="style">
<link rel="apple-touch-icon" href="https://opqrstuvcut.github.io/blog/apple-touch-icon.png">
<link rel="icon" href="https://opqrstuvcut.github.io/blog/favicon.ico">
<meta name="generator" content="Hugo 0.76.5" />



<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'G-G0W1C582TH', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>
<meta property="og:title" content="KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？" />
<meta property="og:description" content="本記事はQrunchからの転載です。
 みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。
KL divergence KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。 具体的には次のように定義されます。 $$ KL(p||q) = \int p(\mathbf{x}) \log \left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) {\rm d\mathbf{x}}. $$ $p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。 なお、$KL(p||q) \geq 0$が成り立つことに注意してください。
KL divergenceの最小化問題 KL(p||q)のケース 仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。
前述したKL divergenceの定義をみてみると、$p(\mathbf{x})$が0でない値をもつ領域では$q(\mathbf{x})$も$p(\mathbf{x})$に近い値かあるいは$p(\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような$q$は$p$全体をカバーするように広がる分布になると考えられます。
KL(q||p)のケース 次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は $$ KL(q||p) = \int q(\mathbf{x}) \log \left(\frac{q(\mathbf{x})}{p(\mathbf{x})}\right) {\rm d\mathbf{x}}$$ ですね。 $KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\mathbf{x})$が0に近いような領域で$q(\mathbf{x})$が小さくなるようにすればよいです。$p(\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような$q$は$p$の値が大きいところに集中するような分布になると考えられます。
実験 上記の話が成り立つのかを実験してみます。
実験準備 $p(\mathbf{x})$は次のようにします。
$$p(\mathbf{x}|\mathbf{u},\Sigma)=\frac{1}{{2\pi}|\Sigma|^{1/2}}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{u})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{u})}{2}\biggr].$$ また$\mathbf{u}$と$\Sigma$はそれぞれ $$\mathbf{u} = \begin{pmatrix} 0.3 \\ -0.2 \end{pmatrix}, \Sigma =\begin{pmatrix} 0.9&amp;-0.7 \\ -0.7 &amp; 0.9 \end{pmatrix}$$ とました。 $p$を確率密度毎に色わけして表示してみると、以下のとおりです。 また$q(\mathbf{x})$は次のようにします。 $$q(\mathbf{x}|\mathbf{s},\alpha)=\frac{1}{{2\pi}\alpha}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{s})^{\top}(\mathbf{x}-\mathbf{s})}{2\alpha}\biggr]." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://opqrstuvcut.github.io/blog/posts/00008/" />
<meta property="article:published_time" content="2020-10-26T00:43:01+09:00" />
<meta property="article:modified_time" content="2020-10-26T00:43:01+09:00" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？"/>
<meta name="twitter:description" content="本記事はQrunchからの転載です。
 みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。
KL divergence KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。 具体的には次のように定義されます。 $$ KL(p||q) = \int p(\mathbf{x}) \log \left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) {\rm d\mathbf{x}}. $$ $p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。 なお、$KL(p||q) \geq 0$が成り立つことに注意してください。
KL divergenceの最小化問題 KL(p||q)のケース 仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。
前述したKL divergenceの定義をみてみると、$p(\mathbf{x})$が0でない値をもつ領域では$q(\mathbf{x})$も$p(\mathbf{x})$に近い値かあるいは$p(\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような$q$は$p$全体をカバーするように広がる分布になると考えられます。
KL(q||p)のケース 次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は $$ KL(q||p) = \int q(\mathbf{x}) \log \left(\frac{q(\mathbf{x})}{p(\mathbf{x})}\right) {\rm d\mathbf{x}}$$ ですね。 $KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\mathbf{x})$が0に近いような領域で$q(\mathbf{x})$が小さくなるようにすればよいです。$p(\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような$q$は$p$の値が大きいところに集中するような分布になると考えられます。
実験 上記の話が成り立つのかを実験してみます。
実験準備 $p(\mathbf{x})$は次のようにします。
$$p(\mathbf{x}|\mathbf{u},\Sigma)=\frac{1}{{2\pi}|\Sigma|^{1/2}}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{u})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{u})}{2}\biggr].$$ また$\mathbf{u}$と$\Sigma$はそれぞれ $$\mathbf{u} = \begin{pmatrix} 0.3 \\ -0.2 \end{pmatrix}, \Sigma =\begin{pmatrix} 0.9&amp;-0.7 \\ -0.7 &amp; 0.9 \end{pmatrix}$$ とました。 $p$を確率密度毎に色わけして表示してみると、以下のとおりです。 また$q(\mathbf{x})$は次のようにします。 $$q(\mathbf{x}|\mathbf{s},\alpha)=\frac{1}{{2\pi}\alpha}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{s})^{\top}(\mathbf{x}-\mathbf{s})}{2\alpha}\biggr]."/>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？",
  "name": "KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？",
  "description": "本記事はQrunchからの転載です。\n みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ …",
  "keywords": [
    "Python", "KLdivergence", "PyTorch", "機械学習", "正規分布"
  ],
  "articleBody": "本記事はQrunchからの転載です。\n みんながよく使うKL(Kullback–Leibler) divergenceの話題です。 KL divergenceといえば2つの確率分布の違いを計算できるやつですね。 KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。 今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。\nKL divergence KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。 具体的には次のように定義されます。 $$ KL(p||q) = \\int p(\\mathbf{x}) \\log \\left(\\frac{p(\\mathbf{x})}{q(\\mathbf{x})}\\right) {\\rm d\\mathbf{x}}. $$ $p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。 なお、$KL(p||q) \\geq 0$が成り立つことに注意してください。\nKL divergenceの最小化問題 KL(p||q)のケース 仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。\n前述したKL divergenceの定義をみてみると、$p(\\mathbf{x})$が0でない値をもつ領域では$q(\\mathbf{x})$も$p(\\mathbf{x})$に近い値かあるいは$p(\\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような$q$は$p$全体をカバーするように広がる分布になると考えられます。\nKL(q||p)のケース 次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は $$ KL(q||p) = \\int q(\\mathbf{x}) \\log \\left(\\frac{q(\\mathbf{x})}{p(\\mathbf{x})}\\right) {\\rm d\\mathbf{x}}$$ ですね。 $KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\\mathbf{x})$が0に近いような領域で$q(\\mathbf{x})$が小さくなるようにすればよいです。$p(\\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような$q$は$p$の値が大きいところに集中するような分布になると考えられます。\n実験 上記の話が成り立つのかを実験してみます。\n実験準備 $p(\\mathbf{x})$は次のようにします。\n$$p(\\mathbf{x}|\\mathbf{u},\\Sigma)=\\frac{1}{{2\\pi}|\\Sigma|^{1/2}}\\exp\\biggl[-\\frac{(\\mathbf{x}-\\mathbf{u})^{\\top}\\Sigma^{-1}(\\mathbf{x}-\\mathbf{u})}{2}\\biggr].$$ また$\\mathbf{u}$と$\\Sigma$はそれぞれ $$\\mathbf{u} = \\begin{pmatrix} 0.3 \\\\ -0.2 \\end{pmatrix}, \\Sigma =\\begin{pmatrix} 0.9\u0026-0.7 \\\\ -0.7 \u0026 0.9 \\end{pmatrix}$$ とました。 $p$を確率密度毎に色わけして表示してみると、以下のとおりです。 また$q(\\mathbf{x})$は次のようにします。 $$q(\\mathbf{x}|\\mathbf{s},\\alpha)=\\frac{1}{{2\\pi}\\alpha}\\exp\\biggl[-\\frac{(\\mathbf{x}-\\mathbf{s})^{\\top}(\\mathbf{x}-\\mathbf{s})}{2\\alpha}\\biggr].$$\n$q$のうち、$\\mathbf{s}$と$\\alpha$が最適化するべきパラメータです。 $q$は同心円状に確率密度をもつ分布になりますので、パラメータをどうやっても$p$と一致することはできません。\n実験結果 $KL(p||q)$を最小化したケースをまず示します。 白い線が$p$の等高線です。色分けされて表示されているのが、$q$の確率密度になります。 先程の話のとおり、$q$は$p$に対して広がった分布になっていることがわかります。\n次に$KL(q||p)$を最小化したケースです。 こちらも先程の話のとおり、$q$は$p$の値が大きい箇所に集中した分布になっています。\nまとめ 今回は$q$を$p$に近づける話に限定しましたが、KL divergenceに与える分布を入れ替えると結果が変わるケースが多そうだなと想像できたんじゃないかと思います。 頭の片隅に留めておくと役立つかもしれません。\n実験に使ったスクリプト #! /usr/bin/env python import argparse import os import logging import matplotlib.pyplot as plt import numpy as np from scipy.stats import multivariate_normal import torch from torch.distributions import MultivariateNormal import torch.optim as optim def parse_argument(): parser = argparse.ArgumentParser(\"\", add_help=True) parser.add_argument(\"-o\", \"--output_dir\", type=str) args = parser.parse_args() return args def make_data(border=5): xy = np.mgrid[-border:border:0.005, -border:border:0.005] grids = xy.shape[1] x = xy[0] y = xy[1] xy = xy.reshape(2, -1).T p_pdf = multivariate_normal.pdf(xy, np.array([0, 0]), np.array([[.9, -.7], [-.7, .9]])) return xy, x, y, p_pdf, grids def kl_div(p, q): finite_index = ~((q == 0.) | (torch.isinf(p))) q = q[finite_index] logq = torch.log(q) return torch.sum(q * (logq - p[finite_index])) def optimize_q(xy, p_pdf, invert=False): p_pdf = torch.tensor(p_pdf, requires_grad=False, dtype=torch.float32) mean = torch.tensor([0.3, -0.2], requires_grad=True) cov_coeff = torch.tensor(1., requires_grad=True) xy = torch.tensor(xy, requires_grad=False, dtype=torch.float32) optimizer = optim.SGD([mean, cov_coeff], lr=.000005) if not invert: p_pdf = torch.log(p_pdf) for i in range(25): optimizer.zero_grad() cov = torch.eye(2, 2) * cov_coeff norm_torch = MultivariateNormal(mean, cov) q_pdf = norm_torch.log_prob(xy) if invert: loss = kl_div(q_pdf, p_pdf) else: q_pdf = torch.exp(q_pdf) loss = kl_div(p_pdf, q_pdf) loss.backward() optimizer.step() logging.info( f\"[{i + 1}iter] loss:{loss}, mean:{mean}, cov_alpha:{cov_coeff}\") return mean.detach().numpy(), cov_coeff.detach().numpy() def plot_dist(x, y, p_pdf, border, output_path, contour=None): plt.pcolormesh(x, y, p_pdf, cmap=\"nipy_spectral\") plt.colorbar() if contour is not None: plt.contour(x, y, contour, colors=\"white\", levels=5) plt.savefig(output_path) plt.close() if __name__ == \"__main__\": logger = logging.basicConfig(level=logging.INFO) args = parse_argument() output_dir = args.output_dir border = 5 xy, x, y, p_pdf, grids = make_data(border) data_dist_path = os.path.join(output_dir, \"data_dist.png\") plot_dist(x, y, p_pdf.reshape(grids, grids), border, data_dist_path) for invert in [True, False]: mean, cov_coeff = optimize_q(xy, p_pdf, invert=invert) output_path = os.path.join( output_dir, f\"gauss_m{mean}_c{cov_coeff}.png\") q_pdf = multivariate_normal.pdf(xy, mean, np.eye(2, 2) * cov_coeff) plot_dist(x, y, q_pdf.reshape(grids, grids), border, output_path, contour=p_pdf.reshape(grids, grids)) logging.info(f\"{output_path} is saved.\") ",
  "wordCount" : "358",
  "inLanguage": "ja",
  
  "datePublished": "2020-10-26T00:43:01+09:00",
  "dateModified": "2020-10-26T00:43:01+09:00",
  "author": {
    "@type": "Person",
    "name": "opqrstuvcut"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://opqrstuvcut.github.io/blog/posts/00008/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "MatLoverによるMatlab以外のブログ",
    "logo": {
      "@type": "ImageObject",
      "url": "https://opqrstuvcut.github.io/blog/favicon.ico"
    }
  }
}
</script>



</head>

<body class="single" id="top">
<noscript>
    <style type="text/css">
        .theme-toggle,
        .top-link {
            display: none;
        }
    </style>
</noscript>
<header class="header">
    <nav class="nav">
        <p class="logo">
            <a href="https://opqrstuvcut.github.io/blog">MatLoverによるMatlab以外のブログ</a>
        </p>
        <ul class="menu" id="menu" onscroll="menu_on_scroll()">
            <li>
                <a href="https://opqrstuvcut.github.io/blog/posts/">
                    <span>
                        Posts
                    </span>
                </a>
            </li>
            <li>
                <a href="https://opqrstuvcut.github.io/blog/tags/">
                    <span>
                        Tags
                    </span>
                </a>
            </li>
        </ul>
    </nav>
</header>

    <main class="main">

<article class="post-single">
  <header class="post-header">
    <h1 class="post-title">
      KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？
    </h1>
    <div class="post-meta"><time>October 26, 2020</time>&nbsp;·&nbsp;2 min&nbsp;·&nbsp;opqrstuvcut
    </div>
  </header> 

  <div class="toc">
    <details >
      <summary>
        <div class="details">Table of Contents</div>
      </summary>
      <blockquote><ul><li>
        <a href="#kl-divergence">KL divergence</a></li><li>
        <a href="#kl-divergence%e3%81%ae%e6%9c%80%e5%b0%8f%e5%8c%96%e5%95%8f%e9%a1%8c">KL divergenceの最小化問題</a><ul>
            <li>
        <a href="#klpq%e3%81%ae%e3%82%b1%e3%83%bc%e3%82%b9">KL(p||q)のケース</a></li><li>
        <a href="#klqp%e3%81%ae%e3%82%b1%e3%83%bc%e3%82%b9">KL(q||p)のケース</a></li></ul>
    </li><li>
        <a href="#%e5%ae%9f%e9%a8%93">実験</a><ul>
            <li>
        <a href="#%e5%ae%9f%e9%a8%93%e6%ba%96%e5%82%99">実験準備</a></li><li>
        <a href="#%e5%ae%9f%e9%a8%93%e7%b5%90%e6%9e%9c">実験結果</a></li></ul>
    </li><li>
        <a href="#%e3%81%be%e3%81%a8%e3%82%81">まとめ</a></li><li>
        <a href="#%e5%ae%9f%e9%a8%93%e3%81%ab%e4%bd%bf%e3%81%a3%e3%81%9f%e3%82%b9%e3%82%af%e3%83%aa%e3%83%97%e3%83%88">実験に使ったスクリプト</a></li></ul>
      </blockquote>
    </details>
  </div>
  <div class="post-content"><p>本記事はQrunchからの転載です。</p>
<hr>
<p>みんながよく使うKL(Kullback–Leibler) divergenceの話題です。
KL divergenceといえば2つの確率分布の違いを計算できるやつですね。
KL divergenceは対称性というものがなく、与えられた2つの分布を入れ替えるとKL divergenceの値が変わります。
今回は、この入れ替えたときの影響を最小化問題を例としてまじめに考えます。</p>
<h1 id="kl-divergence">KL divergence</h1>
<p>KL divergenceは2つの確率分布がどれだけ異なるかを数値としてあらわすものです。
具体的には次のように定義されます。
$$ KL(p||q) = \int p(\mathbf{x}) \log \left(\frac{p(\mathbf{x})}{q(\mathbf{x})}\right) {\rm d\mathbf{x}}. $$
$p$と$q$はそれぞれ確率分布であり、$KL(p||q)$が大きいほど、2つの分布はより異なることをあらわします。また$KL(p||q)=0$のとき、$p$と$q$は等しい分布です。
なお、$KL(p||q) \geq 0$が成り立つことに注意してください。</p>
<h1 id="kl-divergenceの最小化問題">KL divergenceの最小化問題</h1>
<h2 id="klpqのケース">KL(p||q)のケース</h2>
<p>仮に分布$p$が固定されているものだとして、$KL(p||q)$が最小化されるように$q$を決めることを考えます。ただし、$p=q$になることはないとします。</p>
<p>前述したKL divergenceの定義をみてみると、$p(\mathbf{x})$が0でない値をもつ領域では$q(\mathbf{x})$も$p(\mathbf{x})$に近い値かあるいは$p(\mathbf{x})$より大きい値にならなければ、$KL(p||q)$が大きくなってしまいます。よってこの場合にはKL divergenceを最小化するような<!-- raw HTML omitted -->$q$は$p$全体をカバーするように広がる分布<!-- raw HTML omitted -->になると考えられます。</p>
<h2 id="klqpのケース">KL(q||p)のケース</h2>
<p>次にKL divergenceに与える$p$と$q$の順序をひっくり返し、$KL(q||p)$の最小化問題を考えてみます。$KL(q||p)$は
$$ KL(q||p) = \int q(\mathbf{x}) \log \left(\frac{q(\mathbf{x})}{p(\mathbf{x})}\right) {\rm d\mathbf{x}}$$
ですね。
$KL(q||p)$が小さくなるにはどうすればよいかといえば、$p(\mathbf{x})$が0に近いような領域で$q(\mathbf{x})$が小さくなるようにすればよいです。$p(\mathbf{x})$が小さい領域はいくらでもあり、そういったところに大きい$q(\mathbf{x})$が割り当てられると、$KL(p||q)$が大きくなってしまいますね。このため、イメージとしては、$KL(p||q)$を最小化するような<!-- raw HTML omitted -->$q$は$p$の値が大きいところに集中するような分布<!-- raw HTML omitted -->になると考えられます。</p>
<h1 id="実験">実験</h1>
<p>上記の話が成り立つのかを実験してみます。</p>
<h2 id="実験準備">実験準備</h2>
<p>$p(\mathbf{x})$は次のようにします。</p>
<p>$$p(\mathbf{x}|\mathbf{u},\Sigma)=\frac{1}{{2\pi}|\Sigma|^{1/2}}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{u})^{\top}\Sigma^{-1}(\mathbf{x}-\mathbf{u})}{2}\biggr].$$
また$\mathbf{u}$と$\Sigma$はそれぞれ
$$\mathbf{u} = \begin{pmatrix} 0.3 \\ -0.2 \end{pmatrix}, \Sigma =\begin{pmatrix} 0.9&amp;-0.7 \\ -0.7 &amp; 0.9 \end{pmatrix}$$
とました。
$p$を確率密度毎に色わけして表示してみると、以下のとおりです。
<img src="a656f547ccf54333352ff64a2de84cd7.png" alt="undefined.jpg"></p>
<p>また$q(\mathbf{x})$は次のようにします。
$$q(\mathbf{x}|\mathbf{s},\alpha)=\frac{1}{{2\pi}\alpha}\exp\biggl[-\frac{(\mathbf{x}-\mathbf{s})^{\top}(\mathbf{x}-\mathbf{s})}{2\alpha}\biggr].$$</p>
<p>$q$のうち、$\mathbf{s}$と$\alpha$が最適化するべきパラメータです。
$q$は同心円状に確率密度をもつ分布になりますので、パラメータをどうやっても$p$と一致することはできません。</p>
<h2 id="実験結果">実験結果</h2>
<p>$KL(p||q)$を最小化したケースをまず示します。
<img src="983be7a190c4aaf3488cd6c3d4471158.png" alt="undefined.jpg">
白い線が$p$の等高線です。色分けされて表示されているのが、$q$の確率密度になります。
先程の話のとおり、$q$は$p$に対して広がった分布になっていることがわかります。</p>
<p>次に$KL(q||p)$を最小化したケースです。
<img src="ffae8716aa0ab67fa3d7ff25156a8dcb.png" alt="undefined.jpg">
こちらも先程の話のとおり、$q$は$p$の値が大きい箇所に集中した分布になっています。</p>
<h1 id="まとめ">まとめ</h1>
<p>今回は$q$を$p$に近づける話に限定しましたが、KL divergenceに与える分布を入れ替えると結果が変わるケースが多そうだなと想像できたんじゃないかと思います。
頭の片隅に留めておくと役立つかもしれません。</p>
<h1 id="実験に使ったスクリプト">実験に使ったスクリプト</h1>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e">#! /usr/bin/env python</span>
<span style="color:#f92672">import</span> argparse
<span style="color:#f92672">import</span> os
<span style="color:#f92672">import</span> logging

<span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#f92672">as</span> plt
<span style="color:#f92672">import</span> numpy <span style="color:#f92672">as</span> np
<span style="color:#f92672">from</span> scipy.stats <span style="color:#f92672">import</span> multivariate_normal
<span style="color:#f92672">import</span> torch
<span style="color:#f92672">from</span> torch.distributions <span style="color:#f92672">import</span> MultivariateNormal
<span style="color:#f92672">import</span> torch.optim <span style="color:#f92672">as</span> optim


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">parse_argument</span>():
    parser <span style="color:#f92672">=</span> argparse<span style="color:#f92672">.</span>ArgumentParser(<span style="color:#e6db74">&#34;&#34;</span>, add_help<span style="color:#f92672">=</span>True)
    parser<span style="color:#f92672">.</span>add_argument(<span style="color:#e6db74">&#34;-o&#34;</span>, <span style="color:#e6db74">&#34;--output_dir&#34;</span>, type<span style="color:#f92672">=</span>str)
    args <span style="color:#f92672">=</span> parser<span style="color:#f92672">.</span>parse_args()
    <span style="color:#66d9ef">return</span> args


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">make_data</span>(border<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>):
    xy <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mgrid[<span style="color:#f92672">-</span>border:border:<span style="color:#ae81ff">0.005</span>, <span style="color:#f92672">-</span>border:border:<span style="color:#ae81ff">0.005</span>]
    grids <span style="color:#f92672">=</span> xy<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]
    x <span style="color:#f92672">=</span> xy[<span style="color:#ae81ff">0</span>]
    y <span style="color:#f92672">=</span> xy[<span style="color:#ae81ff">1</span>]
    xy <span style="color:#f92672">=</span> xy<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">2</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)<span style="color:#f92672">.</span>T
    p_pdf <span style="color:#f92672">=</span> multivariate_normal<span style="color:#f92672">.</span>pdf(xy, np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>]),
                                    np<span style="color:#f92672">.</span>array([[<span style="color:#f92672">.</span><span style="color:#ae81ff">9</span>, <span style="color:#f92672">-.</span><span style="color:#ae81ff">7</span>], [<span style="color:#f92672">-.</span><span style="color:#ae81ff">7</span>, <span style="color:#f92672">.</span><span style="color:#ae81ff">9</span>]]))

    <span style="color:#66d9ef">return</span> xy, x, y, p_pdf, grids


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">kl_div</span>(p, q):
    finite_index <span style="color:#f92672">=</span> <span style="color:#f92672">~</span>((q <span style="color:#f92672">==</span> <span style="color:#ae81ff">0.</span>) <span style="color:#f92672">|</span> (torch<span style="color:#f92672">.</span>isinf(p)))
    q <span style="color:#f92672">=</span> q[finite_index]
    logq <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log(q)
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>sum(q <span style="color:#f92672">*</span> (logq <span style="color:#f92672">-</span> p[finite_index]))


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">optimize_q</span>(xy, p_pdf, invert<span style="color:#f92672">=</span>False):
    p_pdf <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(p_pdf, requires_grad<span style="color:#f92672">=</span>False, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
    mean <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor([<span style="color:#ae81ff">0.3</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.2</span>], requires_grad<span style="color:#f92672">=</span>True)
    cov_coeff <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(<span style="color:#ae81ff">1.</span>, requires_grad<span style="color:#f92672">=</span>True)

    xy <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>tensor(xy, requires_grad<span style="color:#f92672">=</span>False, dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>float32)
    optimizer <span style="color:#f92672">=</span> optim<span style="color:#f92672">.</span>SGD([mean, cov_coeff], lr<span style="color:#f92672">=.</span><span style="color:#ae81ff">000005</span>)

    <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> invert:
        p_pdf <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log(p_pdf)

    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">25</span>):
        optimizer<span style="color:#f92672">.</span>zero_grad()
        cov <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> cov_coeff
        norm_torch <span style="color:#f92672">=</span> MultivariateNormal(mean, cov)
        q_pdf <span style="color:#f92672">=</span> norm_torch<span style="color:#f92672">.</span>log_prob(xy)
        <span style="color:#66d9ef">if</span> invert:
            loss <span style="color:#f92672">=</span> kl_div(q_pdf, p_pdf)
        <span style="color:#66d9ef">else</span>:
            q_pdf <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>exp(q_pdf)
            loss <span style="color:#f92672">=</span> kl_div(p_pdf, q_pdf)

        loss<span style="color:#f92672">.</span>backward()
        optimizer<span style="color:#f92672">.</span>step()

        logging<span style="color:#f92672">.</span>info(
            f<span style="color:#e6db74">&#34;[{i + 1}iter] loss:{loss}, mean:{mean}, cov_alpha:{cov_coeff}&#34;</span>)

    <span style="color:#66d9ef">return</span> mean<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy(), cov_coeff<span style="color:#f92672">.</span>detach()<span style="color:#f92672">.</span>numpy()


<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_dist</span>(x, y, p_pdf, border, output_path, contour<span style="color:#f92672">=</span>None):
    plt<span style="color:#f92672">.</span>pcolormesh(x, y, p_pdf, cmap<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;nipy_spectral&#34;</span>)
    plt<span style="color:#f92672">.</span>colorbar()
    <span style="color:#66d9ef">if</span> contour <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> None:
        plt<span style="color:#f92672">.</span>contour(x, y, contour, colors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;white&#34;</span>, levels<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)
    plt<span style="color:#f92672">.</span>savefig(output_path)
    plt<span style="color:#f92672">.</span>close()


<span style="color:#66d9ef">if</span> __name__ <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;__main__&#34;</span>:
    logger <span style="color:#f92672">=</span> logging<span style="color:#f92672">.</span>basicConfig(level<span style="color:#f92672">=</span>logging<span style="color:#f92672">.</span>INFO)
    args <span style="color:#f92672">=</span> parse_argument()
    output_dir <span style="color:#f92672">=</span> args<span style="color:#f92672">.</span>output_dir

    border <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>

    xy, x, y, p_pdf, grids <span style="color:#f92672">=</span> make_data(border)
    data_dist_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(output_dir, <span style="color:#e6db74">&#34;data_dist.png&#34;</span>)
    plot_dist(x, y, p_pdf<span style="color:#f92672">.</span>reshape(grids, grids), border, data_dist_path)

    <span style="color:#66d9ef">for</span> invert <span style="color:#f92672">in</span> [True, False]:
        mean, cov_coeff <span style="color:#f92672">=</span> optimize_q(xy, p_pdf, invert<span style="color:#f92672">=</span>invert)
        output_path <span style="color:#f92672">=</span> os<span style="color:#f92672">.</span>path<span style="color:#f92672">.</span>join(
            output_dir, f<span style="color:#e6db74">&#34;gauss_m{mean}_c{cov_coeff}.png&#34;</span>)

        q_pdf <span style="color:#f92672">=</span> multivariate_normal<span style="color:#f92672">.</span>pdf(xy, mean, np<span style="color:#f92672">.</span>eye(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> cov_coeff)
        plot_dist(x, y, q_pdf<span style="color:#f92672">.</span>reshape(grids, grids), border,
                  output_path, contour<span style="color:#f92672">=</span>p_pdf<span style="color:#f92672">.</span>reshape(grids, grids))
        logging<span style="color:#f92672">.</span>info(f<span style="color:#e6db74">&#34;{output_path} is saved.&#34;</span>)

</code></pre></div></div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://opqrstuvcut.github.io/blog/tags/python">Python</a></li>
      <li><a href="https://opqrstuvcut.github.io/blog/tags/kldivergence">KLdivergence</a></li>
      <li><a href="https://opqrstuvcut.github.io/blog/tags/pytorch">PyTorch</a></li>
      <li><a href="https://opqrstuvcut.github.io/blog/tags/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92">機械学習</a></li>
      <li><a href="https://opqrstuvcut.github.io/blog/tags/%E6%AD%A3%E8%A6%8F%E5%88%86%E5%B8%83">正規分布</a></li>
    </ul>
<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ on twitter"
        href="https://twitter.com/intent/tweet/?text=KL%20divergence%e3%81%ab%e4%b8%8e%e3%81%88%e3%82%8b%e5%88%86%e5%b8%83%e3%82%92%e5%85%a5%e3%82%8c%e6%9b%bf%e3%81%88%e3%82%8b%e3%81%93%e3%81%a8%e3%81%ae%e6%84%8f%e5%91%b3%e3%82%92%e3%81%be%e3%81%98%e3%82%81%e3%81%ab%e8%80%83%e3%81%88%e3%81%9f%e3%81%93%e3%81%a8%e3%81%82%e3%82%8a%e3%81%be%e3%81%99%ef%bc%9f&amp;url=https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f&amp;title=KL%20divergence%e3%81%ab%e4%b8%8e%e3%81%88%e3%82%8b%e5%88%86%e5%b8%83%e3%82%92%e5%85%a5%e3%82%8c%e6%9b%bf%e3%81%88%e3%82%8b%e3%81%93%e3%81%a8%e3%81%ae%e6%84%8f%e5%91%b3%e3%82%92%e3%81%be%e3%81%98%e3%82%81%e3%81%ab%e8%80%83%e3%81%88%e3%81%9f%e3%81%93%e3%81%a8%e3%81%82%e3%82%8a%e3%81%be%e3%81%99%ef%bc%9f&amp;summary=KL%20divergence%e3%81%ab%e4%b8%8e%e3%81%88%e3%82%8b%e5%88%86%e5%b8%83%e3%82%92%e5%85%a5%e3%82%8c%e6%9b%bf%e3%81%88%e3%82%8b%e3%81%93%e3%81%a8%e3%81%ae%e6%84%8f%e5%91%b3%e3%82%92%e3%81%be%e3%81%98%e3%82%81%e3%81%ab%e8%80%83%e3%81%88%e3%81%9f%e3%81%93%e3%81%a8%e3%81%82%e3%82%8a%e3%81%be%e3%81%99%ef%bc%9f&amp;source=https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f&title=KL%20divergence%e3%81%ab%e4%b8%8e%e3%81%88%e3%82%8b%e5%88%86%e5%b8%83%e3%82%92%e5%85%a5%e3%82%8c%e6%9b%bf%e3%81%88%e3%82%8b%e3%81%93%e3%81%a8%e3%81%ae%e6%84%8f%e5%91%b3%e3%82%92%e3%81%be%e3%81%98%e3%82%81%e3%81%ab%e8%80%83%e3%81%88%e3%81%9f%e3%81%93%e3%81%a8%e3%81%82%e3%82%8a%e3%81%be%e3%81%99%ef%bc%9f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ on whatsapp"
        href="https://api.whatsapp.com/send?text=KL%20divergence%e3%81%ab%e4%b8%8e%e3%81%88%e3%82%8b%e5%88%86%e5%b8%83%e3%82%92%e5%85%a5%e3%82%8c%e6%9b%bf%e3%81%88%e3%82%8b%e3%81%93%e3%81%a8%e3%81%ae%e6%84%8f%e5%91%b3%e3%82%92%e3%81%be%e3%81%98%e3%82%81%e3%81%ab%e8%80%83%e3%81%88%e3%81%9f%e3%81%93%e3%81%a8%e3%81%82%e3%82%8a%e3%81%be%e3%81%99%ef%bc%9f%20-%20https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share KL divergenceに与える分布を入れ替えることの意味をまじめに考えたことあります？ on telegram"
        href="https://telegram.me/share/url?text=KL%20divergence%e3%81%ab%e4%b8%8e%e3%81%88%e3%82%8b%e5%88%86%e5%b8%83%e3%82%92%e5%85%a5%e3%82%8c%e6%9b%bf%e3%81%88%e3%82%8b%e3%81%93%e3%81%a8%e3%81%ae%e6%84%8f%e5%91%b3%e3%82%92%e3%81%be%e3%81%98%e3%82%81%e3%81%ab%e8%80%83%e3%81%88%e3%81%9f%e3%81%93%e3%81%a8%e3%81%82%e3%82%8a%e3%81%be%e3%81%99%ef%bc%9f&amp;url=https%3a%2f%2fopqrstuvcut.github.io%2fblog%2fposts%2f00008%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>
  </footer>
</article>
    </main><footer>
  <html>
  <head>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
      integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X"
      crossorigin="anonymous"
    />

    
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
      integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
      crossorigin="anonymous"
    ></script>

    
    <script
      defer
      src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
      integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa"
      crossorigin="anonymous"
      onload="renderMathInElement(document.body);"
    ></script>
    <script>
      document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
          delimiters: [
            { left: "$$", right: "$$", display: true },
            { left: "$", right: "$", display: false },
          ],
        });
      });
    </script>
  </head>
</html>

</footer>

</body>

</html>